{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.cnp import CNP\n",
    "from models.wta_cnp import WTA_CNP\n",
    "\n",
    "from data.data_generators import *\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_max_obs, n_max_tar = 10, 10\n",
    "\n",
    "t_steps = 200\n",
    "num_demos = 32\n",
    "num_classes = 4\n",
    "num_indiv = num_demos//num_classes  # number of demos per class\n",
    "noise_clip = 0.0\n",
    "dx, dy = 1, 1\n",
    "\n",
    "num_val = 8\n",
    "num_val_indiv = num_val//num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([32, 200, 1]) Y: torch.Size([32, 200, 1]) VX: torch.Size([8, 200, 1]) VY: torch.Size([8, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "colors = [sns.color_palette('tab10')[0], sns.color_palette('tab10')[1], sns.color_palette('tab10')[2], sns.color_palette('tab10')[3]]\n",
    "sns.set_palette('tab10')\n",
    "\n",
    "x = torch.linspace(0, 1, 200).repeat(num_indiv, 1)\n",
    "y = torch.zeros(num_demos, t_steps, dy)\n",
    "\n",
    "vx = torch.linspace(0, 1, 200).repeat(num_val_indiv, 1)\n",
    "vy = torch.zeros(num_val, t_steps, dy)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    start_ind = i*num_indiv\n",
    "    coeff = (i+1)/2*torch.pi\n",
    "    y[start_ind:start_ind+num_indiv] = (torch.unsqueeze(generate_sin(x*coeff), 2) +1)/2.0\n",
    "\n",
    "    noise = torch.unsqueeze(torch.clamp(torch.randn(vx.shape)*1e-4**0.5, min=0) - noise_clip, -1)\n",
    "\n",
    "    start_ind = i*num_val_indiv\n",
    "    vy[start_ind:start_ind+num_val_indiv] = y[start_ind:start_ind+num_val_indiv].clone() + noise\n",
    "\n",
    "x = torch.unsqueeze(x.repeat(num_classes, 1), 2)  # since dx = 1\n",
    "vx = torch.unsqueeze(vx.repeat(num_classes, 1), 2)\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i in range(num_demos):\n",
    "#     plt.plot(x[i, :, 0].cpu(), y[i, :, 0].cpu(), label=f'Sine Wave {i+1}', linewidth=2.0, color=colors[i])\n",
    "#     # plt.plot(vx[i, :, 0].cpu(), vy[i, :, 0].cpu(), 'k', alpha=0.5)\n",
    "\n",
    "# plt.legend(loc='lower left', fontsize=14)\n",
    "# plt.grid(True)\n",
    "# plt.xlabel('Time (s)', fontsize=14)\n",
    "# plt.ylabel('Amplitude', fontsize=14)\n",
    "# plt.title(f'Sine Wave of 3 Different Frequencies', fontsize=16)\n",
    "# plt.savefig(f'/home/yigit/papers/yildirim_23_ral/fig/3.png', bbox_inches='tight')\n",
    "\n",
    "# x0, y0 = x.to(device_wta), y.to(device_wta)\n",
    "# x1, y1 = x.to(device_cnp), y.to(device_cnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnep2: 302338\n",
      "cnep4: 302468\n",
      "cnep8: 301720\n"
     ]
    }
   ],
   "source": [
    "model2 = WTA_CNP(1, 1, n_max_obs, n_max_tar, [128,128,128], num_decoders=2, decoder_hidden_dims=[306,306,306], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer2 = torch.optim.Adam(lr=1e-4, params=model2.parameters())\n",
    "\n",
    "model4 = WTA_CNP(1, 1, n_max_obs, n_max_tar, [128,128,128], num_decoders=4, decoder_hidden_dims=[201,201,201], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer4 = torch.optim.Adam(lr=1e-4, params=model4.parameters())\n",
    "\n",
    "model8 = WTA_CNP(1, 1, n_max_obs, n_max_tar, [128,128,128], num_decoders=8, decoder_hidden_dims=[128,128,128], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer8 = torch.optim.Adam(lr=1e-4, params=model8.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"cnep2:\", get_parameter_count(model2))\n",
    "print(\"cnep4:\", get_parameter_count(model4))\n",
    "print(\"cnep8:\", get_parameter_count(model8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best 2: 0.8310655057430267\n",
      "New best 4: 0.8665616810321808\n",
      "New best 8: 0.8729958534240723\n",
      "Epoch: 0, WTA-Losses: 0.0012693459987640381, 0.0012839414477348327, 0.0013123385459184646\n",
      "New best 2: 0.17857730388641357\n",
      "New best 4: 0.21216754615306854\n",
      "New best 8: 0.3704148679971695\n",
      "Epoch: 1000, WTA-Losses: 0.6776024906113743, 0.717448109678924, 0.7161427522823215\n",
      "New best 2: 0.171315997838974\n",
      "New best 8: 0.2956749200820923\n",
      "Epoch: 2000, WTA-Losses: 0.6711344202905893, 0.7092190500944853, 0.6979867082238197\n",
      "New best 2: 0.1553683504462242\n",
      "New best 8: 0.2721259891986847\n",
      "Epoch: 3000, WTA-Losses: 0.66865254509449, 0.6966841317936778, 0.6939753285124898\n",
      "New best 2: 0.1363862231373787\n",
      "New best 8: 0.25865761935710907\n",
      "Epoch: 4000, WTA-Losses: 0.6645324417352676, 0.716133811749518, 0.7005838989242912\n",
      "New best 2: 0.060447415336966515\n",
      "New best 8: 0.2543213963508606\n",
      "Epoch: 5000, WTA-Losses: 0.6573581558242441, 0.6931938239932061, 0.6985153225138784\n",
      "New best 8: 0.24909192323684692\n",
      "Epoch: 6000, WTA-Losses: 0.6519629282280802, 0.6857516737803817, 0.6881829321533441\n",
      "New best 2: 0.055563874542713165\n",
      "Epoch: 7000, WTA-Losses: 0.6471182665601373, 0.6815442559793592, 0.6829531219154596\n",
      "New best 2: 0.04301669611595571\n",
      "Epoch: 8000, WTA-Losses: 0.6431807238608599, 0.6800962133854628, 0.6813623255714775\n",
      "New best 2: 0.013020597398281097\n",
      "Epoch: 9000, WTA-Losses: 0.6409989524111152, 0.6823548755124211, 0.6815066067352891\n",
      "Epoch: 10000, WTA-Losses: 0.6382065116614103, 0.6800306658893823, 0.6805352569371462\n",
      "New best 2: 0.004137190408073366\n",
      "New best 8: 0.24868590384721756\n",
      "Epoch: 11000, WTA-Losses: 0.6366843292340636, 0.6795089066550135, 0.6800631868168712\n",
      "Epoch: 12000, WTA-Losses: 0.6356910200938583, 0.6804444299191237, 0.6809020489528775\n",
      "Epoch: 13000, WTA-Losses: 0.6338660214841366, 0.7523838641121984, 0.6814822316393256\n",
      "Epoch: 14000, WTA-Losses: 0.6323826000243425, 0.6798128372207284, 0.6797653711214662\n",
      "New best 2: 0.001942632719874382\n",
      "Epoch: 15000, WTA-Losses: 0.631253215610981, 0.6805895792171359, 0.6800742294713855\n",
      "Epoch: 16000, WTA-Losses: 0.6276985048800707, 0.6795548689216375, 0.6796418309807778\n",
      "Epoch: 17000, WTA-Losses: 0.6261871752366424, 0.6799741809666157, 0.6790872901380062\n",
      "Epoch: 18000, WTA-Losses: 0.624388841971755, 0.6807410727292299, 0.6800815267711878\n",
      "Epoch: 19000, WTA-Losses: 0.6204038881808519, 0.679777260787785, 0.6799009551405907\n",
      "New best 8: 0.24867086112499237\n",
      "Epoch: 20000, WTA-Losses: 0.6184241786748171, 0.6805128218382597, 0.6808169809728861\n",
      "Epoch: 21000, WTA-Losses: 0.616245891071856, 0.6802062810733914, 0.6803359786346554\n",
      "Epoch: 22000, WTA-Losses: 0.6137856620773673, 0.6807596754580736, 0.6804721870049835\n",
      "Epoch: 23000, WTA-Losses: 0.61163648288697, 0.680366431042552, 0.7117115388810634\n",
      "Epoch: 24000, WTA-Losses: 0.609635234490037, 0.6802572368457913, 0.6803449741378427\n",
      "Epoch: 25000, WTA-Losses: 0.6088908694460988, 0.6800591608136892, 0.680223803550005\n",
      "Epoch: 26000, WTA-Losses: 0.6049848280102015, 3.446710905261338, 0.6798229352161288\n",
      "Epoch: 27000, WTA-Losses: 0.6036913284063339, 0.6817814571931958, 0.6806866383478045\n",
      "Epoch: 28000, WTA-Losses: 0.6020573063865304, 0.6807140404582024, 0.6800191543772817\n",
      "Epoch: 29000, WTA-Losses: 0.6003682189434767, 0.6798453404605389, 0.6797649501562119\n",
      "Epoch: 30000, WTA-Losses: 0.5980172261595726, 0.6798681465089321, 0.6792990314215421\n",
      "Epoch: 31000, WTA-Losses: 0.5951524691134691, 0.6802919083312153, 0.679544309809804\n",
      "Epoch: 32000, WTA-Losses: 0.592238316565752, 0.6794264124035835, 0.6787731666937471\n",
      "New best 8: 0.24866779893636703\n",
      "Epoch: 33000, WTA-Losses: 0.590372629456222, 0.6803101596981287, 0.679114053696394\n",
      "Epoch: 34000, WTA-Losses: 0.5881707779243588, 0.6808500964567066, 0.6797382217049599\n",
      "Epoch: 35000, WTA-Losses: 0.5859704671502113, 0.679914380684495, 0.6798927153795957\n",
      "Epoch: 36000, WTA-Losses: 0.5840921407416463, 0.6797235914692282, 0.6793497024849057\n",
      "Epoch: 37000, WTA-Losses: 0.5823392306119204, 0.6785214121267199, 0.8001095127984882\n",
      "Epoch: 38000, WTA-Losses: 0.5819118332490325, 0.6793545732423664, 0.6791057961434126\n",
      "Epoch: 39000, WTA-Losses: 0.5802415393814444, 0.6802830851078033, 0.6799143585264683\n",
      "Epoch: 40000, WTA-Losses: 0.5785438801273703, 0.680398852147162, 0.6801396886110306\n",
      "Epoch: 41000, WTA-Losses: 0.5778802623972297, 0.680242975525558, 0.6803744212388992\n",
      "Epoch: 42000, WTA-Losses: 0.5769230245575309, 0.6803129270300269, 0.6804173647686839\n",
      "Epoch: 43000, WTA-Losses: 0.575893827304244, 0.6807750899642706, 0.680594556875527\n",
      "Epoch: 44000, WTA-Losses: 0.5756209964901209, 0.6795743711516261, 0.6792148464620114\n",
      "Epoch: 45000, WTA-Losses: 0.5751772945225239, 0.6804630597904324, 0.6795623051524162\n",
      "Epoch: 46000, WTA-Losses: 0.5742806545644998, 0.6805613019242883, 0.6803495328202843\n",
      "Epoch: 47000, WTA-Losses: 0.5737684332281351, 0.6797548716440797, 0.6799085520878434\n",
      "Epoch: 48000, WTA-Losses: 0.573181088924408, 0.6794591598585248, 0.6795005182027817\n",
      "Epoch: 49000, WTA-Losses: 0.5725061334818602, 10.719066302374005, 0.6793922693803907\n",
      "Epoch: 50000, WTA-Losses: 0.572522450901568, 0.6816239530146122, 1.1934468046799303\n",
      "New best 8: 0.24866564571857452\n",
      "Epoch: 51000, WTA-Losses: 0.5713811552375555, 0.6805332832187414, 0.6795889494940639\n",
      "Epoch: 52000, WTA-Losses: 0.5709138638377189, 0.6797611797451973, 0.6798646674752236\n",
      "Epoch: 53000, WTA-Losses: 0.5706478184759617, 0.679368299998343, 0.6798962146863341\n",
      "Epoch: 54000, WTA-Losses: 0.5705817602649331, 0.6795739608332515, 0.679458055227995\n",
      "Epoch: 55000, WTA-Losses: 0.5696521710380912, 0.6797128510773182, 0.6798995046392083\n",
      "Epoch: 56000, WTA-Losses: 0.5693462096750737, 0.6799897223114967, 0.6800996959656477\n",
      "Epoch: 57000, WTA-Losses: 0.5693347452506423, 0.6793268439620733, 0.6801320101171732\n",
      "Epoch: 58000, WTA-Losses: 0.5694156302139163, 0.6803114191144705, 0.6807058663442731\n",
      "Epoch: 59000, WTA-Losses: 0.568731930270791, 0.6793826861903072, 0.6798398700654507\n",
      "Epoch: 60000, WTA-Losses: 0.5677632519826293, 0.6803068594634533, 0.679862451761961\n",
      "Epoch: 61000, WTA-Losses: 0.5677213446870446, 0.6795838209614158, 0.6790150087550283\n",
      "Epoch: 62000, WTA-Losses: 0.567763155169785, 0.680284345395863, 0.6795508871749043\n",
      "New best 2: 0.001596086600329727\n",
      "Epoch: 63000, WTA-Losses: 0.5678224643915891, 0.6812931587696075, 0.6801755355075002\n",
      "Epoch: 64000, WTA-Losses: 0.5674310332685709, 0.6804500251188874, 0.6793066094443202\n",
      "Epoch: 65000, WTA-Losses: 0.5675529438629746, 0.6817850933223962, 0.6803230014070869\n",
      "Epoch: 66000, WTA-Losses: 0.5671470340713859, 0.6806491219103337, 1.6070188942179084\n",
      "Epoch: 67000, WTA-Losses: 0.5674214202910661, 0.6797017112523317, 0.6793771819099784\n",
      "Epoch: 68000, WTA-Losses: 0.5667323594912886, 0.6795031692087651, 0.6805233064293862\n",
      "Epoch: 69000, WTA-Losses: 0.5668399190232157, 0.6794281341433525, 0.6802612956762314\n",
      "Epoch: 70000, WTA-Losses: 0.5665757095515728, 0.679779431425035, 0.6799388955011963\n",
      "Epoch: 71000, WTA-Losses: 0.5661650191769004, 0.6794058813154698, 0.6790719024240971\n",
      "Epoch: 72000, WTA-Losses: 0.5657884406894446, 0.6792574186623096, 0.6790128456875681\n",
      "Epoch: 73000, WTA-Losses: 0.565579372189939, 0.6799173368215561, 0.6794459236338735\n",
      "Epoch: 74000, WTA-Losses: 0.565600250005722, 0.6797761095017195, 0.68006435957551\n",
      "Epoch: 75000, WTA-Losses: 0.5653663245737552, 0.6804940412193536, 0.680005543448031\n",
      "Epoch: 76000, WTA-Losses: 0.5654694857671857, 0.6810862108841539, 0.6802841946557164\n",
      "Epoch: 77000, WTA-Losses: 0.5646238795518875, 0.6801846105083823, 0.6797046739086509\n",
      "Epoch: 78000, WTA-Losses: 0.5650674058273435, 0.6792705455049872, 0.6791757333651185\n",
      "Epoch: 79000, WTA-Losses: 0.565327742062509, 0.67922542706877, 0.6798538103476167\n",
      "Epoch: 80000, WTA-Losses: 0.5646912627592683, 0.679132417857647, 0.6799513131156564\n",
      "New best 2: 0.0011558709084056318\n",
      "Epoch: 81000, WTA-Losses: 0.5647910330072046, 0.6800844495147467, 0.6803873277679086\n",
      "Epoch: 82000, WTA-Losses: 0.5652857423126697, 0.679554090924561, 0.6796770507544279\n",
      "New best 2: 0.0008795754984021187\n",
      "Epoch: 83000, WTA-Losses: 0.5643827069327235, 0.6808713304325938, 0.6806395694613456\n",
      "Epoch: 84000, WTA-Losses: 0.5641001860424876, 0.6804466854557395, 0.680218965433538\n",
      "Epoch: 85000, WTA-Losses: 0.5640699898824095, 0.6798503276109695, 0.6801148302406073\n",
      "Epoch: 86000, WTA-Losses: 0.5639215011298656, 0.6798796494454146, 0.6807817164063453\n",
      "Epoch: 87000, WTA-Losses: 0.5637040353789926, 0.6801295065507292, 0.6804596103653312\n",
      "Epoch: 88000, WTA-Losses: 0.5634638742431999, 0.6794406693503261, 0.6801587692201138\n",
      "Epoch: 89000, WTA-Losses: 0.5635724136084318, 0.6794369820952415, 0.6799036636427045\n",
      "Epoch: 90000, WTA-Losses: 0.5639305608049036, 0.6797903034985066, 0.6800180820897221\n",
      "Epoch: 91000, WTA-Losses: 0.5635718365460635, 0.6799821044430137, 0.6800582025647164\n",
      "Epoch: 92000, WTA-Losses: 0.5632310988083482, 0.6803150461614131, 0.6804823859259487\n",
      "Epoch: 93000, WTA-Losses: 0.5634076627120376, 0.6793735800310969, 0.6800370821505785\n",
      "Epoch: 94000, WTA-Losses: 0.5639457314983011, 0.6803522906005383, 0.6808362853005528\n",
      "Epoch: 95000, WTA-Losses: 0.5631174261718989, 0.6799346821755171, 0.680323646478355\n",
      "Epoch: 96000, WTA-Losses: 0.5640474966466427, 0.6801740366593003, 0.6802339284271002\n",
      "Epoch: 97000, WTA-Losses: 0.5627795472815633, 0.6800046131685377, 0.6807240433245897\n",
      "Epoch: 98000, WTA-Losses: 0.563093976162374, 0.6807652260065079, 0.6812853461354971\n",
      "Epoch: 99000, WTA-Losses: 0.5625992214232683, 0.679622170612216, 0.681130779787898\n",
      "Epoch: 100000, WTA-Losses: 0.5626930724829435, 0.6797581148669124, 0.6803362623602152\n",
      "Epoch: 101000, WTA-Losses: 0.5623847140446305, 0.6800725971236825, 0.6802692529633642\n",
      "Epoch: 102000, WTA-Losses: 0.5631782324463129, 0.6812769044265151, 0.6799242666587234\n",
      "Epoch: 103000, WTA-Losses: 0.5623995478227735, 0.6804488241448998, 0.6796252667680382\n",
      "Epoch: 104000, WTA-Losses: 0.5623202899247408, 0.6809187696352601, 0.6804962142258882\n",
      "Epoch: 105000, WTA-Losses: 0.5624650928899646, 0.6802187515124679, 0.6797438419237732\n",
      "Epoch: 106000, WTA-Losses: 0.562387986779213, 0.6803082482516766, 0.6801315724179149\n",
      "Epoch: 107000, WTA-Losses: 0.5615879168584943, 0.6803639965504408, 0.6803857702836394\n",
      "Epoch: 108000, WTA-Losses: 0.5623292653411627, 0.6804370661005378, 0.6799719812870025\n",
      "Epoch: 109000, WTA-Losses: 0.5618247864842415, 0.6801141202449799, 0.6798248380422592\n",
      "Epoch: 110000, WTA-Losses: 0.5621547999680042, 0.6798488055169583, 0.6800266641303897\n",
      "Epoch: 111000, WTA-Losses: 0.5618192379400134, 0.680178269945085, 0.6799931779056787\n",
      "Epoch: 112000, WTA-Losses: 0.5616989406570793, 0.6804079570099711, 0.6797843130975961\n",
      "Epoch: 113000, WTA-Losses: 0.5618075901567936, 0.680408159956336, 0.6799350850507617\n",
      "Epoch: 114000, WTA-Losses: 0.5618759982138872, 0.6803977096378804, 0.6795918123647571\n",
      "Epoch: 115000, WTA-Losses: 0.561720626167953, 0.6797351601943373, 0.6794314357861877\n",
      "Epoch: 116000, WTA-Losses: 0.5618696474060416, 0.6791779613345862, 0.6800130323395133\n",
      "Epoch: 117000, WTA-Losses: 0.5612045670747757, 0.6801630010902882, 0.6800951378047466\n",
      "Epoch: 118000, WTA-Losses: 0.5611404973641038, 0.6799742759615183, 0.6803091753348708\n",
      "Epoch: 119000, WTA-Losses: 0.561489133387804, 0.6802749046087265, 0.6800463278591633\n",
      "Epoch: 120000, WTA-Losses: 0.5616964708119631, 1264.5163106113448, 0.6797216303870082\n",
      "Epoch: 121000, WTA-Losses: 0.5616051280274987, 0.7526012510061264, 0.6808672237694263\n",
      "Epoch: 122000, WTA-Losses: 0.5612399134263396, 0.7300237236917019, 0.6801041402518749\n",
      "Epoch: 123000, WTA-Losses: 0.5612940830513835, 0.7150184181779623, 0.6794254377484321\n",
      "Epoch: 124000, WTA-Losses: 0.5615961315929889, 0.6959059786349535, 0.6798675794750452\n",
      "Epoch: 125000, WTA-Losses: 0.5613713400959969, 0.6913506214767694, 0.6801229234859347\n",
      "Epoch: 126000, WTA-Losses: 0.5611076766848564, 0.6857477078214288, 0.6803418791666627\n",
      "Epoch: 127000, WTA-Losses: 0.5609813574254513, 0.684332490324974, 0.6807912989780307\n",
      "Epoch: 128000, WTA-Losses: 0.5608653781116009, 0.6829084125980734, 0.6803778066560626\n",
      "Epoch: 129000, WTA-Losses: 0.5609370832070708, 0.6813923540636897, 3.084651979893446\n",
      "Epoch: 130000, WTA-Losses: 0.5611605907082557, 0.6808060213029384, 0.6818809495940804\n",
      "Epoch: 131000, WTA-Losses: 0.5605317416191101, 0.6804418654814363, 0.6817373410165309\n",
      "Epoch: 132000, WTA-Losses: 0.5602287614196539, 0.6802970272377133, 0.6812827806100249\n",
      "Epoch: 133000, WTA-Losses: 0.5610437778458, 0.6794257760941982, 0.679970212213695\n",
      "Epoch: 134000, WTA-Losses: 0.5607449175938964, 0.6795110580027104, 0.6796605402678251\n",
      "Epoch: 135000, WTA-Losses: 0.5605350644141436, 0.6797376534640789, 0.679919769667089\n",
      "Epoch: 136000, WTA-Losses: 0.5607149003818631, 0.6799302927702665, 0.6798779596313834\n",
      "Epoch: 137000, WTA-Losses: 0.5602429072335362, 0.6791144187524915, 0.6791723776385188\n",
      "Epoch: 138000, WTA-Losses: 0.5599846154972911, 0.6793405478894711, 0.6794978189393878\n",
      "Epoch: 139000, WTA-Losses: 0.5601927893832326, 0.6795681250020862, 0.6796874057203531\n",
      "Epoch: 140000, WTA-Losses: 0.5601818597540259, 0.6798654728606344, 0.6803161408826709\n",
      "Epoch: 141000, WTA-Losses: 0.560275100722909, 0.6805480867475271, 0.680737679451704\n",
      "Epoch: 142000, WTA-Losses: 0.5593760703131556, 115.77389446618407, 0.6811303332671523\n",
      "New best 2: 0.0007579243101645261\n",
      "Epoch: 143000, WTA-Losses: 0.5598059705644846, 0.6918897849172354, 0.6805691601410508\n",
      "Epoch: 144000, WTA-Losses: 0.5600238857120275, 0.6860801871567964, 0.680407834418118\n",
      "Epoch: 145000, WTA-Losses: 0.5602340599447488, 0.6831975153163076, 0.6792897036522627\n",
      "Epoch: 146000, WTA-Losses: 0.5599442048892379, 0.682151211053133, 0.679304397135973\n",
      "Epoch: 147000, WTA-Losses: 0.5601921388059854, 0.6810388048142195, 0.679087897926569\n",
      "Epoch: 148000, WTA-Losses: 0.559601056754589, 0.6801000558063388, 0.6794734540060162\n",
      "Epoch: 149000, WTA-Losses: 0.5595430605188012, 0.6801472948938608, 0.6801812203899026\n",
      "Epoch: 150000, WTA-Losses: 0.5593707807362079, 0.6799053095728159, 0.680039409570396\n",
      "Epoch: 151000, WTA-Losses: 0.5595990163683892, 0.6804456306248903, 0.6799212336018682\n",
      "Epoch: 152000, WTA-Losses: 0.5592218885719776, 0.6806104865074157, 0.6804428619220853\n",
      "New best 2: 0.0006756268558092415\n",
      "Epoch: 153000, WTA-Losses: 0.5592686264887452, 0.6802034311667084, 0.6803636685088277\n",
      "Epoch: 154000, WTA-Losses: 0.5594087926894427, 0.679123641833663, 0.6796517188847065\n",
      "Epoch: 155000, WTA-Losses: 0.5590410087034107, 0.6799040059670806, 0.6802265272065997\n",
      "Epoch: 156000, WTA-Losses: 0.5593138813301921, 0.6803244365602732, 0.6804782101660967\n",
      "Epoch: 157000, WTA-Losses: 0.5592813448384404, 0.6803512562960387, 0.6798761111721396\n",
      "Epoch: 158000, WTA-Losses: 0.5588843518197536, 0.6799371890500188, 0.6801188673004508\n",
      "Epoch: 159000, WTA-Losses: 0.5591171501725912, 0.6793612353503704, 0.6792101059406995\n",
      "Epoch: 160000, WTA-Losses: 0.558800318017602, 0.6801351265013218, 0.6800924796834588\n",
      "Epoch: 161000, WTA-Losses: 0.5591947934404016, 0.680656236924231, 0.6806467355042696\n",
      "Epoch: 162000, WTA-Losses: 0.5589919149056077, 0.6800204705446958, 0.6800366919636727\n",
      "Epoch: 163000, WTA-Losses: 0.5591140888854861, 0.6797263242676854, 0.6797683356106281\n",
      "Epoch: 164000, WTA-Losses: 0.559273396782577, 0.6802638001963496, 0.6803958102464676\n",
      "New best 2: 0.0005804421380162239\n",
      "Epoch: 165000, WTA-Losses: 0.5589655226767063, 0.6796230160146952, 0.6801120152771473\n",
      "Epoch: 166000, WTA-Losses: 0.5587504887357354, 0.6793734171912074, 0.6800265239551664\n",
      "Epoch: 167000, WTA-Losses: 0.5591210414394736, 0.6792590188011527, 0.6802328661382199\n",
      "Epoch: 168000, WTA-Losses: 0.5589351362660527, 0.680293082639575, 0.6802473852708936\n",
      "Epoch: 169000, WTA-Losses: 0.5585526739954948, 0.6801752061322331, 0.6794967358484865\n",
      "Epoch: 170000, WTA-Losses: 0.5589429445415736, 0.6805420060679317, 0.6800052045434714\n",
      "Epoch: 171000, WTA-Losses: 0.5588041921779513, 0.6809702127575874, 0.679769635528326\n",
      "Epoch: 172000, WTA-Losses: 0.5586776561662554, 0.6797466115504504, 0.6790505211204291\n",
      "Epoch: 173000, WTA-Losses: 0.559086906671524, 0.6797180951088667, 0.6796106196269392\n",
      "Epoch: 174000, WTA-Losses: 0.5583771738409996, 0.6799481621161103, 0.6800361138060689\n",
      "Epoch: 175000, WTA-Losses: 0.5583554173782468, 0.6794490176066756, 0.6796489379554987\n",
      "Epoch: 176000, WTA-Losses: 0.5588511279150844, 0.6794928190782666, 0.6795444848239421\n",
      "Epoch: 177000, WTA-Losses: 0.5585747993290424, 0.6794372234940529, 0.6804601663500071\n",
      "Epoch: 178000, WTA-Losses: 0.5584315389841795, 0.6794919582158327, 0.6805261328294874\n",
      "Epoch: 179000, WTA-Losses: 0.5582316095605493, 0.6805937208235264, 0.6808857764974237\n",
      "Epoch: 180000, WTA-Losses: 0.5582781237959862, 0.6802022254168987, 0.6796083453297616\n",
      "Epoch: 181000, WTA-Losses: 0.5584292853996158, 0.6800459591075778, 0.6802301224693657\n",
      "Epoch: 182000, WTA-Losses: 0.5581726658269763, 0.6793469327688217, 0.6800755345746875\n",
      "Epoch: 183000, WTA-Losses: 0.5582932758331299, 0.6793791542202234, 0.6791636615842581\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/ablation/sines_4/2_4_8/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "if not os.path.exists(f'{root_folder}img/'):\n",
    "    os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 5_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss2, avg_loss4, avg_loss8 = 0, 0, 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_vl2, min_vl4, min_vl8 = 1000000, 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "tl2, tl4, tl8 = [], [], []\n",
    "ve2, ve4, ve8 = [], [], []\n",
    "\n",
    "# wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "# wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss2, epoch_loss4, epoch_loss8 = 0, 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        obs, tar_x, tar_y = get_batch(x, y, traj_ids[i], device)\n",
    "\n",
    "        pred2, gate2 = model2(obs, tar_x)\n",
    "        loss2, nll2 = model2.loss(pred2, gate2, tar_y)\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        pred4, gate4 = model4(obs, tar_x)\n",
    "        loss4, nll4 = model4.loss(pred4, gate4, tar_y)\n",
    "        loss4.backward()\n",
    "        optimizer4.step()\n",
    "\n",
    "        pred8, gate8 = model8(obs, tar_x)\n",
    "        loss8, nll8 = model8.loss(pred8, gate8, tar_y)\n",
    "        loss8.backward()\n",
    "        optimizer8.step()\n",
    "\n",
    "        epoch_loss2 += nll2.item()\n",
    "        epoch_loss4 += nll4.item()\n",
    "        epoch_loss8 += nll8.item()\n",
    "\n",
    "    tl2.append(epoch_loss2)\n",
    "    tl4.append(epoch_loss4)\n",
    "    tl8.append(epoch_loss8)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss2, val_loss4, val_loss8 = 0, 0, 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_wta, g_wta = model2(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss2 += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "                p_wta, g_wta = model4(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss4 += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "                p_wta, g_wta = model8(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss8 += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "            ve2.append(val_loss2)\n",
    "            ve4.append(val_loss4)\n",
    "            ve8.append(val_loss8)\n",
    "\n",
    "            if val_loss2 < min_vl2:\n",
    "                min_vl2 = val_loss2\n",
    "                print(f'New best 2: {min_vl2}')\n",
    "                torch.save(model2.state_dict(), f'{root_folder}saved_models/wta2.pt')\n",
    "\n",
    "            if val_loss4 < min_vl4:\n",
    "                min_vl4 = val_loss4\n",
    "                print(f'New best 4: {min_vl4}')\n",
    "                torch.save(model4.state_dict(), f'{root_folder}saved_models/wta4.pt')\n",
    "\n",
    "            if val_loss8 < min_vl8:\n",
    "                min_vl8 = val_loss8\n",
    "                print(f'New best 8: {min_vl8}')\n",
    "                torch.save(model8.state_dict(), f'{root_folder}saved_models/wta8.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss2 += epoch_loss2/epoch_iter\n",
    "    avg_loss4 += epoch_loss4/epoch_iter\n",
    "    avg_loss8 += epoch_loss8/epoch_iter\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, WTA-Losses: {}, {}, {}\".format(epoch, avg_loss2/val_per_epoch, avg_loss4/val_per_epoch, avg_loss8/val_per_epoch))\n",
    "        avg_loss2, avg_loss4, avg_loss8 = 0, 0, 0\n",
    "\n",
    "# torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "# torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
