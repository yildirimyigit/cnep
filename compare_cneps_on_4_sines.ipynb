{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.cnp import CNP\n",
    "from models.cnep import CNEP\n",
    "\n",
    "from data.data_generators import *\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_max, m_max = 10, 10  # max number of points in context set and target set\n",
    "\n",
    "t_steps = 200\n",
    "num_demos = 128\n",
    "num_classes = 4\n",
    "num_indiv = num_demos//num_classes  # number of demos per class\n",
    "noise_clip = 0.0\n",
    "dx, dy = 1, 1\n",
    "\n",
    "num_val = 32\n",
    "num_val_indiv = num_val//num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([128, 200, 1]) Y: torch.Size([128, 200, 1]) VX: torch.Size([32, 200, 1]) VY: torch.Size([32, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "colors = [sns.color_palette('tab10')[0], sns.color_palette('tab10')[1], sns.color_palette('tab10')[2], sns.color_palette('tab10')[3]]\n",
    "sns.set_palette('tab10')\n",
    "\n",
    "x = torch.linspace(0, 1, 200).repeat(num_indiv, 1)\n",
    "y = torch.zeros(num_demos, t_steps, dy)\n",
    "\n",
    "vx = torch.linspace(0, 1, 200).repeat(num_val_indiv, 1)\n",
    "vy = torch.zeros(num_val, t_steps, dy)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    start_ind = i*num_indiv\n",
    "    coeff = (i+1)/2*torch.pi\n",
    "    y[start_ind:start_ind+num_indiv] = (torch.unsqueeze(generate_sin(x*coeff), 2) +1)/2.0\n",
    "\n",
    "    noise = torch.unsqueeze(torch.clamp(torch.randn(vx.shape)*1e-4**0.5, min=0) - noise_clip, -1)\n",
    "\n",
    "    start_ind = i*num_val_indiv\n",
    "    vy[start_ind:start_ind+num_val_indiv] = y[start_ind:start_ind+num_val_indiv].clone() + noise\n",
    "\n",
    "x = torch.unsqueeze(x.repeat(num_classes, 1), 2)  # since dx = 1\n",
    "vx = torch.unsqueeze(vx.repeat(num_classes, 1), 2)\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i in range(num_demos):\n",
    "#     plt.plot(x[i, :, 0].cpu(), y[i, :, 0].cpu(), label=f'Sine Wave {i+1}', linewidth=2.0, color=colors[i])\n",
    "#     # plt.plot(vx[i, :, 0].cpu(), vy[i, :, 0].cpu(), 'k', alpha=0.5)\n",
    "\n",
    "# plt.legend(loc='lower left', fontsize=14)\n",
    "# plt.grid(True)\n",
    "# plt.xlabel('Time (s)', fontsize=14)\n",
    "# plt.ylabel('Amplitude', fontsize=14)\n",
    "# plt.title(f'Sine Wave of 3 Different Frequencies', fontsize=16)\n",
    "# plt.savefig(f'/home/yigit/papers/yildirim_23_ral/fig/3.png', bbox_inches='tight')\n",
    "\n",
    "x, y = x.to(device), y.to(device)\n",
    "# x1, y1 = x.to(device_cnp), y.to(device_cnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((batch_size, n_max, dx+dy), dtype=torch.float32, device=device)\n",
    "tar_x = torch.zeros((batch_size, m_max, dx), dtype=torch.float32, device=device)\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    obs.fill_(0)\n",
    "    tar_x.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "        m = torch.randint(1, m_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "        \n",
    "        obs[i, :n, :dx] = traj[n_ids]\n",
    "        obs[i, :n, dx:] = (n_ids/t_steps).unsqueeze(1)\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x[i, :m] = traj[m_ids]\n",
    "        tar_y[i, :m] = (m_ids/t_steps).unsqueeze(1)\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "val_obs = torch.zeros((batch_size, n_max, dx+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dx), dtype=torch.float32, device=device)\n",
    "val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_val_batch(t: list, traj_ids: list):\n",
    "    val_obs.fill_(0)\n",
    "    val_tar_x.fill_(0)\n",
    "    val_tar_y.fill_(0)\n",
    "    val_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "        \n",
    "        val_obs[i, :n, :dx] = traj[n_ids]\n",
    "        val_obs[i, :n, dx:] = (n_ids/t_steps).unsqueeze(1)\n",
    "        val_obs_mask[i, :n] = True\n",
    "        \n",
    "        val_tar_x[i] = traj[m_ids]\n",
    "        val_tar_y[i] = (m_ids/t_steps).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnep2: 22166\n",
      "cnep4: 22028\n",
      "cnep8: 22296\n"
     ]
    }
   ],
   "source": [
    "model2_ = CNEP(1, 1, n_max, n_max, [64,64], num_decoders=2, decoder_hidden_dims=[130, 130], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "optimizer2 = torch.optim.Adam(lr=1e-4, params=model2_.parameters())\n",
    "\n",
    "model4_ = CNEP(1, 1, n_max, n_max, [64,64], num_decoders=4, decoder_hidden_dims=[64, 64], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "optimizer4 = torch.optim.Adam(lr=1e-4, params=model4_.parameters())\n",
    "\n",
    "model8_ = CNEP(1, 1, n_max, n_max, [64,64], num_decoders=8, decoder_hidden_dims=[32, 32], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "optimizer8 = torch.optim.Adam(lr=1e-4, params=model8_.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"cnep2:\", get_parameter_count(model2_))\n",
    "print(\"cnep4:\", get_parameter_count(model4_))\n",
    "print(\"cnep8:\", get_parameter_count(model8_))\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model2, model4, model8 = torch.compile(model2_), torch.compile(model4_), torch.compile(model8_)\n",
    "else:\n",
    "    model2, model4, model8 = model2_, model4_, model8_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best 2: 0.21449580788612366\n",
      "New best 4: 0.45854508876800537\n",
      "New best 8: 0.2942187786102295\n",
      "Bests: 0.21449580788612366, 0.45854508876800537, 0.2942187786102295\n",
      "Epoch: 0, CNEP Losses: 0.0005807961225509643, 0.000682275727391243, 0.0005753793120384217\n",
      "New best 2: 6.795772122814014e-08\n",
      "New best 4: 7.988154138161008e-09\n",
      "New best 8: 3.6754208565525914e-08\n",
      "Bests: 6.795772122814014e-08, 7.988154138161008e-09, 3.6754208565525914e-08\n",
      "Epoch: 1000, CNEP Losses: -3.777244910158217, -3.5923025856660677, -3.440753865755163\n",
      "New best 8: 2.2699781609958336e-08\n",
      "Bests: 6.795772122814014e-08, 7.988154138161008e-09, 2.2699781609958336e-08\n",
      "Epoch: 2000, CNEP Losses: -4.84794056884246, -5.019738638137468, -5.151848404187709\n",
      "New best 4: 6.573345423532828e-09\n",
      "Bests: 6.795772122814014e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 3000, CNEP Losses: -5.247668891593814, -5.135708513424033, -5.244077236799989\n",
      "New best 2: 3.364261047522632e-08\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 4000, CNEP Losses: -5.360890556812286, -5.115533463357017, -5.2894959409111175\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 5000, CNEP Losses: -5.347405089855194, -5.130892579189196, -5.237197922669351\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 6000, CNEP Losses: -5.323503587901592, -5.232240925908089, -5.257543494489044\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 7000, CNEP Losses: -5.282557623267174, -5.450574448384344, -5.229924672137946\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 8000, CNEP Losses: -5.43492574443412, -5.689762253284455, -5.213841418255121\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.2699781609958336e-08\n",
      "Epoch: 9000, CNEP Losses: -5.454833416841924, -5.829838288009166, -5.220891014840454\n",
      "New best 8: 2.060866144404372e-08\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 2.060866144404372e-08\n",
      "Epoch: 10000, CNEP Losses: -5.3943392380476, -5.831719119500369, -5.3025626950403675\n",
      "New best 8: 6.216175574280669e-09\n",
      "Bests: 3.364261047522632e-08, 6.573345423532828e-09, 6.216175574280669e-09\n",
      "Epoch: 11000, CNEP Losses: -5.359219006359577, -5.883758366197347, -5.3104165114164354\n",
      "New best 2: 2.3442289887043444e-08\n",
      "Bests: 2.3442289887043444e-08, 6.573345423532828e-09, 6.216175574280669e-09\n",
      "Epoch: 12000, CNEP Losses: -5.3096352376937865, -5.918041862040758, -5.3664359056409445\n",
      "Bests: 2.3442289887043444e-08, 6.573345423532828e-09, 6.216175574280669e-09\n",
      "Epoch: 13000, CNEP Losses: -5.412123762220144, -5.930852036654949, -5.468250552317127\n",
      "Bests: 2.3442289887043444e-08, 6.573345423532828e-09, 6.216175574280669e-09\n",
      "Epoch: 14000, CNEP Losses: -5.405939289748669, -5.9555005427598955, -5.64196211582981\n",
      "Bests: 2.3442289887043444e-08, 6.573345423532828e-09, 6.216175574280669e-09\n",
      "Epoch: 15000, CNEP Losses: -5.499132865428924, -5.971220662064851, -5.660162331491708\n",
      "New best 4: 2.4080855087760256e-09\n",
      "Bests: 2.3442289887043444e-08, 2.4080855087760256e-09, 6.216175574280669e-09\n",
      "Epoch: 16000, CNEP Losses: -5.508686834812164, -5.994377450972795, -5.729545297356323\n",
      "Bests: 2.3442289887043444e-08, 2.4080855087760256e-09, 6.216175574280669e-09\n",
      "Epoch: 17000, CNEP Losses: -5.508066903874278, -6.008848168373108, -5.726640268364688\n",
      "Bests: 2.3442289887043444e-08, 2.4080855087760256e-09, 6.216175574280669e-09\n",
      "Epoch: 18000, CNEP Losses: -5.474236633181572, -5.987037260949612, -5.689891478363425\n",
      "Bests: 2.3442289887043444e-08, 2.4080855087760256e-09, 6.216175574280669e-09\n",
      "Epoch: 19000, CNEP Losses: -5.587647361516953, -6.015057365357876, -5.786328229549341\n",
      "New best 2: 1.7653100314873882e-08\n",
      "New best 8: 3.4517548819223975e-09\n",
      "Bests: 1.7653100314873882e-08, 2.4080855087760256e-09, 3.4517548819223975e-09\n",
      "Epoch: 20000, CNEP Losses: -5.598631130933762, -5.956326787158847, -5.760304150896147\n",
      "New best 8: 2.4892550243293954e-09\n",
      "Bests: 1.7653100314873882e-08, 2.4080855087760256e-09, 2.4892550243293954e-09\n",
      "Epoch: 21000, CNEP Losses: -5.6519799590110775, -6.012874930381775, -5.748473418744281\n",
      "Bests: 1.7653100314873882e-08, 2.4080855087760256e-09, 2.4892550243293954e-09\n",
      "Epoch: 22000, CNEP Losses: -5.562504885196685, -5.981572604179382, -5.732116747682215\n",
      "Bests: 1.7653100314873882e-08, 2.4080855087760256e-09, 2.4892550243293954e-09\n",
      "Epoch: 23000, CNEP Losses: -5.581926751613617, -6.006073953688145, -5.857667122925632\n",
      "Bests: 1.7653100314873882e-08, 2.4080855087760256e-09, 2.4892550243293954e-09\n",
      "Epoch: 24000, CNEP Losses: -5.612802863240242, -6.028919907420874, -5.797472023987211\n",
      "Bests: 1.7653100314873882e-08, 2.4080855087760256e-09, 2.4892550243293954e-09\n",
      "Epoch: 25000, CNEP Losses: -5.628662048697471, -6.027086731493473, -5.781740253701806\n",
      "New best 4: 2.337260829321508e-09\n",
      "Bests: 1.7653100314873882e-08, 2.337260829321508e-09, 2.4892550243293954e-09\n",
      "Epoch: 26000, CNEP Losses: -5.592649350166321, -6.032177480280399, -5.756803644679486\n",
      "New best 2: 1.6893153542696382e-08\n",
      "Bests: 1.6893153542696382e-08, 2.337260829321508e-09, 2.4892550243293954e-09\n",
      "Epoch: 27000, CNEP Losses: -5.702348797559738, -6.01517119204998, -5.809930314555764\n",
      "Bests: 1.6893153542696382e-08, 2.337260829321508e-09, 2.4892550243293954e-09\n",
      "Epoch: 28000, CNEP Losses: -5.842296963214874, -5.9623623012304305, -5.7618802974168215\n",
      "Bests: 1.6893153542696382e-08, 2.337260829321508e-09, 2.4892550243293954e-09\n",
      "Epoch: 29000, CNEP Losses: -5.832719730913639, -5.981586357474327, -5.825095130886883\n",
      "New best 2: 1.3060843073731121e-08\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 2.4892550243293954e-09\n",
      "Epoch: 30000, CNEP Losses: -5.876707534432411, -6.011351934671402, -5.754538219651208\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 2.4892550243293954e-09\n",
      "Epoch: 31000, CNEP Losses: -5.812721424937248, -5.903689637899399, -5.831864030249417\n",
      "New best 8: 7.084685615943442e-10\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 32000, CNEP Losses: -5.767399834036827, -5.977023604631424, -5.835505716510117\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 33000, CNEP Losses: -5.9346554110050205, -5.986581666231156, -5.834566984847188\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 34000, CNEP Losses: -5.9344382802248, -5.979526314616203, -5.802147094707936\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 35000, CNEP Losses: -5.971974581480026, -5.930830398917198, -5.832021981202066\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 36000, CNEP Losses: -5.918383867383003, -5.983908193230629, -5.8683392605185505\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 37000, CNEP Losses: -5.98962869155407, -5.9164079591035845, -5.86585894805193\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 38000, CNEP Losses: -5.980740134239197, -5.90401539850235, -5.883785666611046\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 39000, CNEP Losses: -6.0262115303874015, -5.992970279455185, -5.876022737076506\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 40000, CNEP Losses: -5.9495398131608965, -6.037287662148476, -5.898501317141577\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 41000, CNEP Losses: -5.935805428981781, -6.065539940357208, -5.900415075190366\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 42000, CNEP Losses: -5.899165889620781, -5.995759730696678, -5.907675842924625\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 43000, CNEP Losses: -6.120379841089249, -6.00065147948265, -5.977111832072027\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 44000, CNEP Losses: -5.992320955455304, -6.041916393756867, -5.9755205457359555\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 45000, CNEP Losses: -6.1375050730705265, -6.06579876613617, -6.010862060945481\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 46000, CNEP Losses: -6.033547344446182, -6.0827359907627105, -5.976872393839061\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 47000, CNEP Losses: -6.083332217335701, -6.05840281021595, -5.942849363305955\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 48000, CNEP Losses: -6.177880797475576, -6.086512172222138, -5.93854183100164\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 7.084685615943442e-10\n",
      "Epoch: 49000, CNEP Losses: -6.081831582218409, -6.149168362617493, -5.870699839216657\n",
      "New best 8: 5.755765308812499e-10\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 50000, CNEP Losses: -6.003296465158463, -6.19297531235218, -5.895863981887699\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 51000, CNEP Losses: -6.188969363808632, -6.138522924900055, -6.022233296707273\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 52000, CNEP Losses: -6.239352200269699, -6.168552362203598, -5.88843019467406\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 53000, CNEP Losses: -6.106056024670601, -6.032027106456459, -5.852806321851909\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 54000, CNEP Losses: -6.180633087873459, -6.238548758268356, -5.966842121630907\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 55000, CNEP Losses: -6.153950263023376, -6.229851787269116, -5.881746365703642\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 56000, CNEP Losses: -6.25287723377347, -6.249521312713623, -5.952501410325989\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 57000, CNEP Losses: -6.26363525891304, -6.240117487430573, -5.931950877301395\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 58000, CNEP Losses: -6.254424214363098, -6.325679916381836, -5.959456749234349\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 59000, CNEP Losses: -6.249485596880317, -6.275660876989365, -5.986618561515584\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 60000, CNEP Losses: -6.307993391394615, -6.353225857138634, -5.9899234671182935\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 61000, CNEP Losses: -6.267624128520489, -6.273479759573936, -5.979015735605964\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 62000, CNEP Losses: -6.248080949425697, -6.236174963057041, -5.986994958622177\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 63000, CNEP Losses: -6.284913342952728, -6.213820722937584, -6.052620430244366\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 64000, CNEP Losses: -6.299285080483183, -6.2836682270765305, -6.016690552527085\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 65000, CNEP Losses: -6.318556712388992, -6.254605190694332, -5.931323299311101\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 66000, CNEP Losses: -6.379697236448527, -6.3096411049366, -6.018911292193457\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 67000, CNEP Losses: -6.299880053639412, -6.286569211483002, -6.019668238950893\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 68000, CNEP Losses: -6.300111538529396, -6.417421286433935, -6.0296332974284885\n",
      "Bests: 1.3060843073731121e-08, 2.337260829321508e-09, 5.755765308812499e-10\n",
      "Epoch: 69000, CNEP Losses: -6.268505963683128, -6.368612756729126, -6.070609908029437\n",
      "New best 2: 1.2522350267829552e-08\n",
      "New best 8: 4.744788451027659e-10\n",
      "Bests: 1.2522350267829552e-08, 2.337260829321508e-09, 4.744788451027659e-10\n",
      "Epoch: 70000, CNEP Losses: -6.3089208393096925, -6.367110552072525, -6.025066117847338\n",
      "Bests: 1.2522350267829552e-08, 2.337260829321508e-09, 4.744788451027659e-10\n",
      "Epoch: 71000, CNEP Losses: -6.456290349826217, -6.311994771838188, -6.115331013924675\n",
      "Bests: 1.2522350267829552e-08, 2.337260829321508e-09, 4.744788451027659e-10\n",
      "Epoch: 72000, CNEP Losses: -6.354099541306495, -6.319284480690956, -5.974721236121142\n",
      "Bests: 1.2522350267829552e-08, 2.337260829321508e-09, 4.744788451027659e-10\n",
      "Epoch: 73000, CNEP Losses: -6.233747391343117, -6.340880141854286, -6.016964924111963\n",
      "Bests: 1.2522350267829552e-08, 2.337260829321508e-09, 4.744788451027659e-10\n",
      "Epoch: 74000, CNEP Losses: -6.371503428220749, -6.320443557500839, -6.126198918672278\n",
      "New best 8: 4.3347425648931903e-10\n",
      "Bests: 1.2522350267829552e-08, 2.337260829321508e-09, 4.3347425648931903e-10\n",
      "Epoch: 75000, CNEP Losses: -6.404279115468263, -6.330074582517147, -6.111610182859003\n",
      "New best 4: 1.6788965773173459e-09\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 4.3347425648931903e-10\n",
      "Epoch: 76000, CNEP Losses: -6.437934659600258, -6.367135098874569, -6.016024088267237\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 4.3347425648931903e-10\n",
      "Epoch: 77000, CNEP Losses: -6.473960322856903, -6.421721593126654, -6.011153540957719\n",
      "New best 8: 1.716722597411291e-10\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.716722597411291e-10\n",
      "Epoch: 78000, CNEP Losses: -6.3911778426468375, -6.389984557777643, -6.095387793958653\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.716722597411291e-10\n",
      "Epoch: 79000, CNEP Losses: -6.500393702030182, -6.378726596951485, -6.116846558889375\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.716722597411291e-10\n",
      "Epoch: 80000, CNEP Losses: -6.51721288228035, -6.416165108829737, -6.1442917334358205\n",
      "New best 8: 1.129726867610259e-10\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 81000, CNEP Losses: -6.555802798390388, -6.31446194344759, -6.089376327950507\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 82000, CNEP Losses: -6.551646783620119, -6.385487234890461, -6.131215799002908\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 83000, CNEP Losses: -6.485519464015961, -6.3338128503263, -6.106267126943916\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 84000, CNEP Losses: -6.467633185565472, -6.408598876208067, -6.1321811441505565\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 85000, CNEP Losses: -6.491919716984033, -6.438448103398085, -6.14512516248785\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 86000, CNEP Losses: -6.495404692769051, -6.433554339289665, -6.160459473228082\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 87000, CNEP Losses: -6.422327182650566, -6.502352708995343, -6.117010584913194\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 88000, CNEP Losses: -6.454648957967758, -6.4352341264672575, -6.094213995993138\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 89000, CNEP Losses: -6.595564162492752, -6.382857178986073, -6.153384109545499\n",
      "Bests: 1.2522350267829552e-08, 1.6788965773173459e-09, 1.129726867610259e-10\n",
      "Epoch: 90000, CNEP Losses: -6.531110987782478, -6.4777509852647785, -6.132367182012647\n",
      "New best 4: 7.306097393744437e-10\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 91000, CNEP Losses: -6.542236227631569, -6.577853898271918, -6.128237228085287\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 92000, CNEP Losses: -6.530642155885697, -6.514502059519291, -6.204502851840108\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 93000, CNEP Losses: -6.529624039888382, -6.575096999287605, -6.103448299054056\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 94000, CNEP Losses: -6.644494020700455, -6.571737070560455, -6.230673579340801\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 95000, CNEP Losses: -6.603291382193565, -6.502573311209678, -6.23811354906857\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 96000, CNEP Losses: -6.614670843362808, -6.569890490114689, -6.1542793850423765\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 97000, CNEP Losses: -6.636353107631207, -6.550017839334905, -6.2769528138134625\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 98000, CNEP Losses: -6.477476627349853, -6.522733412861824, -6.232921593435108\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 99000, CNEP Losses: -6.551736404001713, -6.405216667175293, -6.28524704611674\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 100000, CNEP Losses: -6.663058525323867, -6.530814623296261, -6.3132193311734595\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 101000, CNEP Losses: -6.705155039966106, -6.553954303979873, -6.231040874090046\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 102000, CNEP Losses: -6.6335074027776715, -6.5850484783947465, -6.17051114711538\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 103000, CNEP Losses: -6.69399606359005, -6.6042287907861175, -6.31763337629661\n",
      "Bests: 1.2522350267829552e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 104000, CNEP Losses: -6.626982323884964, -6.620090526133776, -6.262720176791772\n",
      "New best 2: 1.2521122805253526e-08\n",
      "Bests: 1.2521122805253526e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 105000, CNEP Losses: -6.650356546640396, -6.495128809630871, -6.311751880202443\n",
      "Bests: 1.2521122805253526e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 106000, CNEP Losses: -6.708938364982605, -6.619614817768335, -6.2679126609209925\n",
      "Bests: 1.2521122805253526e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 107000, CNEP Losses: -6.816885512709618, -6.673122942149639, -6.327622421175241\n",
      "Bests: 1.2521122805253526e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 108000, CNEP Losses: -6.852328859090805, -6.722308685421944, -6.282159443918907\n",
      "Bests: 1.2521122805253526e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 109000, CNEP Losses: -6.699849652677774, -6.729607903599739, -6.305992929426953\n",
      "Bests: 1.2521122805253526e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 110000, CNEP Losses: -6.751535185098648, -6.6863702007532115, -6.351823164891451\n",
      "New best 2: 1.2046027286771732e-08\n",
      "Bests: 1.2046027286771732e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 111000, CNEP Losses: -6.831106757104397, -6.763844399333, -6.332018916441128\n",
      "Bests: 1.2046027286771732e-08, 7.306097393744437e-10, 1.129726867610259e-10\n",
      "Epoch: 112000, CNEP Losses: -6.756231038361788, -6.7664725624322895, -6.296838065817952\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m traj_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:batch_size\u001b[38;5;241m*\u001b[39mepoch_iter]\u001b[38;5;241m.\u001b[39mchunk(epoch_iter)  \u001b[38;5;66;03m# [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_iter):\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mprepare_masked_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraj_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     optimizer2\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m     pred2, gate2 \u001b[38;5;241m=\u001b[39m model2(obs, tar_x, obs_mask)\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mprepare_masked_batch\u001b[0;34m(t, traj_ids)\u001b[0m\n\u001b[1;32m     26\u001b[0m obs_mask[i, :n] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m tar_x[i, :m] \u001b[38;5;241m=\u001b[39m traj[m_ids]\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtar_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (m_ids\u001b[38;5;241m/\u001b[39mt_steps)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m tar_mask[i, :m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/ablation/sines_4/2_4_8/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "if not os.path.exists(f'{root_folder}img/'):\n",
    "    os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 5_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss2, avg_loss4, avg_loss8 = 0, 0, 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_vl2, min_vl4, min_vl8 = 1000000, 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "tl2, tl4, tl8 = [], [], []\n",
    "ve2, ve4, ve8 = [], [], []\n",
    "\n",
    "cnep_tl_path = f'{root_folder}cnep_training_loss.pt'\n",
    "cnep_ve_path = f'{root_folder}cnep_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss2, epoch_loss4, epoch_loss8 = 0, 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(x, traj_ids[i])\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        pred2, gate2 = model2(obs, tar_x, obs_mask)\n",
    "        loss2, nll2 = model2.loss(pred2, gate2, tar_y, tar_mask)\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "\n",
    "        optimizer4.zero_grad()\n",
    "        pred4, gate4 = model4(obs, tar_x, obs_mask)\n",
    "        loss4, nll4 = model4.loss(pred4, gate4, tar_y, tar_mask)\n",
    "        loss4.backward()\n",
    "        optimizer4.step()\n",
    "\n",
    "\n",
    "        optimizer8.zero_grad()\n",
    "        pred8, gate8 = model8(obs, tar_x, obs_mask)\n",
    "        loss8, nll8 = model8.loss(pred8, gate8, tar_y, tar_mask)\n",
    "        loss8.backward()\n",
    "        optimizer8.step()\n",
    "\n",
    "        epoch_loss2 += nll2.item()\n",
    "        epoch_loss4 += nll4.item()\n",
    "        epoch_loss8 += nll8.item()\n",
    "\n",
    "    epoch_loss2 = epoch_loss2/epoch_iter\n",
    "    epoch_loss4 = epoch_loss4/epoch_iter\n",
    "    epoch_loss8 = epoch_loss8/epoch_iter\n",
    "\n",
    "    tl2.append(epoch_loss2)\n",
    "    tl4.append(epoch_loss4)\n",
    "    tl8.append(epoch_loss8)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss2, val_loss4, val_loss8 = 0, 0, 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                prepare_masked_val_batch(vx, v_traj_ids[j])\n",
    "\n",
    "                p_wta, g_wta = model2.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss2 += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "                p_wta, g_wta = model4.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss4 += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "                p_wta, g_wta = model8.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss8 += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "            ve2.append(val_loss2)\n",
    "            ve4.append(val_loss4)\n",
    "            ve8.append(val_loss8)\n",
    "\n",
    "            if val_loss2 < min_vl2:\n",
    "                min_vl2 = val_loss2\n",
    "                print(f'New best 2: {min_vl2}')\n",
    "                torch.save(model2_.state_dict(), f'{root_folder}saved_models/wta2.pt')\n",
    "\n",
    "            if val_loss4 < min_vl4:\n",
    "                min_vl4 = val_loss4\n",
    "                print(f'New best 4: {min_vl4}')\n",
    "                torch.save(model4_.state_dict(), f'{root_folder}saved_models/wta4.pt')\n",
    "\n",
    "            if val_loss8 < min_vl8:\n",
    "                min_vl8 = val_loss8\n",
    "                print(f'New best 8: {min_vl8}')\n",
    "                torch.save(model8_.state_dict(), f'{root_folder}saved_models/wta8.pt')\n",
    "            \n",
    "            print(f'Bests: {min_vl2}, {min_vl4}, {min_vl8}')\n",
    "\n",
    "    avg_loss2 += epoch_loss2\n",
    "    avg_loss4 += epoch_loss4\n",
    "    avg_loss8 += epoch_loss8\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, CNEP Losses: {}, {}, {}\".format(epoch, avg_loss2/val_per_epoch, avg_loss4/val_per_epoch, avg_loss8/val_per_epoch))\n",
    "        avg_loss2, avg_loss4, avg_loss8 = 0, 0, 0\n",
    "\n",
    "torch.save(torch.Tensor(tl2), cnep_tl_path+'_2')\n",
    "torch.save(torch.Tensor(ve2), cnep_ve_path+'_2')\n",
    "torch.save(torch.Tensor(tl4), cnep_tl_path+'_4')\n",
    "torch.save(torch.Tensor(ve4), cnep_ve_path+'_4')\n",
    "torch.save(torch.Tensor(tl8), cnep_tl_path+'_8')\n",
    "torch.save(torch.Tensor(ve8), cnep_ve_path+'_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
