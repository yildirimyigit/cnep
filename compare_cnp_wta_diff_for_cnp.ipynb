{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnp import CNP\n",
    "from models.wta_cnp import WTA_CNP\n",
    "\n",
    "from data.data_generators import *\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_wta = torch.device(\"cuda:0\")\n",
    "    device_cnp = torch.device(\"cuda:1\") if torch.cuda.device_count() > 1 else torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device_wta = torch.device(\"cpu\")\n",
    "    device_cnp = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_max_obs, n_max_tar = 6, 6\n",
    "\n",
    "t_steps = 200\n",
    "num_demos = 32\n",
    "num_classes = 2\n",
    "num_indiv = num_demos//num_classes  # number of demos per class\n",
    "noise_clip = 0.0\n",
    "dx, dy = 1, 1\n",
    "\n",
    "num_val = 8\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['r', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([32, 200, 1]) Y: torch.Size([32, 200, 1]) VX: torch.Size([8, 200, 1]) VY: torch.Size([8, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0, 1, 200).repeat(num_indiv, 1)\n",
    "y = torch.zeros(num_demos, t_steps, dy)\n",
    "\n",
    "vx = torch.linspace(0, 1, 200).repeat(num_val_indiv, 1)\n",
    "vy = torch.zeros(num_val, t_steps, dy)\n",
    "\n",
    "noise = torch.clamp(torch.randn(x.shape)*1e-7**0.5, min=0) - noise_clip\n",
    "coeff = (torch.rand(num_indiv)*0.75+0.25).unsqueeze(-1)\n",
    "y[:num_indiv] = torch.unsqueeze(generate_sin(x)*coeff + noise, 2)\n",
    "y[num_indiv:] = -1 * y[:num_indiv]\n",
    "\n",
    "coeff = (torch.rand(num_val_indiv)*0.75+0.25).unsqueeze(-1)\n",
    "noise = torch.clamp(torch.randn(vx.shape)*1e-7**0.5, min=0) - noise_clip\n",
    "vy[:num_val_indiv] = torch.unsqueeze(generate_sin(vx)*coeff + noise, 2)\n",
    "vy[num_val_indiv:] = -1 * vy[:num_val_indiv]\n",
    "\n",
    "x = torch.unsqueeze(x.repeat(num_classes, 1), 2)  # since dx = 1\n",
    "vx = torch.unsqueeze(vx.repeat(num_classes, 1), 2)\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)\n",
    "\n",
    "x0, y0 = x.to(device_wta), y.to(device_wta)\n",
    "x1, y1 = x.to(device_cnp), y.to(device_cnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device_wta):\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    # print(\"Obs:\", obs.shape, \"Tar:\", tar.shape, \"Tar_val:\", tar_val.shape)\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, o_ids=[0, -1], device=device_wta):\n",
    "    obs = torch.cat((vx[:, o_ids, :], vy[:, o_ids, :]), dim=-1).to(device)\n",
    "    tar = vx[:, torch.arange(t_steps)].to(device)\n",
    "    tar_val= vy[:, torch.arange(t_steps)].to(device)\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wta = WTA_CNP(1, 1, 10, 10, [256, 256, 256], 2, [128, 128], batch_size=batch_size).to(device_wta)\n",
    "optimizer_wta = torch.optim.Adam(lr=1e-4, params=model_wta.parameters())\n",
    "\n",
    "model_cnp = CNP(input_dim=1, hidden_dim=287, output_dim=1, n_max_obs=10, n_max_tar=10, num_layers=2, batch_size=batch_size).to(device_cnp)\n",
    "optimizer_cnp = torch.optim.Adam(lr=1e-4, params=model_cnp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (32) must match the existing size (8) at non-singleton dimension 0.  Target sizes: [32, 1, 2].  Tensor sizes: [8, 1, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yigit/projects/mbcnp/compare_cnp_wta_diff_for_cnp.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/compare_cnp_wta_diff_for_cnp.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m obs_wta, tar_x_wta, tar_y_wta \u001b[39m=\u001b[39m get_batch(x, y, traj_ids[i], device_wta)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/compare_cnp_wta_diff_for_cnp.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m obs_cnp, tar_x_cnp, tar_y_cnp \u001b[39m=\u001b[39m get_batch(x, y, traj_ids[i], device_cnp)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/compare_cnp_wta_diff_for_cnp.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m pred_wta, gate_wta \u001b[39m=\u001b[39m model_wta(obs_wta, tar_x_wta)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/compare_cnp_wta_diff_for_cnp.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m pred_cnp, encoded_rep_cnp \u001b[39m=\u001b[39m model_cnp(obs_cnp, tar_x_cnp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/compare_cnp_wta_diff_for_cnp.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m loss_wta, wta_nll \u001b[39m=\u001b[39m model_wta\u001b[39m.\u001b[39mloss(pred_wta, gate_wta, tar_y_wta)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/mbcnp/models/wta_cnp.py:61\u001b[0m, in \u001b[0;36mWTA_CNP.forward\u001b[0;34m(self, obs, tar)\u001b[0m\n\u001b[1;32m     58\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_decoders, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, n_t, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, device\u001b[39m=\u001b[39mtar\u001b[39m.\u001b[39mdevice)  \u001b[39m# tar is used to get device\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_decoders):\n\u001b[0;32m---> 61\u001b[0m     pred[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders[i](rep_tar)\n\u001b[1;32m     63\u001b[0m gate_vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate(encoded_rep)  \u001b[39m# (batch_size, num_decoders)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m pred, gate_vals\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (32) must match the existing size (8) at non-singleton dimension 0.  Target sizes: [32, 1, 2].  Tensor sizes: [8, 1, 2]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/diff/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "\n",
    "epochs = 1000000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "avg_loss_wta, avg_loss_cnp = 0, 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_wta, min_val_loss_cnp = 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_wta, validation_error_wta = [], []\n",
    "training_loss_cnp, validation_error_cnp = [], []\n",
    "\n",
    "wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "cnp_tr_loss_path = f'{root_folder}cnp_training_loss.pt'\n",
    "cnp_val_err_path = f'{root_folder}cnp_validation_error.pt'\n",
    "\n",
    "o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, device=device_wta)\n",
    "o_cnp, t_cnp, tr_cnp = get_validation_batch(vx, vy, device=device_cnp)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_wta, epoch_loss_cnp = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_wta.zero_grad()\n",
    "        optimizer_cnp.zero_grad()\n",
    "\n",
    "        obs_wta, tar_x_wta, tar_y_wta = get_batch(x, y, traj_ids[i], device_wta)\n",
    "        obs_cnp, tar_x_cnp, tar_y_cnp = get_batch(x, y, traj_ids[i], device_cnp)\n",
    "\n",
    "        pred_wta, gate_wta = model_wta(obs_wta, tar_x_wta)\n",
    "        pred_cnp, encoded_rep_cnp = model_cnp(obs_cnp, tar_x_cnp)\n",
    "\n",
    "        loss_wta, wta_nll = model_wta.loss(pred_wta, gate_wta, tar_y_wta)\n",
    "        loss_wta.backward()\n",
    "        optimizer_wta.step()\n",
    "\n",
    "        loss_cnp = model_cnp.loss(pred_cnp, tar_y_cnp)\n",
    "        loss_cnp.backward()\n",
    "        optimizer_cnp.step()\n",
    "\n",
    "        epoch_loss_wta += wta_nll.item()\n",
    "        epoch_loss_cnp += loss_cnp.item()\n",
    "\n",
    "    training_loss_wta.append(epoch_loss_wta)\n",
    "    training_loss_cnp.append(epoch_loss_cnp)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            p_wta, g_wta = model_wta(o_wta, t_wta)\n",
    "            # dec_id = torch.zeros_like(g, device=device)\n",
    "            dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "            vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "            val_loss_wta = mse_loss(vp_means, tr_wta).item()\n",
    "            validation_error_wta.append(val_loss_wta)\n",
    "            if val_loss_wta < min_val_loss_wta:\n",
    "                min_val_loss_wta = val_loss_wta\n",
    "                print(f'(WTA)New best: {min_val_loss_wta}')\n",
    "                torch.save(model_wta.state_dict(), f'{root_folder}saved_models/wta_on_synth.pt')\n",
    "\n",
    "            pred_cnp, encoded_rep = model_cnp(o_cnp, t_cnp)\n",
    "            val_loss_cnp = mse_loss(pred_cnp[:, :, :model_cnp.output_dim], tr_cnp)\n",
    "            validation_error_cnp.append(val_loss_cnp.item())\n",
    "            if val_loss_cnp < min_val_loss_cnp:\n",
    "                min_val_loss_cnp = val_loss_cnp\n",
    "                print(f'(CNP)New best: {min_val_loss_cnp}')\n",
    "                torch.save(model_cnp.state_dict(), f'{root_folder}saved_models/cnp_on_synth.pt')\n",
    "\n",
    "\n",
    "    avg_loss_wta += epoch_loss_wta\n",
    "    avg_loss_cnp += epoch_loss_cnp\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: {}, WTA-Loss: {}, CNP-Loss: {}\".format(epoch, avg_loss_wta/100, avg_loss_cnp/100))\n",
    "        avg_loss_wta, avg_loss_cnp = 0, 0\n",
    "\n",
    "    if epoch % 100000:\n",
    "        torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "        torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)\n",
    "        torch.save(torch.Tensor(training_loss_cnp), cnp_tr_loss_path)\n",
    "        torch.save(torch.Tensor(validation_error_cnp), cnp_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
