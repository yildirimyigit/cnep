{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load pre-trained MobileNetV2\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobilenet_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Remove classification head (if you only need features)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "is_save = False\n",
    "extract = True\n",
    "device = 'cuda:0'\n",
    "\n",
    "dy = 3\n",
    "\n",
    "# Load pre-trained MobileNetV2\n",
    "model = models.mobilenet_v2(pretrained=True).to(device)\n",
    "\n",
    "# Remove classification head (if you only need features)\n",
    "model.classifier = torch.nn.Identity()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 2\n",
    "num_demos = 10\n",
    "t_steps = 400\n",
    "dims = 1280  # MobileNetV2 feature size\n",
    "feats = torch.zeros(num_demos*num_modes, dims)\n",
    "trajs3 = torch.zeros(num_demos*num_modes, t_steps, 3)\n",
    "trajs8 = torch.zeros(num_demos*num_modes, t_steps, 8)\n",
    "\n",
    "minmax3 = torch.zeros(3, 2)\n",
    "minmax8 = torch.zeros(8, 2)\n",
    "\n",
    "\n",
    "def crop_left(im): \n",
    "    return transforms.functional.crop(im, top=0, left=0, height=300, width=480)\n",
    "\n",
    "if extract:\n",
    "    for mode in range(num_modes):\n",
    "        for i in range(num_demos):\n",
    "            ind = mode*num_demos + i\n",
    "            img_path = f'data/{mode}/{i}/img.jpeg'\n",
    "            img = Image.open(img_path).convert('RGB')  # Load image using PIL\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Lambda(crop_left),  # Crop the top-left side\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            img = transform(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                features = model(img) \n",
    "            feats[ind] = torch.flatten(features)\n",
    "\n",
    "            data_folder = f'/home/yigit/projects/cnep/baxter/data/{mode}/{i}/'\n",
    "            # iterate over all files in the data_folder\n",
    "            for filename in os.listdir(data_folder):\n",
    "                d = os.path.join(data_folder, filename)\n",
    "                if filename.endswith('.csv'):\n",
    "                    temp_data_3, temp_data_8 = [], []\n",
    "                    with open(d, 'r') as f:\n",
    "                        for j, line in enumerate(csv.reader(f)):\n",
    "                            if j > 0:\n",
    "                                temp_data_8.append([float(line[3]), float(line[4]), float(line[5]), float(line[6]), float(line[7]), float(line[8]), float(line[9]), float(line[9])])  # p, q, gripper\n",
    "                                temp_data_3.append([float(line[3]), float(line[4]), float(line[5])])  # p\n",
    "\n",
    "            ids = torch.linspace(0, len(temp_data_3)-1, t_steps).int()\n",
    "            for j in range(t_steps):\n",
    "                trajs3[ind, j] = torch.tensor(temp_data_3[ids[j]])\n",
    "                trajs8[ind, j] = torch.tensor(temp_data_8[ids[j]])\n",
    "\n",
    "    # map each dimension of the trajectory into the range [-1, 1], keep min and max values for each dimension to later unmap the values\n",
    "    for k in range(3):\n",
    "        min_val, max_val = trajs3[:, :, k].min(), trajs3[:, :, k].max()\n",
    "        trajs3[:, :, k] = 2 * (trajs3[:, :, k] - min_val) / (max_val - min_val) - 1\n",
    "        minmax3[k] = torch.tensor([min_val, max_val])\n",
    "    for k in range(8):\n",
    "        min_val, max_val = trajs8[:, :, k].min(), trajs8[:, :, k].max()\n",
    "        trajs8[:, :, k] = 2 * (trajs8[:, :, k] - min_val) / (max_val - min_val) - 1\n",
    "        minmax8[k] = torch.tensor([min_val, max_val])\n",
    "\n",
    "    if is_save:\n",
    "        torch.save(trajs3, 'trajs_normalized_3.pt')\n",
    "        torch.save(trajs8, 'trajs_normalized_8.pt')\n",
    "        torch.save(feats, 'feats_mn.pt')\n",
    "        torch.save(minmax3, 'minmax3.pt')\n",
    "        torch.save(minmax8, 'minmax8.pt')\n",
    "\n",
    "    if dy == 3:\n",
    "        trajs = trajs3\n",
    "    else:\n",
    "        trajs = trajs8\n",
    "else:\n",
    "    if dy == 3:\n",
    "        trajs = torch.load('trajs_normalized_3.pt')\n",
    "    else:\n",
    "        trajs = torch.load('trajs_normalized_8.pt')\n",
    "    feats = torch.load('feats.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnep import CNEP\n",
    "from cnmp import CNMP\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_modes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_demos, v_num_demos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mnum_modes\u001b[49m\n\u001b[1;32m      3\u001b[0m num_indiv \u001b[38;5;241m=\u001b[39m num_demos \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_classes  \u001b[38;5;66;03m# Number of trajectories per mode\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_val_indiv \u001b[38;5;241m=\u001b[39m v_num_demos \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_classes  \u001b[38;5;66;03m# Number of trajectories per mode\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_modes' is not defined"
     ]
    }
   ],
   "source": [
    "num_demos, v_num_demos = 16, 4\n",
    "num_classes = num_modes\n",
    "num_indiv = num_demos // num_classes  # Number of trajectories per mode\n",
    "num_val_indiv = v_num_demos // num_classes  # Number of trajectories per mode\n",
    "\n",
    "dx = 1\n",
    "dg = dims\n",
    "batch_size = 4\n",
    "n_max, m_max = 12, 12\n",
    "\n",
    "perm_ids = torch.randperm(num_demos + v_num_demos)\n",
    "train_ids, val_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "train_trajs, val_trajs = trajs[train_ids], trajs[val_ids]\n",
    "train_feats, val_feats = feats[train_ids], feats[val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[43mbatch_size\u001b[49m, n_max, dx\u001b[38;5;241m+\u001b[39mdg\u001b[38;5;241m+\u001b[39mdy), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m tar_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size, m_max, dx\u001b[38;5;241m+\u001b[39mdg), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m tar_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size, m_max, dy), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(traj_ids: list):\n",
    "    obs.fill_(0)\n",
    "    tar_x.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = train_trajs[traj_id]\n",
    "        feat = train_feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "        m = torch.randint(1, m_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "        \n",
    "        obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)  # X\n",
    "        obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)  # G\n",
    "        obs[i, :n, dx+dg:] = traj[n_ids]  # Y\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x[i, :m, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        tar_x[i, :m, dx:] = feat.repeat(m, 1)\n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "val_obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_val_batch(traj_ids: list):\n",
    "    val_obs.fill_(0)\n",
    "    val_tar_x.fill_(0)\n",
    "    val_tar_y.fill_(0)\n",
    "    val_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = val_trajs[traj_id]\n",
    "        feat = val_feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "        \n",
    "        val_obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)\n",
    "        val_obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)\n",
    "        val_obs[i, :n, dx+dg:] = traj[n_ids]\n",
    "        val_obs_mask[i, :n] = True\n",
    "        \n",
    "        val_tar_x[i, :, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        val_tar_x[i, :, dx:] = feat.repeat(t_steps, 1)\n",
    "        val_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnep: 1580302\n",
      "cnmp: 1579782\n"
     ]
    }
   ],
   "source": [
    "cnep_ = CNEP(dx+dg, dy, n_max, n_max, [512, 256], num_decoders=2, decoder_hidden_dims=[256, 256], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "optimizer_cnep = torch.optim.Adam(lr=3e-4, params=cnep_.parameters())\n",
    "\n",
    "cnmp_ = CNMP(dx+dg, dy, n_max, m_max, [512, 256], decoder_hidden_dims=[512, 512], batch_size=batch_size, device=device)\n",
    "optimizer_cnmp = torch.optim.Adam(lr=3e-4, params=cnmp_.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"cnep:\", get_parameter_count(cnep_))\n",
    "print(\"cnmp:\", get_parameter_count(cnmp_))\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    cnep, cnmp = torch.compile(cnep_), torch.compile(cnmp_)\n",
    "else:\n",
    "    cnep, cnmp = cnep_, cnmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNMP New best: 0.0006925589591264725\n",
      "CNEP New best: 0.0006502748280763626\n",
      "Epoch: 0, Loss: 0.0008362470120191574, 0.00039502926915884016, Min Err: 0.0006925589591264725, 0.0006502748280763626\n",
      "CNMP New best: 0.000600370466709137\n",
      "CNEP New best: 0.0005933015421032906\n",
      "Epoch: 1000, Loss: 0.6301618286110461, 0.314782127735205, Min Err: 0.000600370466709137, 0.0005933015421032906\n",
      "CNMP New best: 0.0005650626122951507\n",
      "CNEP New best: 0.0005176967754960061\n",
      "Epoch: 2000, Loss: 0.49160195258772, 0.24572469890792853, Min Err: 0.0005650626122951507, 0.0005176967754960061\n",
      "CNMP New best: 0.00046815332025289534\n",
      "CNEP New best: 0.00043959803879261016\n",
      "Epoch: 3000, Loss: 0.27217320841690523, 0.1786667397688143, Min Err: 0.00046815332025289534, 0.00043959803879261016\n",
      "CNMP New best: 0.0003382920101284981\n",
      "CNEP New best: 0.00043948814272880553\n",
      "Epoch: 4000, Loss: 0.07461861245776527, 0.12119401098345406, Min Err: 0.0003382920101284981, 0.00043948814272880553\n",
      "CNMP New best: 0.0002721057087182999\n",
      "CNEP New best: 0.00041272353380918505\n",
      "Epoch: 5000, Loss: -0.02768409793288447, 0.07544128321728204, Min Err: 0.0002721057087182999, 0.00041272353380918505\n",
      "CNEP New best: 0.00039839446544647216\n",
      "Epoch: 6000, Loss: -0.0964374115318642, 0.03565485735196853, Min Err: 0.0002721057087182999, 0.00039839446544647216\n",
      "CNMP New best: 0.00019113689661026\n",
      "CNEP New best: 0.0002771900221705437\n",
      "Epoch: 7000, Loss: -0.12914940204995218, 0.0068318426068290136, Min Err: 0.00019113689661026, 0.0002771900221705437\n",
      "CNMP New best: 0.00018937120214104652\n",
      "CNEP New best: 0.00025459343567490577\n",
      "Epoch: 8000, Loss: -0.17285892428527586, -0.015829011068621186, Min Err: 0.00018937120214104652, 0.00025459343567490577\n",
      "CNMP New best: 0.00017809616401791571\n",
      "CNEP New best: 0.00023381818085908888\n",
      "Epoch: 9000, Loss: -0.1620191703469027, -0.02608010947977891, Min Err: 0.00017809616401791571, 0.00023381818085908888\n",
      "CNMP New best: 0.00017689578235149383\n",
      "CNEP New best: 0.0002203117311000824\n",
      "Epoch: 10000, Loss: -0.2012648707542103, -0.04161597884516232, Min Err: 0.00017689578235149383, 0.0002203117311000824\n",
      "CNMP New best: 0.0001619892753660679\n",
      "CNEP New best: 0.00021445494145154952\n",
      "Epoch: 11000, Loss: -0.23580175011453686, -0.047771954815660135, Min Err: 0.0001619892753660679, 0.00021445494145154952\n",
      "CNEP New best: 0.00021190701052546502\n",
      "Epoch: 12000, Loss: -0.2572874683154514, -0.0540389539936441, Min Err: 0.0001619892753660679, 0.00021190701052546502\n",
      "CNEP New best: 0.00019754311069846152\n",
      "Epoch: 13000, Loss: -0.26745217661105564, -0.0606750826969801, Min Err: 0.0001619892753660679, 0.00019754311069846152\n",
      "CNMP New best: 0.0001596972160041332\n",
      "Epoch: 14000, Loss: -0.2765630972206127, -0.06757055525446776, Min Err: 0.0001596972160041332, 0.00019754311069846152\n",
      "Epoch: 15000, Loss: -0.2998388643832877, -0.07480153916502605, Min Err: 0.0001596972160041332, 0.00019754311069846152\n",
      "Epoch: 16000, Loss: -0.31278573427954687, -0.07987441333057359, Min Err: 0.0001596972160041332, 0.00019754311069846152\n",
      "CNMP New best: 0.00015443122014403344\n",
      "Epoch: 17000, Loss: -0.324428267087671, -0.08638575060025323, Min Err: 0.00015443122014403344, 0.00019754311069846152\n",
      "CNMP New best: 0.00015140683390200139\n",
      "Epoch: 18000, Loss: -0.3309360392163508, -0.08837033196166158, Min Err: 0.00015140683390200139, 0.00019754311069846152\n",
      "CNEP New best: 0.00019742010161280632\n",
      "Epoch: 19000, Loss: -0.3287312710471451, -0.08783694596472197, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "Epoch: 20000, Loss: -0.3433412559584249, -0.0941914078296395, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "Epoch: 21000, Loss: -0.3510850937075447, -0.09939951676013879, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "Epoch: 22000, Loss: -0.35682090737717226, -0.09755892727267929, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "Epoch: 23000, Loss: -0.35792829873214943, -0.10326421260769712, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "Epoch: 24000, Loss: -0.3764114107845817, -0.1027428755160945, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "Epoch: 25000, Loss: -0.34981944200443105, -0.10641239957266953, Min Err: 0.00015140683390200139, 0.00019742010161280632\n",
      "CNMP New best: 0.0001501716673374176\n",
      "Epoch: 26000, Loss: -0.369178115449613, -0.11037154693063349, Min Err: 0.0001501716673374176, 0.00019742010161280632\n",
      "CNMP New best: 0.00014564414508640765\n",
      "Epoch: 27000, Loss: -0.3794289981662296, -0.1116776767905103, Min Err: 0.00014564414508640765, 0.00019742010161280632\n",
      "CNEP New best: 0.0001822887174785137\n",
      "Epoch: 28000, Loss: -0.3783488335183356, -0.11098191366891842, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 29000, Loss: -0.32112134441453966, -0.1140081290549133, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 30000, Loss: -0.364150271672057, -0.11698084186989581, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 31000, Loss: -0.37527416680380704, -0.11857117407559417, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 32000, Loss: -0.38874230010109023, -0.12217465653660474, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 33000, Loss: -0.38976997225219384, -0.12277187158365269, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 34000, Loss: -0.35724624180910175, -0.1236073073966545, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 35000, Loss: -0.38901010804669933, -0.1264395255643176, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 36000, Loss: -0.39386293378588744, -0.12796156714431708, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 37000, Loss: -0.38531626858515666, -0.12503638892411253, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "Epoch: 38000, Loss: -0.40200282268715093, -0.13056842446152586, Min Err: 0.00014564414508640765, 0.0001822887174785137\n",
      "CNEP New best: 0.00017936652526259422\n",
      "Epoch: 39000, Loss: -0.4079134310632944, -0.1285664639789611, Min Err: 0.00014564414508640765, 0.00017936652526259422\n",
      "CNMP New best: 0.0001423637382686138\n",
      "CNEP New best: 0.00017597297206521035\n",
      "Epoch: 40000, Loss: -0.42253493138146586, -0.13606092801422345, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 41000, Loss: -0.36486421499401334, -0.13502713737427258, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 42000, Loss: -0.3394726262666518, -0.12838207845855504, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 43000, Loss: -0.38326339932368136, -0.12298097015690292, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 44000, Loss: -0.3893305079941638, -0.13910563241585625, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 45000, Loss: -0.393257002949249, -0.13926794352772412, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 46000, Loss: -0.39545768860331737, -0.13953777774830814, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 47000, Loss: -0.4072707962661516, -0.14345247128396296, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "Epoch: 48000, Loss: -0.3936706569320522, -0.1401162563416874, Min Err: 0.0001423637382686138, 0.00017597297206521035\n",
      "CNEP New best: 0.00017409278079867364\n",
      "Epoch: 49000, Loss: -0.40767110607447105, -0.1379606573776109, Min Err: 0.0001423637382686138, 0.00017409278079867364\n",
      "Epoch: 50000, Loss: -0.41453673619264736, -0.1478608708754182, Min Err: 0.0001423637382686138, 0.00017409278079867364\n",
      "CNEP New best: 0.0001727093756198883\n",
      "Epoch: 51000, Loss: -0.42187573858804533, -0.1504388925501844, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "Epoch: 52000, Loss: -0.41371346345427445, -0.1477298315449152, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "Epoch: 53000, Loss: -0.41054543006420136, -0.1487152347039082, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "Epoch: 54000, Loss: -0.15540570297196973, -0.1428386123429518, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "Epoch: 55000, Loss: -0.18980295852932613, -0.1521191773057799, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "Epoch: 56000, Loss: -0.22225583414407446, -0.15067076232959517, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "Epoch: 57000, Loss: -0.2461910802666098, -0.15354188948811498, Min Err: 0.0001423637382686138, 0.0001727093756198883\n",
      "CNEP New best: 0.00016325991600751877\n",
      "Epoch: 58000, Loss: -0.26021858776698353, -0.1573041585199535, Min Err: 0.0001423637382686138, 0.00016325991600751877\n",
      "Epoch: 59000, Loss: -0.27133980581804645, -0.1534962052477058, Min Err: 0.0001423637382686138, 0.00016325991600751877\n",
      "Epoch: 60000, Loss: -0.29281541832955554, -0.16316239908663557, Min Err: 0.0001423637382686138, 0.00016325991600751877\n",
      "Epoch: 61000, Loss: -0.28753763146093114, -0.1632756786685204, Min Err: 0.0001423637382686138, 0.00016325991600751877\n",
      "Epoch: 62000, Loss: -0.2996354737961665, -0.16569600236753468, Min Err: 0.0001423637382686138, 0.00016325991600751877\n",
      "Epoch: 63000, Loss: -0.2964360483243363, -0.16011828684061766, Min Err: 0.0001423637382686138, 0.00016325991600751877\n",
      "CNEP New best: 0.00016019240021705627\n",
      "Epoch: 64000, Loss: -0.3014042223112192, -0.16715301246929448, Min Err: 0.0001423637382686138, 0.00016019240021705627\n",
      "Epoch: 65000, Loss: -0.307035113356309, -0.16512546443578321, Min Err: 0.0001423637382686138, 0.00016019240021705627\n",
      "CNEP New best: 0.00015807781368494035\n",
      "Epoch: 66000, Loss: -0.30268181150988677, -0.1688330274707987, Min Err: 0.0001423637382686138, 0.00015807781368494035\n",
      "Epoch: 67000, Loss: -0.31467457860684955, -0.17077011780429166, Min Err: 0.0001423637382686138, 0.00015807781368494035\n",
      "CNEP New best: 0.00015547538176178933\n",
      "Epoch: 68000, Loss: -0.30547012921771965, -0.17398937452072277, Min Err: 0.0001423637382686138, 0.00015547538176178933\n",
      "CNEP New best: 0.00015510972589254378\n",
      "Epoch: 69000, Loss: -0.3108026866792934, -0.17835779344290495, Min Err: 0.0001423637382686138, 0.00015510972589254378\n",
      "CNEP New best: 0.00014874061569571494\n",
      "Epoch: 70000, Loss: -0.3186573012957815, -0.17730896517122163, Min Err: 0.0001423637382686138, 0.00014874061569571494\n",
      "Epoch: 71000, Loss: -0.3120163640984101, -0.17664244586284622, Min Err: 0.0001423637382686138, 0.00014874061569571494\n",
      "Epoch: 72000, Loss: -0.33014518856583164, -0.17787448218278587, Min Err: 0.0001423637382686138, 0.00014874061569571494\n",
      "Epoch: 73000, Loss: -0.3223944331689272, -0.1762967485411791, Min Err: 0.0001423637382686138, 0.00014874061569571494\n",
      "CNEP New best: 0.00014253648929297925\n",
      "Epoch: 74000, Loss: -0.31934474777872673, -0.18057316269678994, Min Err: 0.0001423637382686138, 0.00014253648929297925\n",
      "Epoch: 75000, Loss: -0.3150163181202952, -0.18491728787129977, Min Err: 0.0001423637382686138, 0.00014253648929297925\n",
      "Epoch: 76000, Loss: -0.32249332148057874, -0.19242711294139736, Min Err: 0.0001423637382686138, 0.00014253648929297925\n",
      "CNEP New best: 0.000142449289560318\n",
      "Epoch: 77000, Loss: -0.3180058560776524, -0.19285428439523095, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 78000, Loss: -0.32266824748530054, -0.1927482696287334, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 79000, Loss: -0.32025748561951334, -0.20021831093518994, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 80000, Loss: -0.3333413672719616, -0.2022736392824445, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 81000, Loss: -0.3270905919683864, -0.20502136766421608, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 82000, Loss: -0.3244609920014627, -0.21387805517343805, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 83000, Loss: -0.33372749460930934, -0.21195968562108464, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "Epoch: 84000, Loss: -0.33512303240248004, -0.21617788265808485, Min Err: 0.0001423637382686138, 0.000142449289560318\n",
      "CNEP New best: 0.00012834163382649422\n",
      "Epoch: 85000, Loss: -0.33524902382679284, -0.2149478766361717, Min Err: 0.0001423637382686138, 0.00012834163382649422\n",
      "Epoch: 86000, Loss: -0.3382981693628244, -0.21958995714550839, Min Err: 0.0001423637382686138, 0.00012834163382649422\n",
      "Epoch: 87000, Loss: -0.3361707629992161, -0.2201159661517013, Min Err: 0.0001423637382686138, 0.00012834163382649422\n",
      "Epoch: 88000, Loss: -0.3369339856707957, -0.22217852329183368, Min Err: 0.0001423637382686138, 0.00012834163382649422\n",
      "Epoch: 89000, Loss: -0.34163478037551975, -0.2218392612192547, Min Err: 0.0001423637382686138, 0.00012834163382649422\n",
      "CNEP New best: 0.00012308631092309953\n",
      "Epoch: 90000, Loss: -0.3288488172725774, -0.22100142268207856, Min Err: 0.0001423637382686138, 0.00012308631092309953\n",
      "CNEP New best: 0.00011806086637079716\n",
      "Epoch: 91000, Loss: -0.3329675536628347, -0.22813179703149944, Min Err: 0.0001423637382686138, 0.00011806086637079716\n",
      "Epoch: 92000, Loss: -0.3298018136483151, -0.22855111650028265, Min Err: 0.0001423637382686138, 0.00011806086637079716\n",
      "Epoch: 93000, Loss: -0.33680806048191153, -0.2269820379698649, Min Err: 0.0001423637382686138, 0.00011806086637079716\n",
      "CNEP New best: 0.00011683839373290539\n",
      "Epoch: 94000, Loss: -0.3241084028147161, -0.22614207337005063, Min Err: 0.0001423637382686138, 0.00011683839373290539\n",
      "Epoch: 95000, Loss: -0.3400491144475527, -0.23198921600310132, Min Err: 0.0001423637382686138, 0.00011683839373290539\n",
      "Epoch: 96000, Loss: -0.3453943076368887, -0.24017367215873672, Min Err: 0.0001423637382686138, 0.00011683839373290539\n",
      "CNEP New best: 0.00011555703356862068\n",
      "Epoch: 97000, Loss: -0.35068181227776224, -0.2360054133858066, Min Err: 0.0001423637382686138, 0.00011555703356862068\n",
      "Epoch: 98000, Loss: -0.3385129757607356, -0.23768428852444048, Min Err: 0.0001423637382686138, 0.00011555703356862068\n",
      "Epoch: 99000, Loss: -0.3397520795643795, -0.23700017839745852, Min Err: 0.0001423637382686138, 0.00011555703356862068\n",
      "Epoch: 100000, Loss: -0.3373481528409757, -0.23262475941609592, Min Err: 0.0001423637382686138, 0.00011555703356862068\n",
      "CNEP New best: 0.00011114864610135555\n",
      "Epoch: 101000, Loss: -0.3433267347484361, -0.237994697268703, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 102000, Loss: -0.33957684476627037, -0.24059081747732125, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 103000, Loss: -0.35346083164960146, -0.23076983425079378, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 104000, Loss: -0.3420849851951934, -0.24127710518054665, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 105000, Loss: -0.33922736459597946, -0.19648948337975888, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 106000, Loss: -0.34293625843827613, -0.22554938043118455, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 107000, Loss: -0.3527495825488586, -0.23869017786416225, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 108000, Loss: -0.35531860050139946, -0.2441572827126365, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 109000, Loss: -0.3580806735556107, -0.2482883818191476, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 110000, Loss: -0.3509564820870291, -0.24962088032020255, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 111000, Loss: -0.3531346073213499, -0.2549497823669808, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 112000, Loss: -0.3512829610847402, -0.2563954228090588, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 113000, Loss: -0.3460296939157415, -0.2587328086667694, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 114000, Loss: -0.3565198184257606, -0.25830196308367886, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 115000, Loss: -0.35473680794169193, -0.2604132171212696, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 116000, Loss: -0.3493797706203768, -0.2616081042903243, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 117000, Loss: -0.3463805241563823, -0.2565458549861796, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 118000, Loss: -0.3508006630782038, -0.2697797929473454, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 119000, Loss: -0.3517038597080391, -0.2684617567199748, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 120000, Loss: -0.35787169452989476, -0.25934444400609935, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 121000, Loss: -0.3575756939807907, -0.2667909591491334, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 122000, Loss: -0.36001707198936495, -0.25508905594167297, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 123000, Loss: -0.35733050149376505, -0.1782489773974521, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 124000, Loss: -0.36854848986887373, -0.21926252923859282, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 125000, Loss: -0.3569485325794667, -0.22393260062206535, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 126000, Loss: -0.3578415183897596, -0.23883253779727964, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 127000, Loss: -0.2579424114261055, -0.248585869388422, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 128000, Loss: -0.35931758940895087, -0.2480694101343397, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 129000, Loss: -0.35576930495817216, -0.255633584624622, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 130000, Loss: -0.35958998155011795, -0.25845645657414573, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 131000, Loss: -0.35698494241107254, -0.25533971753972584, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 132000, Loss: -0.36623323083308057, -0.26273132765782065, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 133000, Loss: -0.3556167866711039, -0.2734721471269149, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 134000, Loss: -0.3612391468367423, -0.26984515209752136, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 135000, Loss: -0.35681465158611536, -0.26112724622432143, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 136000, Loss: -0.3599246123582125, -0.27448608456202783, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 137000, Loss: -0.3660018693925813, -0.27579257914656774, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 138000, Loss: -0.3654669345590519, -0.2734297218034044, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 139000, Loss: -0.3594885213812813, -0.2812439647144638, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 140000, Loss: -0.35982329777115957, -0.2786636406367179, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 141000, Loss: -0.3580859761938918, -0.2756931229834445, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 142000, Loss: -0.3635824801537674, -0.28573693671263756, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 143000, Loss: -0.36018558432767167, -0.289598555106204, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 144000, Loss: -0.3698818638471421, -0.288582437305944, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 145000, Loss: -0.35744916917709635, -0.28960259128385224, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 146000, Loss: -0.372733798396308, -0.28598596981121227, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 147000, Loss: -0.3583992056227289, -0.289370632432634, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 148000, Loss: -0.36971291576395743, -0.29405269607203083, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 149000, Loss: -0.3647926165217068, -0.2671390382995596, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 150000, Loss: -0.3571014413813828, -0.29412600271124395, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 151000, Loss: -0.37264587088627743, -0.29871684152353556, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 152000, Loss: -0.37252073651785034, -0.27768324711476455, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 153000, Loss: -0.3708237257883884, -0.29917395226308147, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 154000, Loss: -0.3731891605997225, -0.3004050176544115, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 155000, Loss: -0.3697992881881073, -0.29766201904020273, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 156000, Loss: -0.3677300907871686, -0.3046609916465823, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 157000, Loss: -0.36740136782021726, -0.30899759972235186, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 158000, Loss: -0.37358275217353365, -0.3065329984533601, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 159000, Loss: -0.3693020557006821, -0.3060956456942949, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 160000, Loss: -0.3759117449619807, -0.311906435489771, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 161000, Loss: -0.36672522813931574, -0.3141065267387312, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 162000, Loss: -0.37251003294973634, -0.31221248717349953, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 163000, Loss: -0.3707058765643742, -0.3117212692918256, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 164000, Loss: -0.3719886603285559, -0.31320073462091386, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 165000, Loss: -0.3737529716943391, -0.30289228839240967, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 166000, Loss: -0.366290645799716, -0.3055803562900983, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 167000, Loss: -0.37960482624778524, -0.3153972857433837, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 168000, Loss: -0.3748471587621607, -0.3165480076255626, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 169000, Loss: -0.37732635219834626, -0.31171292649349197, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 170000, Loss: -0.37794127622363155, -0.26593737209716345, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 171000, Loss: -0.3734250072191935, -0.16734342569264118, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 172000, Loss: -0.3763066854570061, -0.18932628813001792, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 173000, Loss: -0.3740938117238693, -0.2108099024221301, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 174000, Loss: -0.3746015417096205, -0.21340797902469058, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 175000, Loss: -0.3785942293679109, -0.21369355008401908, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 176000, Loss: -0.37704249937972056, -0.21491599404066802, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 177000, Loss: -0.37178614179603753, -0.2233447352417279, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 178000, Loss: -0.38451797905471174, -0.22764173198933713, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 179000, Loss: -0.37514601774793116, -0.2302198300898308, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 180000, Loss: -0.37776236032717864, -0.22331667354225646, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 181000, Loss: -0.3814877584245987, -0.23096544672490563, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 182000, Loss: -0.37051356677152214, -0.22697932946076615, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 183000, Loss: -0.38692816718737594, -0.22746989108761773, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 184000, Loss: -0.37950816490268335, -0.2346385753392242, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 185000, Loss: -0.3789389630900696, -0.23237374930176885, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 186000, Loss: -0.3787389596602879, -0.233734864217171, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 187000, Loss: -0.3816390585603658, -0.23652221690758596, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 188000, Loss: -0.3829453046284616, -0.23459019400062972, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 189000, Loss: -0.3798148421123624, -0.23387620687135496, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 190000, Loss: -0.37694361514551566, -0.22753329534153455, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 191000, Loss: -0.38255756235914307, -0.23651451516663655, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 192000, Loss: -0.39145536779332907, -0.23947050742572173, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 193000, Loss: -0.3838198542166501, -0.23736398522858507, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 194000, Loss: -0.3827083783280104, -0.23757394262833986, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 195000, Loss: -0.3830542548317462, -0.237101206583553, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 196000, Loss: -0.382681279735174, -0.23688233714341186, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 197000, Loss: -0.38523845268646256, -0.2364837141885655, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 198000, Loss: -0.3805312223179499, -0.23000520993978715, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 199000, Loss: -0.3796675192704424, -0.2349254043677356, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 200000, Loss: -0.38179323594481684, -0.23638230391801335, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 201000, Loss: -0.3851753881950863, -0.2425054769513663, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 202000, Loss: -0.39567419431358575, -0.23927618110738694, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 203000, Loss: -0.3801504142009653, -0.23446892747993114, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 204000, Loss: -0.3900205244976096, -0.24051039922586642, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 205000, Loss: -0.38728147256933154, -0.24186053095653187, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 206000, Loss: -0.389566108440049, -0.24170905011729338, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 207000, Loss: -0.3928395549086854, -0.23507375449175016, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 208000, Loss: -0.3845900867739692, -0.23417169848276534, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 209000, Loss: -0.3887273226068355, -0.24380769691232126, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 210000, Loss: -0.3851347690774128, -0.2417247259961441, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 211000, Loss: -0.38111446671234445, -0.23735568516503555, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 212000, Loss: -0.39222907049418426, -0.24683495733351446, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 213000, Loss: -0.3947705342058325, -0.24312458280008287, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 214000, Loss: -0.38918112823320555, -0.24030101956333966, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 215000, Loss: -0.38633015438634905, -0.2468914304045029, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 216000, Loss: -0.389224179713754, -0.2360048741986975, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 217000, Loss: -0.3899309262582101, -0.23814067974057979, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 218000, Loss: -0.3917652954952791, -0.24683716174704023, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 219000, Loss: -0.3894990215522703, -0.25116986794164403, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 220000, Loss: -0.38951625006762336, -0.24341423172713259, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 221000, Loss: -0.39858930399944076, -0.2440906369095901, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 222000, Loss: -0.39160438592755237, -0.23448863615223672, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 223000, Loss: -0.3933920065204147, -0.24449816364992877, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 224000, Loss: -0.3944816948035732, -0.2473109847640153, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 225000, Loss: -0.3924761400818825, -0.2409944620335009, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 226000, Loss: -0.39637630863604134, -0.24797295929002575, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 227000, Loss: -0.39678700558585117, -0.25040755851892754, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 228000, Loss: -0.39311991017032416, -0.24112686362396926, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 229000, Loss: -0.3893190587819554, -0.24013815086032264, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 230000, Loss: -0.39308599137910644, -0.24812414824380538, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 231000, Loss: -0.3956075530969538, -0.25293253438558894, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 232000, Loss: -0.39476619262993334, -0.2436075881421566, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 233000, Loss: -0.4042801047645044, -0.2532141863212455, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 234000, Loss: -0.3883169843466021, -0.25651182810356843, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 235000, Loss: -0.39563657661224716, -0.24842573381145486, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 236000, Loss: -0.39152936001215133, -0.2459472941707354, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 237000, Loss: -0.3935382847636938, -0.2502488895580173, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 238000, Loss: -0.3881450479757041, -0.25112632049038075, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 239000, Loss: -0.39882502737967296, -0.2568310064299731, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 240000, Loss: -0.39298977215681224, -0.2478824666341534, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 241000, Loss: -0.3972492554322816, -0.2539787076632492, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 242000, Loss: -0.39283065908635034, -0.24972147625044455, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 243000, Loss: -0.39405201650504024, -0.2132986861267127, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 244000, Loss: -0.3954661109871231, -0.22372071836318355, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 245000, Loss: -0.39338622115878386, -0.247513311290415, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 246000, Loss: -0.3987112989722518, -0.2430934208952822, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 247000, Loss: -0.3944920266217086, -0.2517552689234726, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 248000, Loss: -0.4000657865570392, -0.2507297866481822, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 249000, Loss: -0.4001104019903578, -0.25640276875463314, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 250000, Loss: -0.40024339282070287, -0.24815552730078344, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 251000, Loss: -0.40148671170813033, -0.25536099234130233, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 252000, Loss: -0.3914861973943189, -0.2552602414842695, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 253000, Loss: -0.39279942332068457, -0.2519722486675601, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 254000, Loss: -0.392621220305562, -0.2502404782921076, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 255000, Loss: -0.3986544781057164, -0.25303198948828504, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 256000, Loss: -0.40642009040294214, -0.25579591186880135, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 257000, Loss: -0.4003236877126619, -0.24828635734337148, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 258000, Loss: -0.3952026563305408, -0.2431046141840052, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 259000, Loss: -0.397692771893926, -0.2555832024945412, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 260000, Loss: -0.4035671236005146, -0.24037602643307765, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 261000, Loss: -0.4007843191936845, -0.2552819425588241, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 262000, Loss: -0.4020499279343057, -0.25673390928446316, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 263000, Loss: -0.40088668578397485, -0.254457914589555, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 264000, Loss: -0.39748778646113353, -0.25713317523221485, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 265000, Loss: -0.4149449951755814, -0.26056622821395287, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 266000, Loss: -0.40415664868569, -0.2556309942812659, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 267000, Loss: -0.4025953181437217, -0.25853786723536903, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 268000, Loss: -0.4076714390080888, -0.25609219711693004, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 269000, Loss: -0.3945063253450207, -0.252153423640877, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 270000, Loss: -0.40316209282772614, -0.25157568499143235, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 271000, Loss: -0.4079829407595098, -0.2598223137697205, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 272000, Loss: -0.40192620603740215, -0.261916555054253, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 273000, Loss: -0.40679047142574565, -0.26023993556480857, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 274000, Loss: -0.40758529082010503, -0.2625832195507828, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 275000, Loss: -0.39496369545487686, -0.25598036547040104, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 276000, Loss: -0.4130844225543551, -0.25513538654753937, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 277000, Loss: -0.3962508498085663, -0.2511013088256586, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 278000, Loss: -0.4103956465534866, -0.2597929900286254, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 279000, Loss: -0.41015270274016075, -0.25603195284307, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 280000, Loss: -0.3995271340428153, -0.25556059635593553, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 281000, Loss: -0.4097017770672683, -0.26373532889713536, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 282000, Loss: -0.4100596975190565, -0.26310259461868557, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 283000, Loss: -0.4103431294558104, -0.25717773879901507, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 284000, Loss: -0.4102801073261071, -0.2661884785748553, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 285000, Loss: -0.4088719412116334, -0.26108548618666827, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 286000, Loss: -0.4026731476671412, -0.2557698513690848, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 287000, Loss: -0.40077867105952464, -0.25615446751448323, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 288000, Loss: -0.4096008174754679, -0.26353056201757863, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 289000, Loss: -0.4054478152645752, -0.2595943416705122, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 290000, Loss: -0.4095895455153659, -0.2610176997000817, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 291000, Loss: -0.40901905727619303, -0.26475107167172246, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 292000, Loss: -0.4109703729748726, -0.25978126150008757, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 293000, Loss: -0.39943557556218, -0.261463491166709, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 294000, Loss: -0.41459390726499257, -0.2630032043438405, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 295000, Loss: -0.4128044169887435, -0.26473557579563933, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 296000, Loss: -0.41082498617935925, -0.2624775273730047, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 297000, Loss: -0.41263491127733143, -0.2534398857196793, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 298000, Loss: -0.40799871619744227, -0.2605434206763748, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 299000, Loss: -0.4106008413066156, -0.25798979998542926, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 300000, Loss: -0.4039269415817689, -0.2575919274414191, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 301000, Loss: -0.4073386900210753, -0.26023073001788, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 302000, Loss: -0.40901577664818617, -0.25476152399601415, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 303000, Loss: -0.409604155348381, -0.2630502517868299, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 304000, Loss: -0.41377916786400604, -0.266467453379184, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 305000, Loss: -0.412953435969539, -0.2655312956906855, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 306000, Loss: -0.41056346099846996, -0.25649815138150006, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 307000, Loss: -0.40968845840683205, -0.26015771142719313, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 308000, Loss: -0.4185980616521556, -0.26847329788096247, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 309000, Loss: -0.41068154627038167, -0.26071558011136947, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 310000, Loss: -0.40771130518987775, -0.2617735980569851, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 311000, Loss: -0.41113949902495367, -0.26492641641548836, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 312000, Loss: -0.41371488773426973, -0.270458751922939, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 313000, Loss: -0.40691798592032863, -0.25887365865637546, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 314000, Loss: -0.41100663074315524, -0.2743644304845948, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 315000, Loss: -0.41683691157982683, -0.27117208317678887, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 316000, Loss: -0.40509496254008265, -0.2664411829258315, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 317000, Loss: -0.4158443411060143, -0.2687628520182334, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 318000, Loss: -0.41771135364240036, -0.26361799041368067, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 319000, Loss: -0.41765234787226657, -0.2629348581621889, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 320000, Loss: -0.4154196237870492, -0.2707181177765597, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 321000, Loss: -0.4129362636569422, -0.2640136856539175, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 322000, Loss: -0.420757177434396, -0.26976820641220545, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 323000, Loss: -0.408834402681794, -0.2605985757181188, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 324000, Loss: -0.4168132809784729, -0.26831604363641237, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 325000, Loss: -0.4218470432688482, -0.2672017539604567, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 326000, Loss: -0.40624586812313646, -0.2602685381461051, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 327000, Loss: -0.4183709425153211, -0.26436284904228524, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 328000, Loss: -0.41652310235705226, -0.2693654180500889, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 329000, Loss: -0.4176271239295602, -0.26768147249566393, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 330000, Loss: -0.41654426852287724, -0.2661323579864111, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 331000, Loss: -0.41242706863884815, -0.25956615109788256, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 332000, Loss: -0.4254275984563865, -0.2727366695087403, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 333000, Loss: -0.40849455825053155, -0.25720133710396476, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 334000, Loss: -0.41789374892343767, -0.25955755850998685, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 335000, Loss: -0.41787487179017624, -0.2658845305510331, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 336000, Loss: -0.4139116475742776, -0.256876079108566, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 337000, Loss: -0.41682162683759816, -0.265402032539947, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 338000, Loss: -0.41387871563690715, -0.2686411998711992, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 339000, Loss: -0.4146359458512161, -0.2599424839455169, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 340000, Loss: -0.4077612662790343, -0.26873502734769134, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 341000, Loss: -0.4170971155667212, -0.26541607498098163, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 342000, Loss: -0.40885506712598724, -0.26874678730824964, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 343000, Loss: -0.4214584519078489, -0.26782833214849233, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 344000, Loss: -0.4204779481091537, -0.268235679528676, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 345000, Loss: -0.4099576409875881, -0.27286257580539675, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 346000, Loss: -0.4176689347907668, -0.2728988540645223, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 347000, Loss: -0.42221254261839203, -0.27152591072558424, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 348000, Loss: -0.41441315711801874, -0.26422481375420465, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 349000, Loss: -0.4223192786052823, -0.2633422158632893, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 350000, Loss: -0.4220263234945014, -0.2673253044609446, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 351000, Loss: -0.4227602614518255, -0.27521819245314694, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 352000, Loss: -0.4170978908319958, -0.2751775724886102, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 353000, Loss: -0.41912555845302996, -0.25902595434314574, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 354000, Loss: -0.41630924928409513, -0.26975405700225386, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 355000, Loss: -0.4241773560708389, -0.26978139198431744, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 356000, Loss: -0.41309121236391366, -0.265521214546985, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 357000, Loss: -0.42312278188951313, -0.27192846428905615, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 358000, Loss: -0.4231645833202638, -0.26802015530667267, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 359000, Loss: -0.42219981223717334, -0.27356050768750717, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 360000, Loss: -0.42303056523809207, -0.25766340955509803, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 361000, Loss: -0.4221162571250461, -0.27211342517030424, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 362000, Loss: -0.4254707141749095, -0.2719980168279726, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 363000, Loss: -0.4217563251752872, -0.2676922854890581, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 364000, Loss: -0.4228782898676582, -0.27899863696086685, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 365000, Loss: -0.4164159538152162, -0.26329892858618403, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 366000, Loss: -0.4211899984525517, -0.26848501611547543, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 367000, Loss: -0.41567152764298954, -0.2668651443804847, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 368000, Loss: -0.41627138424431903, -0.2599564894507639, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 369000, Loss: -0.42740457044239155, -0.2680301466686651, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 370000, Loss: -0.42806175161595456, -0.2639001410838682, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 371000, Loss: -0.42612428352504506, -0.2666317702201195, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 372000, Loss: -0.42341053578210996, -0.2719923121011816, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 373000, Loss: -0.4298225076145027, -0.2694943745518103, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 374000, Loss: -0.4281805234858766, -0.26781908293743617, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 375000, Loss: -0.43174587001185866, -0.268648588744225, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 376000, Loss: -0.4252124674501829, -0.27811908863298596, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 377000, Loss: -0.43570908467913977, -0.26972198246093465, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 378000, Loss: -0.4190515502805356, -0.2634582492802292, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 379000, Loss: -0.4241934782206081, -0.26319894842628855, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 380000, Loss: -0.4312003549407236, -0.2794232224039733, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 381000, Loss: -0.41999386449158194, -0.2677937505969312, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 382000, Loss: -0.4302914879767923, -0.27276170007395556, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 383000, Loss: -0.4331234787234571, -0.27266233044536786, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 384000, Loss: -0.42547338592493905, -0.2774516732725315, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 385000, Loss: -0.4235730805210769, -0.26817970019357745, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 386000, Loss: -0.4312448067078367, -0.27138029258069585, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 387000, Loss: -0.4229241696060635, -0.2302681301253615, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 388000, Loss: -0.4275762735346798, -0.23978215529862792, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 389000, Loss: -0.4305402470843401, -0.24003882393229287, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 390000, Loss: -0.43010335324215704, -0.23331694178865292, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 391000, Loss: -0.43289473812119106, -0.2373405971270986, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 392000, Loss: -0.43202273863274604, -0.2368104914489668, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 393000, Loss: -0.42148202761774883, -0.23083094146568328, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 394000, Loss: -0.4347121612047777, -0.2393909780875547, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 395000, Loss: -0.4265185156057123, -0.23573009673308115, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 396000, Loss: -0.43160310594481416, -0.2422335887879599, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 397000, Loss: -0.4378791195328813, -0.2286569107634714, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 398000, Loss: -0.42992387974844315, -0.23160213357908652, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 399000, Loss: -0.43273740862263366, -0.23369685019145253, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 400000, Loss: -0.4273769836686552, -0.2350047857617028, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 401000, Loss: -0.42795885642128995, -0.2445794198452495, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 402000, Loss: -0.43047852767654693, -0.2389514378387248, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 403000, Loss: -0.426874548570253, -0.24504335480334702, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 404000, Loss: -0.4388437276999466, -0.24516219202196227, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 405000, Loss: -0.43276802482968196, -0.24556825043598657, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 406000, Loss: -0.43671135512879117, -0.25058605793397876, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 407000, Loss: -0.4391602368154563, -0.23655392157752067, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 408000, Loss: -0.4331015723387245, -0.23994253398454748, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 409000, Loss: -0.42718842531950213, -0.24636940712248906, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 410000, Loss: -0.4451479179630987, -0.24807209903583863, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 411000, Loss: -0.42378872134559786, -0.24281882220203987, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 412000, Loss: -0.4341535148923285, -0.24910761842154897, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 413000, Loss: -0.43985204451018944, -0.2473066779093351, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 414000, Loss: -0.4375665112815332, -0.23878570934128948, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 415000, Loss: -0.4283361376025714, -0.2382694216427626, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 416000, Loss: -0.4300278523839079, -0.25383778467494994, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 417000, Loss: -0.4383074172914494, -0.24544886916293762, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 418000, Loss: -0.4356137088942342, -0.24307817761483602, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 419000, Loss: -0.4392274142461829, -0.24583450329024345, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 420000, Loss: -0.44361909456690773, -0.250044521022588, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 421000, Loss: -0.43959000130335335, -0.2520274236020632, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 422000, Loss: -0.4412789410012774, -0.24717732152179814, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 423000, Loss: -0.44409576784819366, -0.2498877571595367, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 424000, Loss: -0.4409255477937404, -0.23840920969552826, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 425000, Loss: -0.4445648823040538, -0.25280877596617213, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 426000, Loss: -0.44253219408681616, -0.2607206713212654, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 427000, Loss: -0.4409114278116031, -0.23825773402815684, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 428000, Loss: -0.4425020724190399, -0.24775210119551047, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 429000, Loss: -0.4423370031740051, -0.24188407556258607, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 430000, Loss: -0.4415557929216884, -0.2516624018260045, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 431000, Loss: -0.43208055009366947, -0.24509910636220594, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 432000, Loss: -0.44588829598343, -0.24865198826021515, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 433000, Loss: -0.44595292356348365, -0.2494762645001756, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 434000, Loss: -0.44529548192908985, -0.24757496398349757, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 435000, Loss: -0.43917568095680326, -0.24319159357738682, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 436000, Loss: -0.4440482894021552, -0.25740044186776506, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 437000, Loss: -0.44421418856689704, -0.25146730200131423, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 438000, Loss: -0.44618192334333434, -0.24653567349619698, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 439000, Loss: -0.4431154663891066, -0.24367155917477795, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 440000, Loss: -0.43813765463989696, -0.24057847228366883, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 441000, Loss: -0.445431697845459, -0.2507792785584461, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 442000, Loss: -0.4458974268264137, -0.2409687067414634, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 443000, Loss: -0.4374931018587668, -0.24158991871634497, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 444000, Loss: -0.4444301732201129, -0.2552785696373321, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 445000, Loss: -0.4409529074698221, -0.2475096478026826, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 446000, Loss: -0.44092629809235223, -0.2442658843959216, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 447000, Loss: -0.4491393315717578, -0.24127429894299712, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 448000, Loss: -0.44291242681699805, -0.24496943231276236, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 449000, Loss: -0.44777898188028487, -0.25154722594330087, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 450000, Loss: -0.44873145898175426, -0.25006830179248934, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 451000, Loss: -0.4473222017851658, -0.254663121917285, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 452000, Loss: -0.44109414595970886, -0.24713882958539762, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 453000, Loss: -0.45528268250729886, -0.2560672278839629, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 454000, Loss: -0.4401425773613155, -0.2494030717657879, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 455000, Loss: -0.4474786934896838, -0.2480796186036896, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 456000, Loss: -0.44968229774944485, -0.2524439733147156, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 457000, Loss: -0.45172199251945133, -0.2476801510597579, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 458000, Loss: -0.45717113866237924, -0.2510110549984966, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 459000, Loss: -0.44491061081388034, -0.23952959340647795, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 460000, Loss: -0.455311294865096, -0.24753244440443814, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 461000, Loss: -0.4467891312073916, -0.25005317230976654, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 462000, Loss: -0.44757900658936706, -0.2501143929845421, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 463000, Loss: -0.45027600395306944, -0.2523135063259397, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 464000, Loss: -0.45333815619186496, -0.2502240651356988, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 465000, Loss: -0.45602508022869004, -0.2604560184155125, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 466000, Loss: -0.44744865532033146, -0.24868112089158967, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 467000, Loss: -0.454817265081685, -0.23952701909746973, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 468000, Loss: -0.45185719409945885, -0.24510533986531663, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 469000, Loss: -0.457557348679984, -0.25504114768351427, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 470000, Loss: -0.456563069766853, -0.24126953943865373, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 471000, Loss: -0.44615943244076334, -0.24971658378629946, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 472000, Loss: -0.457984252484981, -0.2542619208721444, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 473000, Loss: -0.45869349389709535, -0.25146884459024293, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 474000, Loss: -0.45143425107980145, -0.24802383702038788, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 475000, Loss: -0.44787415191764013, -0.2475708203385584, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 476000, Loss: -0.44635655513266104, -0.24960529465624132, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 477000, Loss: -0.46188035880110695, -0.25876570857246406, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 478000, Loss: -0.4561924616894685, -0.2568771654397715, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 479000, Loss: -0.4578478705857415, -0.20173543158522808, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 480000, Loss: -0.455881602620706, -0.2877932460415177, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 481000, Loss: -0.4560903802905232, -0.28828754667704926, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 482000, Loss: -0.4571379135220777, -0.26248022229946216, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 483000, Loss: -0.46171928989794103, -0.25544319961732254, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 484000, Loss: -0.462164598438656, -0.25760059368028304, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 485000, Loss: -0.4557561180193443, -0.244378027636325, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 486000, Loss: -0.4528207625048235, -0.2580285461089807, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 487000, Loss: -0.46285765668051315, -0.25590968203148806, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 488000, Loss: -0.4595674287984148, -0.25526070755976255, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 489000, Loss: -0.456610270509962, -0.25007035164441915, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 490000, Loss: -0.46064073030417785, -0.24058977064606735, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 491000, Loss: -0.4514873360453639, -0.24385315858735704, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 492000, Loss: -0.46246465123817326, -0.2518424042694969, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 493000, Loss: -0.46269728606496935, -0.2516489898553118, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 494000, Loss: -0.4575523528382182, -0.27187817947100845, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 495000, Loss: -0.45977224147086965, -0.2513631337839179, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 496000, Loss: -0.46163109855121004, -0.25082775081251746, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 497000, Loss: -0.45752506315801295, -0.2411063931400422, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 498000, Loss: -0.47308935393486173, -0.2624787015505135, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 499000, Loss: -0.46799718793970535, -0.25372748484113256, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 500000, Loss: -0.46428336776513607, -0.27883712251426185, Min Err: 0.0001423637382686138, 0.00011114864610135555\n",
      "Epoch: 501000, Loss: -0.4670096510073636, -0.2578660376665648, Min Err: 0.0001423637382686138, 0.00011114864610135555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m prepare_masked_batch(traj_ids[i])\n\u001b[1;32m     38\u001b[0m optimizer_cnmp\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mcnmp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m cnmp\u001b[38;5;241m.\u001b[39mloss(pred, tar_y, tar_mask)\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/cnep/baxter/../models/cnmp.py:43\u001b[0m, in \u001b[0;36mCNMP.forward\u001b[0;34m(self, obs, tar, obs_mask, latent)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39md_layers)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, tar, obs_mask, latent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# obs: (batch_size, n_max, input_dim+output_dim)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# tar: (batch_size, m_max, input_dim)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# obs_mask: (batch_size, n_max)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# encoding\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     encoded_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(obs)  \u001b[38;5;66;03m# (batch_size, n_max, encoder_hidden_dims[-1])\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     obs_mask_exp \u001b[38;5;241m=\u001b[39m obs_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(encoded_obs)  \u001b[38;5;66;03m# (batch_size, n_max, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3905\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   3903\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   3904\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 3905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1482\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2527\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2525\u001b[0m             args_[idx] \u001b[38;5;241m=\u001b[39m args_[idx]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 2527\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2533\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m call_func_with_args(\n\u001b[1;32m   2534\u001b[0m         compiled_fn,\n\u001b[1;32m   2535\u001b[0m         args,\n\u001b[1;32m   2536\u001b[0m         disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   2537\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1482\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3010\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, seed, offset)\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;66;03m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m-> 3010\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3016\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   3017\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:374\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:628\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs):\n\u001b[1;32m    627\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:401\u001b[0m, in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[1;32m    393\u001b[0m     compiled_graph\u001b[38;5;241m.\u001b[39mcompiled_artifact \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mload_by_key_path(\n\u001b[1;32m    394\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mcache_key,\n\u001b[1;32m    395\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39martifact_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m (),\n\u001b[1;32m    399\u001b[0m     )\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_yigit/t6/ct6dnbpm762taoyrtc3fgugwee7npogzc2mhuvnveldv2rzvrvo4.py:224\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    222\u001b[0m buf0 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m512\u001b[39m), (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Source Nodes: [], Original ATen: []\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m \u001b[43mextern_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1284\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1284\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1284\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1284\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m primals_1\n\u001b[1;32m    226\u001b[0m buf1 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m512\u001b[39m), (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/baxter/cnmp_cnep/mobile_net/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "\n",
    "epochs = 5_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = v_num_demos//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_cnmp, avg_loss_cnep = 0, 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_vl_cnmp, min_vl_cnep = 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "tl_cnmp, tl_cnep = [], []\n",
    "ve_cnmp, ve_cnep = [], []\n",
    "\n",
    "cnmp_tl_path, cnep_tl_path = f'{root_folder}cnmp_training_loss.pt', f'{root_folder}cnep_training_loss.pt'\n",
    "cnmp_ve_path, cnep_ve_path = f'{root_folder}cnmp_validation_error.pt', f'{root_folder}cnep_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_cnmp, epoch_loss_cnep = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(traj_ids[i])\n",
    "\n",
    "        optimizer_cnmp.zero_grad()\n",
    "        pred = cnmp(obs, tar_x, obs_mask)\n",
    "        loss = cnmp.loss(pred, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer_cnmp.step()\n",
    "\n",
    "        epoch_loss_cnmp += loss.item()\n",
    "\n",
    "        optimizer_cnep.zero_grad()\n",
    "        pred, gate = cnep(obs, tar_x, obs_mask)\n",
    "        loss, nll = cnep.loss(pred, gate, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer_cnep.step()\n",
    "\n",
    "        epoch_loss_cnep += nll.item()\n",
    "\n",
    "    epoch_loss_cnmp = epoch_loss_cnmp/epoch_iter\n",
    "    tl_cnmp.append(epoch_loss_cnmp)\n",
    "    epoch_loss_cnep = epoch_loss_cnep/epoch_iter\n",
    "    tl_cnep.append(epoch_loss_cnep)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(v_num_demos)[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_err_cnmp, val_err_cnep = 0, 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                prepare_masked_val_batch(v_traj_ids[j])\n",
    "\n",
    "                p = cnmp.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                vp_means = p[:, :, :dy]\n",
    "                val_err_cnmp += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "                p, g = cnep.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                dec_id = torch.argmax(g.squeeze(1), dim=-1)\n",
    "                vp_means = p[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_err_cnep += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "            val_err_cnmp = val_err_cnmp/(v_epoch_iter*t_steps)\n",
    "            val_err_cnep = val_err_cnep/(v_epoch_iter*t_steps)\n",
    "\n",
    "            if val_err_cnmp < min_vl_cnmp:\n",
    "                min_vl_cnmp = val_err_cnmp\n",
    "                print(f'CNMP New best: {min_vl_cnmp}')\n",
    "                torch.save(cnmp_.state_dict(), f'{root_folder}saved_models/cnmp.pt')\n",
    "\n",
    "            if val_err_cnep < min_vl_cnep:\n",
    "                min_vl_cnep = val_err_cnep\n",
    "                print(f'CNEP New best: {min_vl_cnep}')\n",
    "                torch.save(cnep_.state_dict(), f'{root_folder}saved_models/cnep.pt')\n",
    "\n",
    "            ve_cnmp.append(val_err_cnmp)\n",
    "            ve_cnep.append(val_err_cnep)\n",
    "\n",
    "    avg_loss_cnmp += epoch_loss_cnmp\n",
    "    avg_loss_cnep += epoch_loss_cnep\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, {}, Min Err: {}, {}\".format(epoch, avg_loss_cnmp/val_per_epoch, avg_loss_cnep/val_per_epoch, min_vl_cnmp, min_vl_cnep))\n",
    "        avg_loss_cnmp, avg_loss_cnep = 0, 0\n",
    "\n",
    "    if epoch % 500_000 == 0 and epoch > 1:\n",
    "        torch.save(cnmp_.state_dict(), f'{root_folder}saved_models/last_cnmp.pt')\n",
    "        torch.save(cnep_.state_dict(), f'{root_folder}saved_models/last_cnep.pt')\n",
    "\n",
    "torch.save(torch.Tensor(tl_cnmp), cnmp_tl_path)\n",
    "torch.save(torch.Tensor(ve_cnmp), cnmp_ve_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
