{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Identity()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "is_save = True\n",
    "extract = True\n",
    "device = 'cuda:0'\n",
    "\n",
    "dy = 3\n",
    "\n",
    "# Load pre-trained MobileNetV2\n",
    "model = models.mobilenet_v2(pretrained=True).to(device)\n",
    "\n",
    "# Remove classification head (if you only need features)\n",
    "model.classifier = torch.nn.Identity()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 4\n",
    "modes = [0, 1, 1, 3]\n",
    "num_demos = 10\n",
    "t_steps = 400\n",
    "dims = 1280  # MobileNetV2 feature size\n",
    "feats = torch.zeros(num_demos*num_modes, dims)\n",
    "trajs3 = torch.zeros(num_demos*num_modes, t_steps, 3)\n",
    "trajs8 = torch.zeros(num_demos*num_modes, t_steps, 8)\n",
    "\n",
    "minmax3 = torch.zeros(3, 2)\n",
    "minmax8 = torch.zeros(8, 2)\n",
    "\n",
    "def crop_left(im): \n",
    "    return transforms.functional.crop(im, top=0, left=0, height=300, width=480)\n",
    "\n",
    "if extract:\n",
    "    for m, mode in enumerate(modes):\n",
    "        for i in range(num_demos):\n",
    "            ind = m*num_demos + i\n",
    "            img_path = f'data/{mode}/{i}/img.jpeg'\n",
    "            img = Image.open(img_path).convert('RGB')  # Load image using PIL\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Lambda(crop_left),  # Crop the top-left side\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            img = transform(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                features = model(img) \n",
    "            feats[ind] = torch.flatten(features) / dims\n",
    "\n",
    "            data_folder = f'/home/yigit/projects/cnep/baxter/data/{mode}/{i}/'\n",
    "            # iterate over all files in the data_folder\n",
    "            for filename in os.listdir(data_folder):\n",
    "                d = os.path.join(data_folder, filename)\n",
    "                if filename.endswith('.csv'):\n",
    "                    temp_data_3, temp_data_8 = [], []\n",
    "                    with open(d, 'r') as f:\n",
    "                        for j, line in enumerate(csv.reader(f)):\n",
    "                            if j > 0:\n",
    "                                temp_data_8.append([float(line[3]), float(line[4]), float(line[5]), float(line[6]), float(line[7]), float(line[8]), float(line[9]), float(line[9])])  # p, q, gripper\n",
    "                                temp_data_3.append([float(line[3]), float(line[4]), float(line[5])])  # p\n",
    "\n",
    "            ids = torch.linspace(0, len(temp_data_3)-1, t_steps).int()\n",
    "            for j in range(t_steps):\n",
    "                trajs3[ind, j] = torch.tensor(temp_data_3[ids[j]])\n",
    "                trajs8[ind, j] = torch.tensor(temp_data_8[ids[j]])\n",
    "\n",
    "    # map each dimension of the trajectory into the range [-1, 1], keep min and max values for each dimension to later unmap the values\n",
    "    for k in range(3):\n",
    "        min_val, max_val = trajs3[:, :, k].min(), trajs3[:, :, k].max()\n",
    "        trajs3[:, :, k] = 2 * (trajs3[:, :, k] - min_val) / (max_val - min_val) - 1\n",
    "        minmax3[k] = torch.tensor([min_val, max_val])\n",
    "    for k in range(8):\n",
    "        min_val, max_val = trajs8[:, :, k].min(), trajs8[:, :, k].max()\n",
    "        trajs8[:, :, k] = 2 * (trajs8[:, :, k] - min_val) / (max_val - min_val) - 1\n",
    "        minmax8[k] = torch.tensor([min_val, max_val])\n",
    "\n",
    "    if is_save:\n",
    "        torch.save(trajs3, '0_1_2_3/trajs_normalized_3.pt')\n",
    "        torch.save(trajs8, '0_1_2_3/trajs_normalized_8.pt')\n",
    "        torch.save(feats, '0_1_2_3/feats_mn.pt')\n",
    "        torch.save(minmax3, '0_1_2_3/minmax3.pt')\n",
    "        torch.save(minmax8, '0_1_2_3/minmax8.pt')\n",
    "\n",
    "    if dy == 3:\n",
    "        trajs = trajs3\n",
    "    else:\n",
    "        trajs = trajs8\n",
    "else:\n",
    "    if dy == 3:\n",
    "        trajs = torch.load('0_1_2_3/trajs_normalized_3.pt')\n",
    "    else:\n",
    "        trajs = torch.load('0_1_2_3/trajs_normalized_8.pt')\n",
    "    feats = torch.load('0_1_2_3/feats.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnep import CNEP\n",
    "from cnmp import CNMP\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 400, 3]) torch.Size([4, 400, 3]) torch.Size([4, 1280]) torch.Size([4, 1280])\n"
     ]
    }
   ],
   "source": [
    "num_demos, v_num_demos = 32, 8\n",
    "num_classes = num_modes\n",
    "num_indiv = num_demos // num_classes  # Number of trajectories per mode\n",
    "num_val_indiv = v_num_demos // num_classes  # Number of trajectories per mode\n",
    "\n",
    "dx = 1\n",
    "dg = dims\n",
    "batch_size = 4\n",
    "n_max, m_max = 20, 20\n",
    "\n",
    "perm_ids = torch.randperm(num_demos + v_num_demos)\n",
    "train_ids, val_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "train_trajs = torch.zeros(num_demos, t_steps, dy)\n",
    "train_feats = torch.zeros(num_demos, dg)\n",
    "val_trajs = torch.zeros(v_num_demos, t_steps, dy)\n",
    "val_feats = torch.zeros(v_num_demos, dg)\n",
    "\n",
    "for i in range(num_modes):\n",
    "    perm_ids = torch.randperm(num_indiv + num_val_indiv)\n",
    "    train_ids, val_ids = perm_ids[:num_indiv], perm_ids[num_indiv:]\n",
    "    train_trajs[i*num_indiv:(i+1)*num_indiv] = trajs[train_ids + i*num_indiv]\n",
    "    train_feats[i*num_indiv:(i+1)*num_indiv] = feats[train_ids + i*num_indiv]\n",
    "    val_trajs[i*num_val_indiv:(i+1)*num_val_indiv] = trajs[val_ids + i*num_indiv]\n",
    "    val_feats[i*num_val_indiv:(i+1)*num_val_indiv] = feats[val_ids + i*num_indiv]\n",
    "\n",
    "\n",
    "# train_trajs, val_trajs = trajs[train_ids], trajs[val_ids]\n",
    "# train_feats, val_feats = feats[train_ids], feats[val_ids]\n",
    "# train_trajs, val_trajs = trajs, trajs\n",
    "# train_feats, val_feats = feats, feats\n",
    "\n",
    "print(train_trajs.shape, val_trajs.shape, train_feats.shape, val_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(traj_ids: list):\n",
    "    obs.fill_(0)\n",
    "    tar_x.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = train_trajs[traj_id]\n",
    "        feat = train_feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "        m = torch.randint(1, m_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "        \n",
    "        obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)  # X\n",
    "        obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)  # G\n",
    "        obs[i, :n, dx+dg:] = traj[n_ids]  # Y\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x[i, :m, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        tar_x[i, :m, dx:] = feat.repeat(m, 1)\n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "val_obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_val_batch(traj_ids: list):\n",
    "    val_obs.fill_(0)\n",
    "    val_tar_x.fill_(0)\n",
    "    val_tar_y.fill_(0)\n",
    "    val_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = val_trajs[traj_id]\n",
    "        feat = val_feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "        \n",
    "        val_obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)\n",
    "        val_obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)\n",
    "        val_obs[i, :n, dx+dg:] = traj[n_ids]\n",
    "        val_obs_mask[i, :n] = True\n",
    "        \n",
    "        val_tar_x[i, :, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        val_tar_x[i, :, dx:] = feat.repeat(t_steps, 1)\n",
    "        val_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnep: 1648028\n",
      "cnmp: 1647494\n"
     ]
    }
   ],
   "source": [
    "cnep_ = CNEP(dx+dg, dy, n_max, n_max, [256, 256, 256], num_decoders=4, decoder_hidden_dims=[256, 256], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "optimizer_cnep = torch.optim.Adam(lr=3e-4, params=cnep_.parameters())\n",
    "\n",
    "cnmp_ = CNMP(dx+dg, dy, n_max, m_max, [256, 256, 256], decoder_hidden_dims=[1024, 1024], batch_size=batch_size, device=device)\n",
    "optimizer_cnmp = torch.optim.Adam(lr=3e-4, params=cnmp_.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"cnep:\", get_parameter_count(cnep_))\n",
    "print(\"cnmp:\", get_parameter_count(cnmp_))\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    cnep, cnmp = torch.compile(cnep_), torch.compile(cnmp_)\n",
    "else:\n",
    "    cnep, cnmp = cnep_, cnmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNMP New best: 0.0006814492493867875\n",
      "CNEP New best: 0.0006948132067918777\n",
      "Epoch: 0, Loss: 0.0008839625120162964, 0.00021811747550964355, Min Err: 0.0006814492493867875, 0.0006948132067918777\n",
      "CNMP New best: 0.00044748734682798386\n",
      "CNEP New best: 0.0005537591502070427\n",
      "Epoch: 1000, Loss: 0.5072024204581976, 0.15265316646546126, Min Err: 0.00044748734682798386, 0.0005537591502070427\n",
      "CNMP New best: 0.00026476817205548284\n",
      "CNEP New best: 0.0005020433664321899\n",
      "Epoch: 2000, Loss: 0.09692001095134765, 0.10663727197796107, Min Err: 0.00026476817205548284, 0.0005020433664321899\n",
      "CNMP New best: 0.0002106320671737194\n",
      "CNEP New best: 0.0004623544216156006\n",
      "Epoch: 3000, Loss: -0.1190351165542379, 0.07452017688099295, Min Err: 0.0002106320671737194, 0.0004623544216156006\n",
      "CNMP New best: 0.00017972515895962716\n",
      "CNEP New best: 0.00037844061851501467\n",
      "Epoch: 4000, Loss: -0.23201159683428704, 0.042685728984419254, Min Err: 0.00017972515895962716, 0.00037844061851501467\n",
      "CNMP New best: 0.0001593063399195671\n",
      "CNEP New best: 0.0003627507761120796\n",
      "Epoch: 5000, Loss: -0.32727991335000844, 0.017422546085901557, Min Err: 0.0001593063399195671, 0.0003627507761120796\n",
      "CNMP New best: 0.00015480502508580685\n",
      "CNEP New best: 0.00033678464591503144\n",
      "Epoch: 6000, Loss: -0.35818969273939727, 0.0017582836055662482, Min Err: 0.00015480502508580685, 0.00033678464591503144\n",
      "CNMP New best: 0.00014312705025076866\n",
      "CNEP New best: 0.0003076433949172497\n",
      "Epoch: 7000, Loss: -0.4376157644446939, -0.014187395137734711, Min Err: 0.00014312705025076866, 0.0003076433949172497\n",
      "CNMP New best: 0.0001370216440409422\n",
      "Epoch: 8000, Loss: -0.5155224281400442, -0.03508210049243644, Min Err: 0.0001370216440409422, 0.0003076433949172497\n",
      "CNMP New best: 0.00013581736013293266\n",
      "CNEP New best: 0.00026343777775764463\n",
      "Epoch: 9000, Loss: -0.5738417861312628, -0.0454177603230346, Min Err: 0.00013581736013293266, 0.00026343777775764463\n",
      "CNEP New best: 0.00024400796741247177\n",
      "Epoch: 10000, Loss: -0.6377323583513498, -0.05788582742563449, Min Err: 0.00013581736013293266, 0.00024400796741247177\n",
      "CNMP New best: 0.00012594224885106086\n",
      "CNEP New best: 0.0002078128233551979\n",
      "Epoch: 11000, Loss: -0.6427532063573599, -0.06398753832071088, Min Err: 0.00012594224885106086, 0.0002078128233551979\n",
      "CNEP New best: 0.00018587669357657431\n",
      "Epoch: 12000, Loss: -0.6579311452638358, -0.07846406460693106, Min Err: 0.00012594224885106086, 0.00018587669357657431\n",
      "CNMP New best: 0.00011775931343436242\n",
      "CNEP New best: 0.00018014229834079743\n",
      "Epoch: 13000, Loss: -0.6978452349156141, -0.09211733590136283, Min Err: 0.00011775931343436242, 0.00018014229834079743\n",
      "CNMP New best: 0.00011494116857647896\n",
      "CNEP New best: 0.00015522776171565057\n",
      "Epoch: 14000, Loss: -0.7666552464365959, -0.10025034704431891, Min Err: 0.00011494116857647896, 0.00015522776171565057\n",
      "CNMP New best: 0.00011057286523282529\n",
      "CNEP New best: 0.00014943201094865798\n",
      "Epoch: 15000, Loss: -0.8073354976177216, -0.11194147211220115, Min Err: 0.00011057286523282529, 0.00014943201094865798\n",
      "CNMP New best: 0.00010737994685769081\n",
      "CNEP New best: 0.00014208277687430382\n",
      "Epoch: 16000, Loss: -0.8468472137600184, -0.12287731523253023, Min Err: 0.00010737994685769081, 0.00014208277687430382\n",
      "CNEP New best: 0.00013488450087606907\n",
      "Epoch: 17000, Loss: -0.8874243775457143, -0.1320339632332325, Min Err: 0.00010737994685769081, 0.00013488450087606907\n",
      "CNMP New best: 0.00010221030563116073\n",
      "CNEP New best: 0.0001264985091984272\n",
      "Epoch: 18000, Loss: -0.9202923632413149, -0.14176426917687057, Min Err: 0.00010221030563116073, 0.0001264985091984272\n",
      "CNEP New best: 0.0001263049989938736\n",
      "Epoch: 19000, Loss: -0.907235908806324, -0.13773908937349916, Min Err: 0.00010221030563116073, 0.0001263049989938736\n",
      "CNMP New best: 0.00010202164761722088\n",
      "CNEP New best: 0.000125410296022892\n",
      "Epoch: 20000, Loss: -0.9447157763466238, -0.15533604059182107, Min Err: 0.00010202164761722088, 0.000125410296022892\n",
      "CNEP New best: 0.00012018857523798943\n",
      "Epoch: 21000, Loss: -0.8775685047768056, -0.15757267823629081, Min Err: 0.00010202164761722088, 0.00012018857523798943\n",
      "CNEP New best: 0.00011732690036296845\n",
      "Epoch: 22000, Loss: -1.0093273931443691, -0.17138634310662745, Min Err: 0.00010202164761722088, 0.00011732690036296845\n",
      "CNEP New best: 0.00011586283333599568\n",
      "Epoch: 23000, Loss: -1.0025483270734548, -0.17782032340578735, Min Err: 0.00010202164761722088, 0.00011586283333599568\n",
      "CNEP New best: 0.00011346240527927876\n",
      "Epoch: 24000, Loss: -1.0549991681277753, -0.18321226826496423, Min Err: 0.00010202164761722088, 0.00011346240527927876\n",
      "Epoch: 25000, Loss: -1.0794978251755238, -0.18157536481320857, Min Err: 0.00010202164761722088, 0.00011346240527927876\n",
      "CNMP New best: 0.00010037253610789776\n",
      "CNEP New best: 0.00010994963347911835\n",
      "Epoch: 26000, Loss: -1.1117234539687633, -0.19442821603268384, Min Err: 0.00010037253610789776, 0.00010994963347911835\n",
      "CNEP New best: 0.00010770131833851338\n",
      "Epoch: 27000, Loss: -1.11581255030632, -0.20745093878358603, Min Err: 0.00010037253610789776, 0.00010770131833851338\n",
      "CNEP New best: 0.0001046660728752613\n",
      "Epoch: 28000, Loss: -1.1373811952471733, -0.21116141514480113, Min Err: 0.00010037253610789776, 0.0001046660728752613\n",
      "Epoch: 29000, Loss: -1.151887659817934, -0.1996947007365525, Min Err: 0.00010037253610789776, 0.0001046660728752613\n",
      "CNEP New best: 0.00010316853411495685\n",
      "Epoch: 30000, Loss: -1.1725020877420902, -0.2197185971289873, Min Err: 0.00010037253610789776, 0.00010316853411495685\n",
      "CNEP New best: 0.00010184790007770062\n",
      "Epoch: 31000, Loss: -1.1623162621855736, -0.22732008837535977, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 32000, Loss: -1.2248112504780293, -0.23279362185299396, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 33000, Loss: -1.249190849751234, -0.23383206001669168, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 34000, Loss: -1.2616846564412116, -0.2418188215047121, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 35000, Loss: -1.2580919972658158, -0.24854380045831204, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 36000, Loss: -1.3025693647563457, -0.2498124305009842, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 37000, Loss: -1.3225812909603119, -0.24109840191714466, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 38000, Loss: -1.323868268430233, -0.2587421311251819, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 39000, Loss: -1.3548204639405013, -0.2452978453077376, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 40000, Loss: -1.3094117428064347, -0.2670512538030744, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 41000, Loss: -1.3855135000050067, -0.2700959817245603, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 42000, Loss: -1.359209845393896, -0.2741246490329504, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 43000, Loss: -1.4190598556101321, -0.27942877121269705, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 44000, Loss: -1.4322987812161445, -0.2830957938581705, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 45000, Loss: -1.3597756600677966, -0.2900518012195826, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 46000, Loss: -1.4713714933991433, -0.2884946368634701, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 47000, Loss: -1.4682143171429634, -0.2905181629359722, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 48000, Loss: -1.4973663613796233, -0.29867046196758745, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 49000, Loss: -1.520052962243557, -0.30223880833387373, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 50000, Loss: -1.4954568243324757, -0.2684123311266303, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 51000, Loss: -1.5444576554894447, -0.3101723076850176, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 52000, Loss: -1.5418650742769242, -0.19591083811363205, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 53000, Loss: -1.5285082746148109, -0.2852593340165913, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 54000, Loss: -1.577535542845726, -0.3056151565164328, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 55000, Loss: -1.5461580758094788, -0.3161785282343626, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 56000, Loss: -1.5943424779176711, -0.32476819248497485, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 57000, Loss: -1.6019394102692603, -0.3306237488389015, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 58000, Loss: -1.616651017010212, -0.3333811691403389, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 59000, Loss: -1.6207358309030533, -0.3320215725526214, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 60000, Loss: -1.6491535281538963, -0.3426187172085047, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 61000, Loss: -1.5673127422779798, -0.3334115500897169, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 62000, Loss: -1.6546353929638862, -0.33677838832885026, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 63000, Loss: -1.6828947789669038, -0.3537465533018112, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "Epoch: 64000, Loss: -1.6810402258634567, -0.3522753791511059, Min Err: 0.00010037253610789776, 0.00010184790007770062\n",
      "CNMP New best: 9.868880733847618e-05\n",
      "Epoch: 65000, Loss: -1.693211370050907, -0.3501148242950439, Min Err: 9.868880733847618e-05, 0.00010184790007770062\n",
      "Epoch: 66000, Loss: -1.727764086186886, -0.3554751687347889, Min Err: 9.868880733847618e-05, 0.00010184790007770062\n",
      "Epoch: 67000, Loss: -1.731281684577465, -0.35399547112733126, Min Err: 9.868880733847618e-05, 0.00010184790007770062\n",
      "Epoch: 68000, Loss: -1.7146642137169839, -0.3588910398185253, Min Err: 9.868880733847618e-05, 0.00010184790007770062\n",
      "Epoch: 69000, Loss: -1.7656992085576058, -0.36570807164907454, Min Err: 9.868880733847618e-05, 0.00010184790007770062\n",
      "CNMP New best: 9.696166031062602e-05\n",
      "Epoch: 70000, Loss: -1.7537369910478593, -0.33978042770642786, Min Err: 9.696166031062602e-05, 0.00010184790007770062\n",
      "Epoch: 71000, Loss: -1.7718028493523599, -0.3653995841294527, Min Err: 9.696166031062602e-05, 0.00010184790007770062\n",
      "CNMP New best: 9.016010910272598e-05\n",
      "Epoch: 72000, Loss: -1.7651524004936219, -0.36488226102292537, Min Err: 9.016010910272598e-05, 0.00010184790007770062\n",
      "Epoch: 73000, Loss: -1.7977855194211005, -0.36940259163081646, Min Err: 9.016010910272598e-05, 0.00010184790007770062\n",
      "CNMP New best: 8.685316890478135e-05\n",
      "Epoch: 74000, Loss: -1.7995018696188927, -0.3699661842286587, Min Err: 8.685316890478135e-05, 0.00010184790007770062\n",
      "Epoch: 75000, Loss: -1.8476351323127747, -0.3688895512521267, Min Err: 8.685316890478135e-05, 0.00010184790007770062\n",
      "CNMP New best: 8.60331580042839e-05\n",
      "Epoch: 76000, Loss: -1.8431914656162263, -0.3755279746055603, Min Err: 8.60331580042839e-05, 0.00010184790007770062\n",
      "CNMP New best: 8.165528997778892e-05\n",
      "Epoch: 77000, Loss: -1.841909265756607, -0.3769181880652904, Min Err: 8.165528997778892e-05, 0.00010184790007770062\n",
      "Epoch: 78000, Loss: -1.854189970523119, -0.3812010373175144, Min Err: 8.165528997778892e-05, 0.00010184790007770062\n",
      "Epoch: 79000, Loss: -1.8614227914214134, -0.383973080933094, Min Err: 8.165528997778892e-05, 0.00010184790007770062\n",
      "CNMP New best: 7.75574380531907e-05\n",
      "Epoch: 80000, Loss: -1.8929191751480103, -0.38231837379932404, Min Err: 7.75574380531907e-05, 0.00010184790007770062\n",
      "Epoch: 81000, Loss: -1.8647110419273376, -0.3796822026446462, Min Err: 7.75574380531907e-05, 0.00010184790007770062\n",
      "Epoch: 82000, Loss: -1.926133672952652, -0.3829492769762874, Min Err: 7.75574380531907e-05, 0.00010184790007770062\n",
      "CNMP New best: 7.367919664829969e-05\n",
      "Epoch: 83000, Loss: -1.8887675251364708, -0.3884153452813625, Min Err: 7.367919664829969e-05, 0.00010184790007770062\n",
      "Epoch: 84000, Loss: -1.929969557762146, -0.38974800042808055, Min Err: 7.367919664829969e-05, 0.00010184790007770062\n",
      "CNMP New best: 7.076934445649386e-05\n",
      "Epoch: 85000, Loss: -1.9311731933355332, -0.3924110012054443, Min Err: 7.076934445649386e-05, 0.00010184790007770062\n",
      "Epoch: 86000, Loss: -1.9359037238359451, -0.3947232385277748, Min Err: 7.076934445649386e-05, 0.00010184790007770062\n",
      "Epoch: 87000, Loss: -1.9326017279028893, -0.39696835772693156, Min Err: 7.076934445649386e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.817596964538097e-05\n",
      "Epoch: 88000, Loss: -1.9374436374902726, -0.4018066205531359, Min Err: 6.817596964538097e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.792206782847643e-05\n",
      "Epoch: 89000, Loss: -1.9731492428779602, -0.3882029826417565, Min Err: 6.792206782847643e-05, 0.00010184790007770062\n",
      "Epoch: 90000, Loss: -1.9636297398805618, -0.4043074043542147, Min Err: 6.792206782847643e-05, 0.00010184790007770062\n",
      "Epoch: 91000, Loss: -1.9829316143989564, -0.40611738787591456, Min Err: 6.792206782847643e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.659959908574819e-05\n",
      "Epoch: 92000, Loss: -1.9983762662410736, -0.4109355014115572, Min Err: 6.659959908574819e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.638756953179836e-05\n",
      "Epoch: 93000, Loss: -2.009649256467819, -0.40491426080465315, Min Err: 6.638756953179836e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.444771308451891e-05\n",
      "Epoch: 94000, Loss: -1.9651614962816237, -0.41081197568774225, Min Err: 6.444771308451891e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.245923228561878e-05\n",
      "Epoch: 95000, Loss: -1.9800890627205372, -0.40955295234918593, Min Err: 6.245923228561878e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.036089267581701e-05\n",
      "Epoch: 96000, Loss: -2.0098748262524606, -0.4149353402704, Min Err: 6.036089267581701e-05, 0.00010184790007770062\n",
      "Epoch: 97000, Loss: -2.0024890122413637, -0.34751865760609507, Min Err: 6.036089267581701e-05, 0.00010184790007770062\n",
      "CNMP New best: 6.0213045217096805e-05\n",
      "Epoch: 98000, Loss: -2.011629742503166, -0.4115157966166735, Min Err: 6.0213045217096805e-05, 0.00010184790007770062\n",
      "CNMP New best: 5.7408963330090045e-05\n",
      "Epoch: 99000, Loss: -2.03812689435482, -0.41389614333212377, Min Err: 5.7408963330090045e-05, 0.00010184790007770062\n",
      "Epoch: 100000, Loss: -2.021759886085987, -0.4225360253602266, Min Err: 5.7408963330090045e-05, 0.00010184790007770062\n",
      "Epoch: 101000, Loss: -2.0572657067775726, -0.4186738059222698, Min Err: 5.7408963330090045e-05, 0.00010184790007770062\n",
      "Epoch: 102000, Loss: -2.0562976919412614, -0.3914117113798857, Min Err: 5.7408963330090045e-05, 0.00010184790007770062\n",
      "CNMP New best: 5.47135341912508e-05\n",
      "Epoch: 103000, Loss: -2.045957104921341, -0.4198288151919842, Min Err: 5.47135341912508e-05, 0.00010184790007770062\n",
      "Epoch: 104000, Loss: -2.043264849036932, -0.41940990841388703, Min Err: 5.47135341912508e-05, 0.00010184790007770062\n",
      "Epoch: 105000, Loss: -2.050742660820484, -0.286159060548991, Min Err: 5.47135341912508e-05, 0.00010184790007770062\n",
      "CNMP New best: 5.444113630801439e-05\n",
      "Epoch: 106000, Loss: -2.081850592494011, -0.3390032095611095, Min Err: 5.444113630801439e-05, 0.00010184790007770062\n",
      "CNMP New best: 5.0565260462462905e-05\n",
      "Epoch: 107000, Loss: -2.0586411973237992, -0.36290618099272254, Min Err: 5.0565260462462905e-05, 0.00010184790007770062\n",
      "CNMP New best: 5.022929050028324e-05\n",
      "Epoch: 108000, Loss: -2.092854650616646, -0.37805198825895786, Min Err: 5.022929050028324e-05, 0.00010184790007770062\n",
      "Epoch: 109000, Loss: -2.0581925877332687, -0.3837328889667988, Min Err: 5.022929050028324e-05, 0.00010184790007770062\n",
      "Epoch: 110000, Loss: -2.1101952942609787, -0.38616799291968346, Min Err: 5.022929050028324e-05, 0.00010184790007770062\n",
      "CNMP New best: 4.7855484299361704e-05\n",
      "Epoch: 111000, Loss: -2.0842218981981278, -0.39339115826785565, Min Err: 4.7855484299361704e-05, 0.00010184790007770062\n",
      "Epoch: 112000, Loss: -2.0879674047231673, -0.42845866072177885, Min Err: 4.7855484299361704e-05, 0.00010184790007770062\n",
      "CNMP New best: 4.489550832659006e-05\n",
      "Epoch: 113000, Loss: -2.1164907647371294, -0.4321883403360844, Min Err: 4.489550832659006e-05, 0.00010184790007770062\n",
      "Epoch: 114000, Loss: -2.111397082090378, -0.43518037380278113, Min Err: 4.489550832659006e-05, 0.00010184790007770062\n",
      "Epoch: 115000, Loss: -2.0955379613637923, -0.43515970471501353, Min Err: 4.489550832659006e-05, 0.00010184790007770062\n",
      "Epoch: 116000, Loss: -2.1059129214286805, -0.4369288578480482, Min Err: 4.489550832659006e-05, 0.00010184790007770062\n",
      "CNMP New best: 4.2705261148512366e-05\n",
      "Epoch: 117000, Loss: -2.117251068294048, -0.43741868712008, Min Err: 4.2705261148512366e-05, 0.00010184790007770062\n",
      "Epoch: 118000, Loss: -2.1489785232543945, -0.4403165731430054, Min Err: 4.2705261148512366e-05, 0.00010184790007770062\n",
      "CNMP New best: 4.126366227865219e-05\n",
      "Epoch: 119000, Loss: -2.0872728177309035, -0.4425880691707134, Min Err: 4.126366227865219e-05, 0.00010184790007770062\n",
      "Epoch: 120000, Loss: -2.143518855214119, -0.4429236863702536, Min Err: 4.126366227865219e-05, 0.00010184790007770062\n",
      "Epoch: 121000, Loss: -2.154343347907066, -0.44935690486431124, Min Err: 4.126366227865219e-05, 0.00010184790007770062\n",
      "Epoch: 122000, Loss: -2.1534389932751656, -0.44483694578707217, Min Err: 4.126366227865219e-05, 0.00010184790007770062\n",
      "Epoch: 123000, Loss: -2.1771234840750693, -0.4554520947635174, Min Err: 4.126366227865219e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.809767309576273e-05\n",
      "Epoch: 124000, Loss: -2.1481857733130454, -0.4517739418745041, Min Err: 3.809767309576273e-05, 0.00010184790007770062\n",
      "Epoch: 125000, Loss: -2.171305162191391, -0.4568574064671993, Min Err: 3.809767309576273e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.703923430293798e-05\n",
      "Epoch: 126000, Loss: -2.1845154975652696, -0.4063092960081994, Min Err: 3.703923430293798e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.62519477494061e-05\n",
      "Epoch: 127000, Loss: -2.1890718922019006, -0.45485717311501506, Min Err: 3.62519477494061e-05, 0.00010184790007770062\n",
      "Epoch: 128000, Loss: -2.1992816363573073, -0.45651564140617845, Min Err: 3.62519477494061e-05, 0.00010184790007770062\n",
      "Epoch: 129000, Loss: -2.213760969936848, -0.46383278886973855, Min Err: 3.62519477494061e-05, 0.00010184790007770062\n",
      "Epoch: 130000, Loss: -2.160071078002453, -0.4519116320014, Min Err: 3.62519477494061e-05, 0.00010184790007770062\n",
      "Epoch: 131000, Loss: -2.1754943525791166, -0.4612815994843841, Min Err: 3.62519477494061e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.5005402751266956e-05\n",
      "Epoch: 132000, Loss: -2.205754975557327, -0.4643354616612196, Min Err: 3.5005402751266956e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.431461285799742e-05\n",
      "Epoch: 133000, Loss: -2.211692255556583, -0.46626013235747815, Min Err: 3.431461285799742e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.413900965824723e-05\n",
      "Epoch: 134000, Loss: -2.20521770593524, -0.4672214149832726, Min Err: 3.413900965824723e-05, 0.00010184790007770062\n",
      "Epoch: 135000, Loss: -2.2050181437730787, -0.4670004612803459, Min Err: 3.413900965824723e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.394477535039187e-05\n",
      "Epoch: 136000, Loss: -2.224709789276123, -0.4682093871831894, Min Err: 3.394477535039187e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.190677613019943e-05\n",
      "Epoch: 137000, Loss: -2.2382175130844115, -0.4678331967741251, Min Err: 3.190677613019943e-05, 0.00010184790007770062\n",
      "Epoch: 138000, Loss: -2.2041391258239744, -0.4649912955611944, Min Err: 3.190677613019943e-05, 0.00010184790007770062\n",
      "Epoch: 139000, Loss: -2.248437172651291, -0.47432450047135355, Min Err: 3.190677613019943e-05, 0.00010184790007770062\n",
      "Epoch: 140000, Loss: -2.2529406049251555, -0.4760434932410717, Min Err: 3.190677613019943e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.0282542575150728e-05\n",
      "Epoch: 141000, Loss: -2.253951339006424, -0.4764854877442122, Min Err: 3.0282542575150728e-05, 0.00010184790007770062\n",
      "Epoch: 142000, Loss: -2.2312801302671432, -0.4817665580660105, Min Err: 3.0282542575150728e-05, 0.00010184790007770062\n",
      "Epoch: 143000, Loss: -2.2798308787345887, -0.47892832443118094, Min Err: 3.0282542575150728e-05, 0.00010184790007770062\n",
      "CNMP New best: 3.027539001777768e-05\n",
      "CNEP New best: 9.975147433578968e-05\n",
      "Epoch: 144000, Loss: -2.2736432186365128, -0.483938149780035, Min Err: 3.027539001777768e-05, 9.975147433578968e-05\n",
      "CNMP New best: 2.8676141519099473e-05\n",
      "Epoch: 145000, Loss: -2.2604351808428764, -0.4807836843430996, Min Err: 2.8676141519099473e-05, 9.975147433578968e-05\n",
      "Epoch: 146000, Loss: -2.2863035012483595, -0.4240102144926786, Min Err: 2.8676141519099473e-05, 9.975147433578968e-05\n",
      "Epoch: 147000, Loss: -2.2799243181943893, -0.4799557998776436, Min Err: 2.8676141519099473e-05, 9.975147433578968e-05\n",
      "CNMP New best: 2.852923236787319e-05\n",
      "CNEP New best: 9.608188644051552e-05\n",
      "Epoch: 148000, Loss: -2.258214566230774, -0.4830586037784815, Min Err: 2.852923236787319e-05, 9.608188644051552e-05\n",
      "Epoch: 149000, Loss: -2.2580850754380224, -0.4848657410144806, Min Err: 2.852923236787319e-05, 9.608188644051552e-05\n",
      "Epoch: 150000, Loss: -2.291797769129276, -0.49123978862166406, Min Err: 2.852923236787319e-05, 9.608188644051552e-05\n",
      "Epoch: 151000, Loss: -2.277877349615097, -0.4928887805044651, Min Err: 2.852923236787319e-05, 9.608188644051552e-05\n",
      "CNMP New best: 2.7369956951588394e-05\n",
      "CNEP New best: 9.574869647622109e-05\n",
      "Epoch: 152000, Loss: -2.308606462955475, -0.4889456238001585, Min Err: 2.7369956951588394e-05, 9.574869647622109e-05\n",
      "CNEP New best: 9.414340369403363e-05\n",
      "Epoch: 153000, Loss: -2.3062430087327956, -0.49284371577203273, Min Err: 2.7369956951588394e-05, 9.414340369403363e-05\n",
      "Epoch: 154000, Loss: -2.295954306125641, -0.4991436116397381, Min Err: 2.7369956951588394e-05, 9.414340369403363e-05\n",
      "Epoch: 155000, Loss: -2.277138741016388, -0.49398438458144667, Min Err: 2.7369956951588394e-05, 9.414340369403363e-05\n",
      "CNEP New best: 9.072304703295231e-05\n",
      "Epoch: 156000, Loss: -2.2884889734983442, -0.4628220169842243, Min Err: 2.7369956951588394e-05, 9.072304703295231e-05\n",
      "CNMP New best: 2.7361433021724226e-05\n",
      "Epoch: 157000, Loss: -2.3296446821689605, -0.49613598047196866, Min Err: 2.7361433021724226e-05, 9.072304703295231e-05\n",
      "CNMP New best: 2.700778655707836e-05\n",
      "Epoch: 158000, Loss: -2.3312876912355422, -0.49706381383538245, Min Err: 2.700778655707836e-05, 9.072304703295231e-05\n",
      "CNMP New best: 2.6615692768245935e-05\n",
      "CNEP New best: 8.641285821795464e-05\n",
      "Epoch: 159000, Loss: -2.3352922418117523, -0.4993208789229393, Min Err: 2.6615692768245935e-05, 8.641285821795464e-05\n",
      "Epoch: 160000, Loss: -2.3214852484464648, -0.5027206963598728, Min Err: 2.6615692768245935e-05, 8.641285821795464e-05\n",
      "Epoch: 161000, Loss: -2.328438713669777, -0.5049644854664802, Min Err: 2.6615692768245935e-05, 8.641285821795464e-05\n",
      "CNMP New best: 2.612197073176503e-05\n",
      "Epoch: 162000, Loss: -2.3397434779405595, -0.5091734642237424, Min Err: 2.612197073176503e-05, 8.641285821795464e-05\n",
      "CNEP New best: 8.565373718738556e-05\n",
      "Epoch: 163000, Loss: -2.349819659233093, -0.5053386493474246, Min Err: 2.612197073176503e-05, 8.565373718738556e-05\n",
      "Epoch: 164000, Loss: -2.340595181465149, -0.49377218762040137, Min Err: 2.612197073176503e-05, 8.565373718738556e-05\n",
      "Epoch: 165000, Loss: -2.3478764770030973, -0.41344037004187706, Min Err: 2.612197073176503e-05, 8.565373718738556e-05\n",
      "CNEP New best: 8.153009228408337e-05\n",
      "Epoch: 166000, Loss: -2.338676601409912, -0.40458226984739304, Min Err: 2.612197073176503e-05, 8.153009228408337e-05\n",
      "Epoch: 167000, Loss: -2.3576984540224077, -0.49392465262115004, Min Err: 2.612197073176503e-05, 8.153009228408337e-05\n",
      "Epoch: 168000, Loss: -2.3695441776514055, -0.5011865111887455, Min Err: 2.612197073176503e-05, 8.153009228408337e-05\n",
      "Epoch: 169000, Loss: -2.3598332619667053, -0.5087726229131222, Min Err: 2.612197073176503e-05, 8.153009228408337e-05\n",
      "CNMP New best: 2.557763596996665e-05\n",
      "Epoch: 170000, Loss: -2.3684424282312393, -0.514206619143486, Min Err: 2.557763596996665e-05, 8.153009228408337e-05\n",
      "Epoch: 171000, Loss: -2.374066892504692, -0.5121949882358312, Min Err: 2.557763596996665e-05, 8.153009228408337e-05\n",
      "CNEP New best: 8.136567659676075e-05\n",
      "Epoch: 172000, Loss: -2.356033476948738, -0.511024461299181, Min Err: 2.557763596996665e-05, 8.136567659676075e-05\n",
      "Epoch: 173000, Loss: -2.367945007920265, -0.5114400987476111, Min Err: 2.557763596996665e-05, 8.136567659676075e-05\n",
      "Epoch: 174000, Loss: -2.3870963814258577, -0.5183143617808819, Min Err: 2.557763596996665e-05, 8.136567659676075e-05\n",
      "Epoch: 175000, Loss: -2.3713435754179955, -0.5126667013764381, Min Err: 2.557763596996665e-05, 8.136567659676075e-05\n",
      "CNEP New best: 7.633482106029987e-05\n",
      "Epoch: 176000, Loss: -2.3580004425644874, -0.5188294251561165, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "Epoch: 177000, Loss: -2.382966151237488, -0.516687756344676, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "Epoch: 178000, Loss: -2.377506604075432, -0.4278811267837882, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "Epoch: 179000, Loss: -2.3812938613891603, -0.46824677316099406, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "Epoch: 180000, Loss: -2.3984669444561004, -0.515148609161377, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "Epoch: 181000, Loss: -2.3755776802301405, -0.5214041763246059, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "Epoch: 182000, Loss: -2.391334131240845, -0.5231288608312606, Min Err: 2.557763596996665e-05, 7.633482106029987e-05\n",
      "CNMP New best: 2.5064775254577398e-05\n",
      "Epoch: 183000, Loss: -2.384085802644491, -0.5257860842049122, Min Err: 2.5064775254577398e-05, 7.633482106029987e-05\n",
      "CNEP New best: 7.609560620039702e-05\n",
      "Epoch: 184000, Loss: -2.4238483984470367, -0.5262533509135247, Min Err: 2.5064775254577398e-05, 7.609560620039702e-05\n",
      "CNEP New best: 7.532903458923101e-05\n",
      "Epoch: 185000, Loss: -2.4171534337997436, -0.5276367440670728, Min Err: 2.5064775254577398e-05, 7.532903458923101e-05\n",
      "Epoch: 186000, Loss: -2.3787797172665597, -0.5094261293262243, Min Err: 2.5064775254577398e-05, 7.532903458923101e-05\n",
      "Epoch: 187000, Loss: -2.390971630692482, -0.524186205714941, Min Err: 2.5064775254577398e-05, 7.532903458923101e-05\n",
      "Epoch: 188000, Loss: -2.4285452597141264, -0.5333544989228248, Min Err: 2.5064775254577398e-05, 7.532903458923101e-05\n",
      "Epoch: 189000, Loss: -2.4223142856359483, -0.522921933978796, Min Err: 2.5064775254577398e-05, 7.532903458923101e-05\n",
      "CNEP New best: 7.26460013538599e-05\n",
      "Epoch: 190000, Loss: -2.426652485251427, -0.4210040601491928, Min Err: 2.5064775254577398e-05, 7.26460013538599e-05\n",
      "Epoch: 191000, Loss: -2.4198579882383346, -0.44093236967548727, Min Err: 2.5064775254577398e-05, 7.26460013538599e-05\n",
      "Epoch: 192000, Loss: -2.4000952134132385, -0.5223566159158946, Min Err: 2.5064775254577398e-05, 7.26460013538599e-05\n",
      "Epoch: 193000, Loss: -2.408614480853081, -0.5236519110947847, Min Err: 2.5064775254577398e-05, 7.26460013538599e-05\n",
      "CNEP New best: 7.089586462825536e-05\n",
      "Epoch: 194000, Loss: -2.38715325987339, -0.5258523564040661, Min Err: 2.5064775254577398e-05, 7.089586462825536e-05\n",
      "CNEP New best: 6.653453223407269e-05\n",
      "Epoch: 195000, Loss: -2.4334825880527498, -0.5385386971533298, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "Epoch: 196000, Loss: -2.446708923101425, -0.537305635124445, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "Epoch: 197000, Loss: -2.442198420166969, -0.5374783256947995, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "Epoch: 198000, Loss: -2.44071307015419, -0.46870738711953164, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "Epoch: 199000, Loss: -2.4533431581258776, -0.5211415983140468, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "Epoch: 200000, Loss: -2.434429471731186, -0.5385257627665997, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "Epoch: 201000, Loss: -2.43892325091362, -0.5358603161275387, Min Err: 2.5064775254577398e-05, 6.653453223407269e-05\n",
      "CNMP New best: 2.4670497514307498e-05\n",
      "CNEP New best: 6.580403540283441e-05\n",
      "Epoch: 202000, Loss: -2.444728979229927, -0.5361160407811403, Min Err: 2.4670497514307498e-05, 6.580403540283441e-05\n",
      "CNMP New best: 2.4561616592109203e-05\n",
      "Epoch: 203000, Loss: -2.4449403171539306, -0.5432554063498973, Min Err: 2.4561616592109203e-05, 6.580403540283441e-05\n",
      "CNEP New best: 6.445916835218668e-05\n",
      "Epoch: 204000, Loss: -2.4298088282346724, -0.5393190418332815, Min Err: 2.4561616592109203e-05, 6.445916835218668e-05\n",
      "Epoch: 205000, Loss: -2.4318539139032365, -0.5452239669263362, Min Err: 2.4561616592109203e-05, 6.445916835218668e-05\n",
      "Epoch: 206000, Loss: -2.4651137783527375, -0.5404896377623081, Min Err: 2.4561616592109203e-05, 6.445916835218668e-05\n",
      "CNMP New best: 2.4285956751555204e-05\n",
      "Epoch: 207000, Loss: -2.4298728051185607, -0.5494322821646929, Min Err: 2.4285956751555204e-05, 6.445916835218668e-05\n",
      "Epoch: 208000, Loss: -2.448154479146004, -0.544055034801364, Min Err: 2.4285956751555204e-05, 6.445916835218668e-05\n",
      "Epoch: 209000, Loss: -2.4721356589198114, -0.5380197502076626, Min Err: 2.4285956751555204e-05, 6.445916835218668e-05\n",
      "Epoch: 210000, Loss: -2.4722568345069886, -0.5453465945720672, Min Err: 2.4285956751555204e-05, 6.445916835218668e-05\n",
      "CNEP New best: 6.4051509834826e-05\n",
      "Epoch: 211000, Loss: -2.4584798954725264, -0.5503439440429211, Min Err: 2.4285956751555204e-05, 6.4051509834826e-05\n",
      "CNEP New best: 6.338452454656363e-05\n",
      "Epoch: 212000, Loss: -2.4245749613642693, -0.5442208496034145, Min Err: 2.4285956751555204e-05, 6.338452454656363e-05\n",
      "Epoch: 213000, Loss: -2.4652962790727617, -0.5505964196622372, Min Err: 2.4285956751555204e-05, 6.338452454656363e-05\n",
      "CNMP New best: 2.4068013299256564e-05\n",
      "Epoch: 214000, Loss: -2.478913734793663, -0.5551475349664688, Min Err: 2.4068013299256564e-05, 6.338452454656363e-05\n",
      "CNEP New best: 6.318296305835247e-05\n",
      "Epoch: 215000, Loss: -2.465535478115082, -0.5594683608710765, Min Err: 2.4068013299256564e-05, 6.318296305835247e-05\n",
      "Epoch: 216000, Loss: -2.5022232764959336, -0.5583737044930458, Min Err: 2.4068013299256564e-05, 6.318296305835247e-05\n",
      "CNEP New best: 6.230161059647799e-05\n",
      "Epoch: 217000, Loss: -2.4980404396057128, -0.5607616120576858, Min Err: 2.4068013299256564e-05, 6.230161059647799e-05\n",
      "Epoch: 218000, Loss: -2.495385381102562, -0.5495967295542359, Min Err: 2.4068013299256564e-05, 6.230161059647799e-05\n",
      "Epoch: 219000, Loss: -2.4882439442873, -0.5458730384260416, Min Err: 2.4068013299256564e-05, 6.230161059647799e-05\n",
      "Epoch: 220000, Loss: -2.494338942408562, -0.5568689329326153, Min Err: 2.4068013299256564e-05, 6.230161059647799e-05\n",
      "CNEP New best: 6.140954792499542e-05\n",
      "Epoch: 221000, Loss: -2.4942933250069617, -0.5526927651837468, Min Err: 2.4068013299256564e-05, 6.140954792499542e-05\n",
      "CNEP New best: 5.941100884228945e-05\n",
      "Epoch: 222000, Loss: -2.500766478061676, -0.5620156144499778, Min Err: 2.4068013299256564e-05, 5.941100884228945e-05\n",
      "Epoch: 223000, Loss: -2.502715427041054, -0.5596431068778038, Min Err: 2.4068013299256564e-05, 5.941100884228945e-05\n",
      "Epoch: 224000, Loss: -2.4938290374279024, -0.5561356427520514, Min Err: 2.4068013299256564e-05, 5.941100884228945e-05\n",
      "CNEP New best: 5.810853559523821e-05\n",
      "Epoch: 225000, Loss: -2.4815627344846725, -0.5618610654473305, Min Err: 2.4068013299256564e-05, 5.810853559523821e-05\n",
      "Epoch: 226000, Loss: -2.5047396092414855, -0.5620817003846168, Min Err: 2.4068013299256564e-05, 5.810853559523821e-05\n",
      "Epoch: 227000, Loss: -2.4946202372312545, -0.5589020237326622, Min Err: 2.4068013299256564e-05, 5.810853559523821e-05\n",
      "Epoch: 228000, Loss: -2.520977416396141, -0.5691293117403984, Min Err: 2.4068013299256564e-05, 5.810853559523821e-05\n",
      "CNMP New best: 2.395337913185358e-05\n",
      "Epoch: 229000, Loss: -2.5186575800180435, -0.5651178531199693, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 230000, Loss: -2.5287231512069703, -0.5655758927166462, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 231000, Loss: -2.513534039139748, -0.5644634152650834, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 232000, Loss: -2.531784063696861, -0.5063532618433237, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 233000, Loss: -2.48021852183342, -0.5629980592429638, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 234000, Loss: -2.536943931698799, -0.5678723918497562, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 235000, Loss: -2.5013331694602967, -0.5698439258635044, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 236000, Loss: -2.550212410926819, -0.5693235698938369, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 237000, Loss: -2.523142756462097, -0.5670413813591003, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 238000, Loss: -2.526914167046547, -0.569987986266613, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "Epoch: 239000, Loss: -2.548116876721382, -0.5788409135937691, Min Err: 2.395337913185358e-05, 5.810853559523821e-05\n",
      "CNEP New best: 5.753195844590664e-05\n",
      "Epoch: 240000, Loss: -2.537235621333122, -0.574105501383543, Min Err: 2.395337913185358e-05, 5.753195844590664e-05\n",
      "CNMP New best: 2.3710643872618676e-05\n",
      "CNEP New best: 5.427665077149868e-05\n",
      "Epoch: 241000, Loss: -2.5653273841142656, -0.5766849532723427, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 242000, Loss: -2.515101085305214, -0.5698790066838264, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 243000, Loss: -2.550735368847847, -0.5146818778812885, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 244000, Loss: -2.524730134487152, -0.5655412967354059, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 245000, Loss: -2.5616923878192903, -0.5760679575651884, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 246000, Loss: -2.5519064202308654, -0.5828043477535247, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 247000, Loss: -2.5440196380615236, -0.578017221480608, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "Epoch: 248000, Loss: -2.5638544343709944, -0.585055732101202, Min Err: 2.3710643872618676e-05, 5.427665077149868e-05\n",
      "CNEP New best: 5.363529548048973e-05\n",
      "Epoch: 249000, Loss: -2.566386180639267, -0.5782944094538689, Min Err: 2.3710643872618676e-05, 5.363529548048973e-05\n",
      "CNEP New best: 5.362889263778925e-05\n",
      "Epoch: 250000, Loss: -2.5760394632816315, -0.5695928110182286, Min Err: 2.3710643872618676e-05, 5.362889263778925e-05\n",
      "Epoch: 251000, Loss: -2.562495234966278, -0.5767269488573075, Min Err: 2.3710643872618676e-05, 5.362889263778925e-05\n",
      "Epoch: 252000, Loss: -2.582674267888069, -0.5785077897608281, Min Err: 2.3710643872618676e-05, 5.362889263778925e-05\n",
      "CNEP New best: 5.3038340993225575e-05\n",
      "Epoch: 253000, Loss: -2.562085383176804, -0.5774381858110428, Min Err: 2.3710643872618676e-05, 5.3038340993225575e-05\n",
      "Epoch: 254000, Loss: -2.5697928541898727, -0.5875498047471046, Min Err: 2.3710643872618676e-05, 5.3038340993225575e-05\n",
      "CNEP New best: 5.250364076346159e-05\n",
      "Epoch: 255000, Loss: -2.536438357830048, -0.5775944104045629, Min Err: 2.3710643872618676e-05, 5.250364076346159e-05\n",
      "CNEP New best: 5.225887056440115e-05\n",
      "Epoch: 256000, Loss: -2.548968452990055, -0.579464646577835, Min Err: 2.3710643872618676e-05, 5.225887056440115e-05\n",
      "CNEP New best: 5.184943787753582e-05\n",
      "Epoch: 257000, Loss: -2.566891744017601, -0.582645850867033, Min Err: 2.3710643872618676e-05, 5.184943787753582e-05\n",
      "Epoch: 258000, Loss: -2.594336006760597, -0.5887784304618835, Min Err: 2.3710643872618676e-05, 5.184943787753582e-05\n",
      "Epoch: 259000, Loss: -2.5658280564546585, -0.583205930441618, Min Err: 2.3710643872618676e-05, 5.184943787753582e-05\n",
      "Epoch: 260000, Loss: -2.5522022469043733, -0.5807685211002827, Min Err: 2.3710643872618676e-05, 5.184943787753582e-05\n",
      "CNEP New best: 5.050409119576216e-05\n",
      "Epoch: 261000, Loss: -2.588568492770195, -0.5383006025701761, Min Err: 2.3710643872618676e-05, 5.050409119576216e-05\n",
      "Epoch: 262000, Loss: -2.5682212029695513, -0.5816288991272449, Min Err: 2.3710643872618676e-05, 5.050409119576216e-05\n",
      "CNEP New best: 5.0045349635183814e-05\n",
      "Epoch: 263000, Loss: -2.5690312097072603, -0.5886449098587037, Min Err: 2.3710643872618676e-05, 5.0045349635183814e-05\n",
      "Epoch: 264000, Loss: -2.571632663965225, -0.5826477760076523, Min Err: 2.3710643872618676e-05, 5.0045349635183814e-05\n",
      "Epoch: 265000, Loss: -2.5639122141599655, -0.5875316517651081, Min Err: 2.3710643872618676e-05, 5.0045349635183814e-05\n",
      "Epoch: 266000, Loss: -2.565784374833107, -0.5866156630367041, Min Err: 2.3710643872618676e-05, 5.0045349635183814e-05\n",
      "CNEP New best: 4.820817150175571e-05\n",
      "Epoch: 267000, Loss: -2.59305004799366, -0.5931842871904374, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 268000, Loss: -2.5864375569820406, -0.5838130608350038, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 269000, Loss: -2.5474917540550233, -0.584113332554698, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 270000, Loss: -2.563334246873856, -0.5890772607028484, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 271000, Loss: -2.560577515721321, -0.5863834701478481, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 272000, Loss: -2.5865575387477873, -0.5913214491903782, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 273000, Loss: -2.5821466470956804, -0.5916245131194592, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 274000, Loss: -2.5923923909664155, -0.5941594362556935, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "Epoch: 275000, Loss: -2.5916601510047914, -0.577577748209238, Min Err: 2.3710643872618676e-05, 4.820817150175571e-05\n",
      "CNEP New best: 4.706152249127627e-05\n",
      "Epoch: 276000, Loss: -2.581641482412815, -0.5942661991268396, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 277000, Loss: -2.614062879920006, -0.5985642534196377, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 278000, Loss: -2.582123647809029, -0.5973730615973473, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 279000, Loss: -2.599286368250847, -0.5946548075973987, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 280000, Loss: -2.6103540967702865, -0.5949965194165706, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 281000, Loss: -2.6116092557907105, -0.5878863427042961, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 282000, Loss: -2.5687479199171066, -0.5914372847676277, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 283000, Loss: -2.610652576327324, -0.6013184283077717, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 284000, Loss: -2.575955186545849, -0.5925235576033592, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 285000, Loss: -2.5927443219423294, -0.5995522426068782, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "Epoch: 286000, Loss: -2.602022044181824, -0.588648296892643, Min Err: 2.3710643872618676e-05, 4.706152249127627e-05\n",
      "CNEP New best: 4.697846714407206e-05\n",
      "Epoch: 287000, Loss: -2.6017519516944887, -0.5989435409009457, Min Err: 2.3710643872618676e-05, 4.697846714407206e-05\n",
      "Epoch: 288000, Loss: -2.609172736287117, -0.5976826220750808, Min Err: 2.3710643872618676e-05, 4.697846714407206e-05\n",
      "Epoch: 289000, Loss: -2.6043760497570037, -0.5956913235485554, Min Err: 2.3710643872618676e-05, 4.697846714407206e-05\n",
      "Epoch: 290000, Loss: -2.5908391485214235, -0.5943918936550617, Min Err: 2.3710643872618676e-05, 4.697846714407206e-05\n",
      "CNEP New best: 4.65840008109808e-05\n",
      "Epoch: 291000, Loss: -2.63208714056015, -0.6050129384696483, Min Err: 2.3710643872618676e-05, 4.65840008109808e-05\n",
      "CNMP New best: 2.338301856070757e-05\n",
      "CNEP New best: 4.5751738362014294e-05\n",
      "Epoch: 292000, Loss: -2.605725330114365, -0.5990879384279251, Min Err: 2.338301856070757e-05, 4.5751738362014294e-05\n",
      "Epoch: 293000, Loss: -2.607398375928402, -0.6069266752749681, Min Err: 2.338301856070757e-05, 4.5751738362014294e-05\n",
      "CNEP New best: 4.452447406947613e-05\n",
      "Epoch: 294000, Loss: -2.608951719403267, -0.604311159312725, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 295000, Loss: -2.614070855975151, -0.6078899010717869, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 296000, Loss: -2.630565660357475, -0.6025583167523145, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 297000, Loss: -2.6327357984781266, -0.6025618904381991, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 298000, Loss: -2.6149913277626036, -0.5966494497954845, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 299000, Loss: -2.6287550280094147, -0.5956945178508759, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 300000, Loss: -2.6325506604909896, -0.605447965234518, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 301000, Loss: -2.6383105643987657, -0.6019939366281033, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 302000, Loss: -2.6060782603621484, -0.6057013435065747, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "Epoch: 303000, Loss: -2.6441693094968794, -0.6080806165635586, Min Err: 2.338301856070757e-05, 4.452447406947613e-05\n",
      "CNEP New best: 4.3943985365331175e-05\n",
      "Epoch: 304000, Loss: -2.6340524547100066, -0.6072448717355728, Min Err: 2.338301856070757e-05, 4.3943985365331175e-05\n",
      "Epoch: 305000, Loss: -2.639433235287666, -0.609249471873045, Min Err: 2.338301856070757e-05, 4.3943985365331175e-05\n",
      "Epoch: 306000, Loss: -2.5575627189278602, -0.6026419675648212, Min Err: 2.338301856070757e-05, 4.3943985365331175e-05\n",
      "Epoch: 307000, Loss: -2.655153080224991, -0.6109791796505452, Min Err: 2.338301856070757e-05, 4.3943985365331175e-05\n",
      "Epoch: 308000, Loss: -2.651849918961525, -0.6072573099136352, Min Err: 2.338301856070757e-05, 4.3943985365331175e-05\n",
      "CNEP New best: 4.3015824630856513e-05\n",
      "Epoch: 309000, Loss: -2.6436824525594713, -0.6042834223657847, Min Err: 2.338301856070757e-05, 4.3015824630856513e-05\n",
      "Epoch: 310000, Loss: -2.665715397357941, -0.6084727038294077, Min Err: 2.338301856070757e-05, 4.3015824630856513e-05\n",
      "Epoch: 311000, Loss: -2.6476107984781265, -0.61170942106843, Min Err: 2.338301856070757e-05, 4.3015824630856513e-05\n",
      "Epoch: 312000, Loss: -2.613674018025398, -0.6043064112961293, Min Err: 2.338301856070757e-05, 4.3015824630856513e-05\n",
      "Epoch: 313000, Loss: -2.6427521572113037, -0.5870139817595482, Min Err: 2.338301856070757e-05, 4.3015824630856513e-05\n",
      "CNEP New best: 4.281370434910059e-05\n",
      "Epoch: 314000, Loss: -2.6298107377290725, -0.607597362369299, Min Err: 2.338301856070757e-05, 4.281370434910059e-05\n",
      "Epoch: 315000, Loss: -2.65444220995903, -0.6104080108255148, Min Err: 2.338301856070757e-05, 4.281370434910059e-05\n",
      "Epoch: 316000, Loss: -2.599433490395546, -0.6054213520437479, Min Err: 2.338301856070757e-05, 4.281370434910059e-05\n",
      "CNEP New best: 4.243490751832724e-05\n",
      "Epoch: 317000, Loss: -2.6462275295257567, -0.6176951172947883, Min Err: 2.338301856070757e-05, 4.243490751832724e-05\n",
      "Epoch: 318000, Loss: -2.648354392528534, -0.6080058846771718, Min Err: 2.338301856070757e-05, 4.243490751832724e-05\n",
      "Epoch: 319000, Loss: -2.6285979920625686, -0.6118770472109317, Min Err: 2.338301856070757e-05, 4.243490751832724e-05\n",
      "Epoch: 320000, Loss: -2.6434450283050537, -0.611682993620634, Min Err: 2.338301856070757e-05, 4.243490751832724e-05\n",
      "Epoch: 321000, Loss: -2.6588082509040833, -0.6135970954895019, Min Err: 2.338301856070757e-05, 4.243490751832724e-05\n",
      "Epoch: 322000, Loss: -2.65107654106617, -0.61329564884305, Min Err: 2.338301856070757e-05, 4.243490751832724e-05\n",
      "CNEP New best: 4.2042075656354427e-05\n",
      "Epoch: 323000, Loss: -2.6835696185827254, -0.6143515300154686, Min Err: 2.338301856070757e-05, 4.2042075656354427e-05\n",
      "CNMP New best: 2.2897650487720967e-05\n",
      "Epoch: 324000, Loss: -2.6383325220942497, -0.6120531215369701, Min Err: 2.2897650487720967e-05, 4.2042075656354427e-05\n",
      "Epoch: 325000, Loss: -2.650866045475006, -0.6110450825989246, Min Err: 2.2897650487720967e-05, 4.2042075656354427e-05\n",
      "CNEP New best: 4.066313151270151e-05\n",
      "Epoch: 326000, Loss: -2.649525580525398, -0.604278903529048, Min Err: 2.2897650487720967e-05, 4.066313151270151e-05\n",
      "Epoch: 327000, Loss: -2.6554086982011795, -0.6142944414913655, Min Err: 2.2897650487720967e-05, 4.066313151270151e-05\n",
      "CNEP New best: 4.006544128060341e-05\n",
      "Epoch: 328000, Loss: -2.572743484079838, -0.6160427528917789, Min Err: 2.2897650487720967e-05, 4.006544128060341e-05\n",
      "Epoch: 329000, Loss: -2.6763187101483346, -0.6185050606131554, Min Err: 2.2897650487720967e-05, 4.006544128060341e-05\n",
      "Epoch: 330000, Loss: -2.6617999204397202, -0.6240378821194172, Min Err: 2.2897650487720967e-05, 4.006544128060341e-05\n",
      "Epoch: 331000, Loss: -2.6823102107048036, -0.6206339990794658, Min Err: 2.2897650487720967e-05, 4.006544128060341e-05\n",
      "Epoch: 332000, Loss: -2.669976569533348, -0.6140505380183459, Min Err: 2.2897650487720967e-05, 4.006544128060341e-05\n",
      "Epoch: 333000, Loss: -2.6703426896333693, -0.6238619379997253, Min Err: 2.2897650487720967e-05, 4.006544128060341e-05\n",
      "CNMP New best: 2.259105909615755e-05\n",
      "Epoch: 334000, Loss: -2.6693681914806366, -0.5546953679770231, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "Epoch: 335000, Loss: -2.576741280555725, -0.5128058863729239, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "Epoch: 336000, Loss: -2.684791353583336, -0.5731437110900879, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "Epoch: 337000, Loss: -2.6883906730413436, -0.5826145641207695, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "Epoch: 338000, Loss: -2.673315764784813, -0.5830918319225311, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "Epoch: 339000, Loss: -2.6922937812805174, -0.5865355841815472, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "Epoch: 340000, Loss: -2.67545772087574, -0.5937556882798671, Min Err: 2.259105909615755e-05, 4.006544128060341e-05\n",
      "CNEP New best: 3.611257998272777e-05\n",
      "Epoch: 341000, Loss: -2.6817297602891923, -0.6139013092219829, Min Err: 2.259105909615755e-05, 3.611257998272777e-05\n",
      "Epoch: 342000, Loss: -2.6526840451955795, -0.6152294642329216, Min Err: 2.259105909615755e-05, 3.611257998272777e-05\n",
      "Epoch: 343000, Loss: -2.6646782636642454, -0.6130583632439375, Min Err: 2.259105909615755e-05, 3.611257998272777e-05\n",
      "Epoch: 344000, Loss: -2.6592450337409974, -0.6104255214333534, Min Err: 2.259105909615755e-05, 3.611257998272777e-05\n",
      "CNMP New best: 2.1962558384984732e-05\n",
      "Epoch: 345000, Loss: -2.689910801410675, -0.6185618408322334, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 346000, Loss: -2.694449933886528, -0.6204233817607164, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 347000, Loss: -2.6878720318078995, -0.6223848341703415, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 348000, Loss: -2.6725743358135223, -0.6197286315262318, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 349000, Loss: -2.7088829548358917, -0.6245469736158847, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 350000, Loss: -2.7210765055418014, -0.6282265834510327, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 351000, Loss: -2.686540842413902, -0.6230423897206784, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 352000, Loss: -2.7079346692562103, -0.6139589516520501, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 353000, Loss: -2.6716268212199212, -0.62489040876925, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 354000, Loss: -2.6758928500413894, -0.6204447776377201, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 355000, Loss: -2.681732606828213, -0.6277770358324051, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 356000, Loss: -2.693746421337128, -0.6146852097958326, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 357000, Loss: -2.6886700747013093, -0.6196954498589039, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 358000, Loss: -2.709124832749367, -0.6229298907518387, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 359000, Loss: -2.6965873423814775, -0.629873311161995, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 360000, Loss: -2.6298850239515303, -0.5183215701878071, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 361000, Loss: -2.6916078619360926, -0.6197830868661404, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 362000, Loss: -2.7092478737831116, -0.634711255133152, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 363000, Loss: -2.6895283696651457, -0.6220483216643333, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 364000, Loss: -2.708905164539814, -0.6256172913610936, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 365000, Loss: -2.7165875750780106, -0.6279144642949104, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 366000, Loss: -2.724758518099785, -0.629377309769392, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 367000, Loss: -2.7194446268081665, -0.6300563679635525, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 368000, Loss: -2.7124519302845003, -0.6308878347873688, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 369000, Loss: -2.7056406975984575, -0.6273227780163289, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "Epoch: 370000, Loss: -2.7150493314266204, -0.6282441069483757, Min Err: 2.1962558384984732e-05, 3.611257998272777e-05\n",
      "CNEP New best: 3.5420458298176525e-05\n",
      "Epoch: 371000, Loss: -2.723530988097191, -0.6312137155532836, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 372000, Loss: -2.724346032500267, -0.629879274815321, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 373000, Loss: -2.7254376401901244, -0.6222053644061089, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 374000, Loss: -2.7259349348545077, -0.6289936891496182, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 375000, Loss: -2.7402280814647675, -0.6339420780241489, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 376000, Loss: -2.735033279776573, -0.634700375765562, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 377000, Loss: -2.730777768611908, -0.6311965674459934, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "Epoch: 378000, Loss: -2.7168536052703858, -0.6232007175385952, Min Err: 2.1962558384984732e-05, 3.5420458298176525e-05\n",
      "CNEP New best: 3.516201162710786e-05\n",
      "Epoch: 379000, Loss: -2.7101049377918245, -0.623948974519968, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 380000, Loss: -2.7233689453601837, -0.6330302421450615, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 381000, Loss: -2.7467464978694918, -0.5205041014254094, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 382000, Loss: -2.720873218715191, -0.5864883667826652, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 383000, Loss: -2.738661102294922, -0.6242751465439796, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 384000, Loss: -2.7162819920778274, -0.6337904736995698, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 385000, Loss: -2.7084944268465043, -0.6314165278375149, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "Epoch: 386000, Loss: -2.7235942035913467, -0.6289400271773339, Min Err: 2.1962558384984732e-05, 3.516201162710786e-05\n",
      "CNEP New best: 3.509945934638381e-05\n",
      "Epoch: 387000, Loss: -2.7181629600524904, -0.6265918241143227, Min Err: 2.1962558384984732e-05, 3.509945934638381e-05\n",
      "Epoch: 388000, Loss: -2.7220122250318526, -0.6347273780107499, Min Err: 2.1962558384984732e-05, 3.509945934638381e-05\n",
      "CNMP New best: 2.180720679461956e-05\n",
      "Epoch: 389000, Loss: -2.7404209221005438, -0.6288804173916578, Min Err: 2.180720679461956e-05, 3.509945934638381e-05\n",
      "CNEP New best: 3.489788854494691e-05\n",
      "Epoch: 390000, Loss: -2.719082403898239, -0.6326662261486053, Min Err: 2.180720679461956e-05, 3.489788854494691e-05\n",
      "Epoch: 391000, Loss: -2.694064141392708, -0.6274328898787499, Min Err: 2.180720679461956e-05, 3.489788854494691e-05\n",
      "Epoch: 392000, Loss: -2.711105036973953, -0.6324376119673252, Min Err: 2.180720679461956e-05, 3.489788854494691e-05\n",
      "CNEP New best: 3.4781042486429215e-05\n",
      "Epoch: 393000, Loss: -2.7517444767951966, -0.6373852151334286, Min Err: 2.180720679461956e-05, 3.4781042486429215e-05\n",
      "Epoch: 394000, Loss: -2.7244666156768798, -0.5912172493934631, Min Err: 2.180720679461956e-05, 3.4781042486429215e-05\n",
      "CNEP New best: 3.434300422668457e-05\n",
      "Epoch: 395000, Loss: -2.739890167474747, -0.613887187346816, Min Err: 2.180720679461956e-05, 3.434300422668457e-05\n",
      "Epoch: 396000, Loss: -2.7430490003824235, -0.6433935048878193, Min Err: 2.180720679461956e-05, 3.434300422668457e-05\n",
      "Epoch: 397000, Loss: -2.7406147569417953, -0.6363954062759877, Min Err: 2.180720679461956e-05, 3.434300422668457e-05\n",
      "Epoch: 398000, Loss: -2.7245728516578676, -0.6328637996912002, Min Err: 2.180720679461956e-05, 3.434300422668457e-05\n",
      "Epoch: 399000, Loss: -2.7568829857110977, -0.6351762138605118, Min Err: 2.180720679461956e-05, 3.434300422668457e-05\n",
      "Epoch: 400000, Loss: -2.750450614452362, -0.6328462616801261, Min Err: 2.180720679461956e-05, 3.434300422668457e-05\n",
      "CNEP New best: 3.377954242751002e-05\n",
      "Epoch: 401000, Loss: -2.7387006933689118, -0.6193112138211727, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 402000, Loss: -2.7567083426713945, -0.6317002387344837, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 403000, Loss: -2.724458898425102, -0.6329647096991539, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 404000, Loss: -2.755564831733704, -0.6379769272506237, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 405000, Loss: -2.7509093396663666, -0.638265496134758, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 406000, Loss: -2.7271267011761666, -0.6409507700502872, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 407000, Loss: -2.7422963297367096, -0.6405292206406593, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 408000, Loss: -2.7726556307077406, -0.6371069621741772, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 409000, Loss: -2.7565932575464247, -0.6408089632987977, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 410000, Loss: -2.751018451333046, -0.639756826609373, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 411000, Loss: -2.728382196545601, -0.6349203057438135, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 412000, Loss: -2.7433550379276275, -0.6406563169062137, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 413000, Loss: -2.7640983946323394, -0.5060792517848313, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "Epoch: 414000, Loss: -2.7474628100395204, -0.6092340666353703, Min Err: 2.180720679461956e-05, 3.377954242751002e-05\n",
      "CNEP New best: 3.364116419106722e-05\n",
      "Epoch: 415000, Loss: -2.7756187443733213, -0.6339142090976239, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 416000, Loss: -2.7794732468128203, -0.636376234203577, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 417000, Loss: -2.7614971187114716, -0.640832281768322, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 418000, Loss: -2.751861184716225, -0.6418156783878803, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 419000, Loss: -2.7872533000707627, -0.6380391556918621, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 420000, Loss: -2.759175808429718, -0.6395907045900822, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 421000, Loss: -2.7696477422714234, -0.6423549696803093, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 422000, Loss: -2.750124755322933, -0.6384961192309856, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "Epoch: 423000, Loss: -2.722788477897644, -0.6318089619129896, Min Err: 2.180720679461956e-05, 3.364116419106722e-05\n",
      "CNEP New best: 3.315598936751485e-05\n",
      "Epoch: 424000, Loss: -2.773289314508438, -0.6461006507277489, Min Err: 2.180720679461956e-05, 3.315598936751485e-05\n",
      "Epoch: 425000, Loss: -2.7731737775802614, -0.6480745435357094, Min Err: 2.180720679461956e-05, 3.315598936751485e-05\n",
      "Epoch: 426000, Loss: -2.778479817032814, -0.6457610392868519, Min Err: 2.180720679461956e-05, 3.315598936751485e-05\n",
      "Epoch: 427000, Loss: -2.766422763586044, -0.6342019602954387, Min Err: 2.180720679461956e-05, 3.315598936751485e-05\n",
      "CNEP New best: 3.3003147691488267e-05\n",
      "Epoch: 428000, Loss: -2.7817065342664717, -0.6433455891907215, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 429000, Loss: -2.7565788128376005, -0.6459879201352596, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 430000, Loss: -2.7870502209663393, -0.6484112013280392, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 431000, Loss: -2.7532812288999557, -0.6472044369578361, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 432000, Loss: -2.7914734219312667, -0.6486194957494735, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 433000, Loss: -2.7706998499631883, -0.6451495288014412, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 434000, Loss: -2.757807715058327, -0.6494833794534206, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 435000, Loss: -2.755264511346817, -0.6045582863390446, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 436000, Loss: -2.7693987315893174, -0.6498449172079563, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 437000, Loss: -2.786483486175537, -0.6479158771336079, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 438000, Loss: -2.7590517168045046, -0.6486087114214897, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 439000, Loss: -2.7699332464933395, -0.6476749772131443, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 440000, Loss: -2.7819954662322997, -0.648150565803051, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 441000, Loss: -2.7885574148893357, -0.6463999387323857, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 442000, Loss: -2.788285268902779, -0.6503134315311909, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 443000, Loss: -2.7844857566356658, -0.6499281679391861, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 444000, Loss: -2.7825305718183517, -0.6488967038691044, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 445000, Loss: -2.777176952958107, -0.6528768582046032, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 446000, Loss: -2.790073826313019, -0.6508346417695284, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 447000, Loss: -2.7786233459711074, -0.6526006456613541, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n",
      "Epoch: 448000, Loss: -2.7987915259599685, -0.6560605844855308, Min Err: 2.180720679461956e-05, 3.3003147691488267e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m epoch_loss_cnmp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     55\u001b[0m optimizer_cnep\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m pred, gate \u001b[38;5;241m=\u001b[39m \u001b[43mcnep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss, nll \u001b[38;5;241m=\u001b[39m cnep\u001b[38;5;241m.\u001b[39mloss(pred, gate, tar_y, tar_mask)\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/cnep/baxter/../models/cnep.py:65\u001b[0m, in \u001b[0;36mCNEP.forward\u001b[0;34m(self, obs, tar, obs_mask, latent)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     59\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(encoder_hidden_dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], num_decoders),\n\u001b[1;32m     60\u001b[0m         nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, tar, obs_mask, latent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# obs: (batch_size, n_max, input_dim+output_dim)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# tar: (batch_size, m_max, input_dim)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# obs_mask: (batch_size, n_max)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# encoding\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     encoded_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(obs)  \u001b[38;5;66;03m# (batch_size,  encoder_hidden_dims[-1])\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     obs_mask_exp \u001b[38;5;241m=\u001b[39m obs_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(encoded_obs)  \u001b[38;5;66;03m# (batch_size, n_max, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3905\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   3903\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   3904\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 3905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1482\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2527\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2525\u001b[0m             args_[idx] \u001b[38;5;241m=\u001b[39m args_[idx]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 2527\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2533\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m call_func_with_args(\n\u001b[1;32m   2534\u001b[0m         compiled_fn,\n\u001b[1;32m   2535\u001b[0m         args,\n\u001b[1;32m   2536\u001b[0m         disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   2537\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1482\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3010\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, seed, offset)\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;66;03m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m-> 3010\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3016\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   3017\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:374\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:628\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs):\n\u001b[1;32m    627\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:401\u001b[0m, in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[1;32m    393\u001b[0m     compiled_graph\u001b[38;5;241m.\u001b[39mcompiled_artifact \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mload_by_key_path(\n\u001b[1;32m    394\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mcache_key,\n\u001b[1;32m    395\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39martifact_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m (),\n\u001b[1;32m    399\u001b[0m     )\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_yigit/wl/cwly4sx6jub23j7fijkurubbzkxthmqs3xq46cwge77vlzdaelug.py:473\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    471\u001b[0m buf27 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m256\u001b[39m), (\u001b[38;5;241m5120\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Source Nodes: [sub3_3_1, sub3_3_2], Original ATen: [aten.relu, aten.threshold_backward, aten.view]\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m triton_poi_fused_relu_threshold_backward_view_5\u001b[38;5;241m.\u001b[39mrun(primals_20, buf19, buf20, buf27, \u001b[38;5;241m20480\u001b[39m, grid\u001b[38;5;241m=\u001b[39m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20480\u001b[39;49m\u001b[43m)\u001b[49m, stream\u001b[38;5;241m=\u001b[39mstream0)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m buf19\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m primals_20\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/baxter/cnmp_cnep/mobile_net/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "\n",
    "epochs = 5_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = v_num_demos//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_cnmp, avg_loss_cnep = 0, 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_vl_cnmp, min_vl_cnep = 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "tl_cnmp, tl_cnep = [], []\n",
    "ve_cnmp, ve_cnep = [], []\n",
    "\n",
    "cnmp_tl_path, cnep_tl_path = f'{root_folder}cnmp_training_loss.pt', f'{root_folder}cnep_training_loss.pt'\n",
    "cnmp_ve_path, cnep_ve_path = f'{root_folder}cnmp_validation_error.pt', f'{root_folder}cnep_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # perm_ids = torch.randperm(num_demos + v_num_demos)\n",
    "    # train_ids, val_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "    perm_ids = torch.randperm(num_demos)\n",
    "    train_ids, val_ids = perm_ids[:num_demos], perm_ids[:num_demos]\n",
    "\n",
    "    train_trajs, val_trajs = trajs[train_ids], trajs[val_ids]\n",
    "    train_feats, val_feats = feats[train_ids], feats[val_ids]\n",
    "\n",
    "    epoch_loss_cnmp, epoch_loss_cnep = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(traj_ids[i])\n",
    "\n",
    "        optimizer_cnmp.zero_grad()\n",
    "        pred = cnmp(obs, tar_x, obs_mask)\n",
    "        loss = cnmp.loss(pred, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer_cnmp.step()\n",
    "\n",
    "        epoch_loss_cnmp += loss.item()\n",
    "\n",
    "        optimizer_cnep.zero_grad()\n",
    "        pred, gate = cnep(obs, tar_x, obs_mask)\n",
    "        loss, nll = cnep.loss(pred, gate, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer_cnep.step()\n",
    "\n",
    "        epoch_loss_cnep += nll.item()\n",
    "\n",
    "    epoch_loss_cnmp = epoch_loss_cnmp/epoch_iter\n",
    "    tl_cnmp.append(epoch_loss_cnmp)\n",
    "    epoch_loss_cnep = epoch_loss_cnep/epoch_iter\n",
    "    tl_cnep.append(epoch_loss_cnep)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(v_num_demos)[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_err_cnmp, val_err_cnep = 0, 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                prepare_masked_val_batch(v_traj_ids[j])\n",
    "\n",
    "                p = cnmp.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                vp_means = p[:, :, :dy]\n",
    "                val_err_cnmp += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "                p, g = cnep.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                dec_id = torch.argmax(g.squeeze(1), dim=-1)\n",
    "                vp_means = p[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_err_cnep += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "            val_err_cnmp = val_err_cnmp/(v_epoch_iter*t_steps)\n",
    "            val_err_cnep = val_err_cnep/(v_epoch_iter*t_steps)\n",
    "\n",
    "            if val_err_cnmp < min_vl_cnmp:\n",
    "                min_vl_cnmp = val_err_cnmp\n",
    "                print(f'CNMP New best: {min_vl_cnmp}')\n",
    "                torch.save(cnmp_.state_dict(), f'{root_folder}saved_models/cnmp.pt')\n",
    "\n",
    "            if val_err_cnep < min_vl_cnep:\n",
    "                min_vl_cnep = val_err_cnep\n",
    "                print(f'CNEP New best: {min_vl_cnep}')\n",
    "                torch.save(cnep_.state_dict(), f'{root_folder}saved_models/cnep.pt')\n",
    "\n",
    "            ve_cnmp.append(val_err_cnmp)\n",
    "            ve_cnep.append(val_err_cnep)\n",
    "\n",
    "    avg_loss_cnmp += epoch_loss_cnmp\n",
    "    avg_loss_cnep += epoch_loss_cnep\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, {}, Min Err: {}, {}\".format(epoch, avg_loss_cnmp/val_per_epoch, avg_loss_cnep/val_per_epoch, min_vl_cnmp, min_vl_cnep))\n",
    "        avg_loss_cnmp, avg_loss_cnep = 0, 0\n",
    "\n",
    "    if epoch % 500_000 == 0 and epoch > 1:\n",
    "        torch.save(cnmp_.state_dict(), f'{root_folder}saved_models/last_cnmp.pt')\n",
    "        torch.save(cnep_.state_dict(), f'{root_folder}saved_models/last_cnep.pt')\n",
    "\n",
    "torch.save(torch.Tensor(tl_cnmp), cnmp_tl_path)\n",
    "torch.save(torch.Tensor(ve_cnmp), cnmp_ve_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnmp_.state_dict(), f'{root_folder}saved_models/last_cnmp.pt')\n",
    "torch.save(cnep_.state_dict(), f'{root_folder}saved_models/last_cnep.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
