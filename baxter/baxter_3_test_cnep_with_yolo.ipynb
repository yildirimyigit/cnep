{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "is_save = False\n",
    "extract = True\n",
    "comp_filter_size = 160\n",
    "\n",
    "\n",
    "def crop_left(im): \n",
    "    return transforms.functional.crop(im, top=0, left=0, height=320, width=500)\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model, device='cuda:0'):\n",
    "        self.model = model\n",
    "        self.features = []\n",
    "        self.device = device\n",
    "\n",
    "        # Define a downsampling layer\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(512, comp_filter_size, kernel_size=1, stride=1),  # Reducing channels from 576 to 256\n",
    "            # nn.ReLU(),\n",
    "            # nn.AdaptiveAvgPool2d((16, 16))  # Reducing spatial dimensions to 10x10\n",
    "        ).to(self.device)\n",
    "\n",
    "    def hook(self, module, input, output):\n",
    "        print(f\"Hook called for layer: {module}\")\n",
    "        # Apply downsampling\n",
    "        output = output.to(self.device)\n",
    "        downsampled_output = self.downsample(output)\n",
    "        self.features.append(downsampled_output)\n",
    "\n",
    "    def register_hooks(self, layer_names):\n",
    "        self.hooks = []\n",
    "        for name, module in self.model.model.named_modules():  # Adjusted for YOLOv8 specific submodule access\n",
    "            if name in layer_names:\n",
    "                print(f\"Registering hook for layer: {name}\")\n",
    "                hook = module.register_forward_hook(self.hook)\n",
    "                self.hooks.append(hook)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def extract_features(self, img_path):\n",
    "        self.features = []\n",
    "        img = Image.open(img_path).convert('RGB')  # Load image using PIL\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Lambda(crop_left),  # Crop the left side\n",
    "            transforms.Pad(padding=(70, 160, 70, 160)),  # Pad to 640x640\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        img = transform(img).unsqueeze(0).to(self.device)\n",
    "        self.model(img)\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "t_steps = 400\n",
    "dims = 0#128 * 20 * 20\n",
    "# feats = torch.zeros(1, dims)\n",
    "trajs = torch.zeros(1, t_steps, 3)\n",
    "\n",
    "# model = YOLO('yolov8l.pt').to(device)\n",
    "# feature_extractor = FeatureExtractor(model, device=device)\n",
    "\n",
    "# layer_names = ['model.21']\n",
    "# feature_extractor.register_hooks(layer_names)\n",
    "\n",
    "# img_path = f'data/0/0/img.jpeg'\n",
    "# features = feature_extractor.extract_features(img_path)\n",
    "# feats[0] = features[0].view(-1)\n",
    "# feats[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnep import CNEP\n",
    "from cnmp import CNMP\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_demos, v_num_demos = 1, 1\n",
    "num_classes = 1  # Number of modes\n",
    "num_indiv = num_demos // num_classes  # Number of trajectories per mode\n",
    "num_val_indiv = v_num_demos // num_classes  # Number of trajectories per mode\n",
    "\n",
    "dx = 1\n",
    "dg = dims\n",
    "dy = 3\n",
    "batch_size = 1\n",
    "n_max, m_max = 10, 10\n",
    "t_steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(traj_ids: list):\n",
    "    obs.fill_(0)\n",
    "    tar_x.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = trajs[traj_id]\n",
    "        # feat = feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "        m = torch.randint(1, m_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "        \n",
    "        obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)  # X\n",
    "        # obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)  # G\n",
    "        obs[i, :n, dx+dg:] = traj[n_ids]  # Y\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x[i, :m, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        # tar_x[i, :m, dx:] = feat.repeat(m, 1)\n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "val_obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_val_batch(traj_ids: list):\n",
    "    val_obs.fill_(0)\n",
    "    val_tar_x.fill_(0)\n",
    "    val_tar_y.fill_(0)\n",
    "    val_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = trajs[traj_id]\n",
    "        # feat = feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "        \n",
    "        val_obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)\n",
    "        # val_obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)\n",
    "        val_obs[i, :n, dx+dg:] = traj[n_ids]\n",
    "        val_obs_mask[i, :n] = True\n",
    "        \n",
    "        val_tar_x[i, :, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        # val_tar_x[i, :, dx:] = feat.repeat(t_steps, 1)\n",
    "        val_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/yigit/projects/cnep/outputs/'\n",
    "test_type = 'baxter/'\n",
    "test_path = 'cnmp_cnep_no_feats/1718611343/'\n",
    "run_path = root_path + test_type + test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'saved_models/'\n",
    "models_path = f'{run_path}{model_folder}'\n",
    "\n",
    "cnep_path, cnmp_path = f'{models_path}cnep.pt', f'{models_path}cnmp.pt'\n",
    "\n",
    "batch_size = 1\n",
    "device = 'cpu'\n",
    "\n",
    "cnep = CNEP(dx+dg, dy, n_max, n_max, [512, 128], num_decoders=2, decoder_hidden_dims=[128, 128], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "cnmp = CNMP(dx+dg, dy, n_max, m_max, [512, 128], decoder_hidden_dims=[256, 256], batch_size=batch_size, device=device)\n",
    "\n",
    "cnep.load_state_dict(torch.load(cnep_path, map_location='cpu'))\n",
    "cnmp.load_state_dict(torch.load(cnmp_path, map_location='cpu'))\n",
    "\n",
    "# data for testing cnxp\n",
    "val_obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "# val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps = 400\n",
    "val_obs.fill_(0)\n",
    "val_tar_x.fill_(0)\n",
    "val_obs_mask.fill_(False)\n",
    "###############\n",
    "m = 2\n",
    "val_obs_mask[0, :m] = True\n",
    "val_tar_x[0, :, 0] = torch.linspace(0, 1, t_steps)\n",
    "# val_tar_x[0, :, 1:dx+dg] = feats[0].repeat(t_steps, 1)\n",
    "###############\n",
    "val_obs[0, 0, 0] = 0\n",
    "# val_obs[0, 0, 1:dx+dg] = feats[0]\n",
    "val_obs[0, 0, dx+dg:] = torch.tensor([0.647,-0.8414, 0.046])\n",
    "val_obs[0, 1, 0] = 1\n",
    "# val_obs[0, 1, 1:dx+dg] = feats[0]\n",
    "val_obs[0, 1, dx+dg:] = torch.tensor([0.45, -0.956, 0.02])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_cnmp = cnmp.val(val_obs, val_tar_x, val_obs_mask)\n",
    "    traj_cnmp = pred_cnmp[0, :, :dy]\n",
    "\n",
    "    pred_cnep, gate = cnep.val(val_obs, val_tar_x, val_obs_mask)\n",
    "    dec_id = torch.argmax(gate.squeeze(1), dim=-1)\n",
    "    traj_cnep = pred_cnep[dec_id, 0, :, :dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_cnep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate traj_cnmp (400x3) to 1159x3 points\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "x = np.linspace(0, 1, 400)\n",
    "f = interp1d(x, traj_cnmp.squeeze(0), axis=0)\n",
    "xnew = np.linspace(0, 1, 1370)\n",
    "traj_cnmp_interpolated = f(xnew)\n",
    "# interpolate traj_cnep (400x3) to 1159x3 points\n",
    "f = interp1d(x, traj_cnep.squeeze(0), axis=0)\n",
    "traj_cnep_interpolated = f(xnew)\n",
    "\n",
    "t_steps_int = 1370\n",
    "\n",
    "with open('data/current/cnmp.txt', 'w') as f:\n",
    "    for i in range(t_steps_int):\n",
    "        f.write(f'{traj_cnmp_interpolated[i, 0].item()},{traj_cnmp_interpolated[i, 1].item()},{traj_cnmp_interpolated[i, 2].item()}\\n')\n",
    "\n",
    "with open('data/current/cnep.txt', 'w') as f:\n",
    "    for i in range(t_steps_int):\n",
    "        f.write(f'{traj_cnep_interpolated[i, 0].item()},{traj_cnep_interpolated[i, 1].item()},{traj_cnep_interpolated[i, 2].item()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3406, -1.1864,  0.0206])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_cnep[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
