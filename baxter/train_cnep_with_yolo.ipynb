{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# def crop_left(im): \n",
    "#     return transforms.functional.crop(im, top=0, left=0, height=420, width=560)\n",
    "\n",
    "\n",
    "# class FeatureExtractor:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "#         self.features = []\n",
    "\n",
    "#     def hook(self, module, input, output):\n",
    "#         print(f\"Hook called for layer: {module}\")\n",
    "#         self.features.append(output)\n",
    "\n",
    "#     def register_hooks(self, layer_names):\n",
    "#         self.hooks = []\n",
    "#         for name, module in self.model.named_modules():\n",
    "#             if name in layer_names:\n",
    "#                 print(f\"Registering hook for layer: {name}\")\n",
    "#                 hook = module.register_forward_hook(self.hook)\n",
    "#                 self.hooks.append(hook)\n",
    "\n",
    "#     def remove_hooks(self):\n",
    "#         for hook in self.hooks:\n",
    "#             hook.remove()\n",
    "\n",
    "#     def extract_features(self, img_path):\n",
    "#         self.features = []\n",
    "#         img = Image.open(img_path).convert('RGB')  # Load image using PIL\n",
    "#         transform = transforms.Compose([\n",
    "#             transforms.Lambda(crop_left),  # Crop the left side\n",
    "#             transforms.Pad(padding=(40, 110, 40, 110)), # Pad to 640x640\n",
    "#             transforms.ToTensor()\n",
    "#         ])\n",
    "#         img = transform(img).unsqueeze(0)  # Transform to tensor and add batch dimension\n",
    "#         self.model(img)\n",
    "#         return self.features\n",
    "\n",
    "\n",
    "\n",
    "# model = YOLO('yolov8m.pt')\n",
    "# feature_extractor = FeatureExtractor(model)\n",
    "\n",
    "# layer_names = ['model.model.21']\n",
    "# feature_extractor.register_hooks(layer_names)\n",
    "\n",
    "# img_path = f'data/0/img.jpeg'\n",
    "# img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# # Pass dummy input through the model to inspect outputs\n",
    "# features = feature_extractor.extract_features(img_path)\n",
    "# # detections = model.predictor.decode_outputs(outputs[0])\n",
    "# print(features[0].shape)\n",
    "# # print(torch.all(features[0] == features[1]))\n",
    "\n",
    "# feature_extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "is_save = True\n",
    "\n",
    "\n",
    "def crop_left(im): \n",
    "    return transforms.functional.crop(im, top=0, left=0, height=420, width=560)\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.features = []\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "        # Define a downsampling layer\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(576, 128, kernel_size=1, stride=1),  # Reducing channels from 576 to 128\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((16, 16))  # Reducing spatial dimensions to 10x10\n",
    "        ).to(self.device)\n",
    "\n",
    "    def hook(self, module, input, output):\n",
    "        print(f\"Hook called for layer: {module}\")\n",
    "        # Apply downsampling\n",
    "        output = output.to(self.device)\n",
    "        downsampled_output = self.downsample(output)\n",
    "        self.features.append(downsampled_output)\n",
    "\n",
    "    def register_hooks(self, layer_names):\n",
    "        self.hooks = []\n",
    "        for name, module in self.model.model.named_modules():  # Adjusted for YOLOv8 specific submodule access\n",
    "            if name in layer_names:\n",
    "                print(f\"Registering hook for layer: {name}\")\n",
    "                hook = module.register_forward_hook(self.hook)\n",
    "                self.hooks.append(hook)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def extract_features(self, img_path):\n",
    "        self.features = []\n",
    "        img = Image.open(img_path).convert('RGB')  # Load image using PIL\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Lambda(crop_left),  # Crop the left side\n",
    "            transforms.Pad(padding=(40, 110, 40, 110)),  # Pad to 640x640\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        img = transform(img).unsqueeze(0).to('cuda:0')  # Transform to tensor and add batch dimension\n",
    "        self.model(img)\n",
    "        return self.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering hook for layer: model.21\n",
      "\n",
      "Hook called for layer: C2f(\n",
      "  (cv1): Conv(\n",
      "    (conv): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (act): SiLU(inplace=True)\n",
      "  )\n",
      "  (cv2): Conv(\n",
      "    (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (act): SiLU(inplace=True)\n",
      "  )\n",
      "  (m): ModuleList(\n",
      "    (0-1): 2 x Bottleneck(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Hook called for layer: C2f(\n",
      "  (cv1): Conv(\n",
      "    (conv): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (act): SiLU(inplace=True)\n",
      "  )\n",
      "  (cv2): Conv(\n",
      "    (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (act): SiLU(inplace=True)\n",
      "  )\n",
      "  (m): ModuleList(\n",
      "    (0-1): 2 x Bottleneck(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "0: 640x640 1 cup, 12.8ms\n",
      "Speed: 0.0ms preprocess, 12.8ms inference, 310.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([1, 128, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "num_demos = 24\n",
    "t_steps = 400\n",
    "dims = 128 * 16 * 16\n",
    "feats = torch.zeros(num_demos, dims)\n",
    "trajs = torch.zeros(num_demos, t_steps, 8)\n",
    "\n",
    "if is_save:\n",
    "    model = YOLO('yolov8m.pt').to('cuda:0')\n",
    "    feature_extractor = FeatureExtractor(model)\n",
    "\n",
    "    layer_names = ['model.21']\n",
    "    feature_extractor.register_hooks(layer_names)\n",
    "\n",
    "    # Extract features for a given image\n",
    "    for i in range(num_demos):\n",
    "        img_path = f'data/{i}/img.jpeg'\n",
    "        features = feature_extractor.extract_features(img_path)\n",
    "        print(features[0].shape)\n",
    "        break\n",
    "        feats[i] = features[0].view(-1)\n",
    "\n",
    "        data_folder = f'/home/yigit/projects/cnep/baxter/data/{i}/'\n",
    "        # iterate over all files in the in_folder\n",
    "        for filename in os.listdir(data_folder):\n",
    "            d = os.path.join(data_folder, filename)\n",
    "            if filename.endswith('.csv'):\n",
    "                temp_data = []\n",
    "                with open(d, 'r') as f:\n",
    "                    for j, line in enumerate(csv.reader(f)):\n",
    "                        if j > 0:\n",
    "                            temp_data.append([float(line[3]), float(line[4]), float(line[5]), float(line[6]), float(line[7]), float(line[8]), float(line[9]), float(line[10])])  # p, q, gripper\n",
    "\n",
    "        ids = torch.linspace(0, len(temp_data)-1, t_steps).int()\n",
    "\n",
    "        for j in range(t_steps):\n",
    "            trajs[i, j] = torch.tensor(temp_data[ids[j]])\n",
    "\n",
    "    torch.save(trajs, 'trajs.pt')\n",
    "    torch.save(feats, 'feats.pt')\n",
    "\n",
    "    feature_extractor.remove_hooks()\n",
    "else:\n",
    "    trajs = torch.load('trajs.pt')\n",
    "    feats = torch.load('feats.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnep import CNEP\n",
    "from cnmp import CNMP\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_demos, v_num_demos = 2, 2\n",
    "num_classes = 2  # Number of modes\n",
    "num_indiv = num_demos // num_classes  # Number of trajectories per mode\n",
    "num_val_indiv = v_num_demos // num_classes  # Number of trajectories per mode\n",
    "\n",
    "dx = 1\n",
    "dg = dims\n",
    "dy = 8\n",
    "batch_size = 2\n",
    "n_max, m_max = 20, 20\n",
    "t_steps = trajs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(traj_ids: list):\n",
    "    obs.fill_(0)\n",
    "    tar_x.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = trajs[traj_id]\n",
    "        feat = feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "        m = torch.randint(1, m_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "        \n",
    "        obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)  # X\n",
    "        obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)  # G\n",
    "        obs[i, :n, dx+dg:] = traj[n_ids]  # Y\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x[i, :m, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        tar_x[i, :m, dx:] = feat.repeat(m, 1)\n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "val_obs = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_val_batch(traj_ids: list):\n",
    "    val_obs.fill_(0)\n",
    "    val_tar_x.fill_(0)\n",
    "    val_tar_y.fill_(0)\n",
    "    val_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = trajs[traj_id]\n",
    "        feat = feats[traj_id]\n",
    "        n = torch.randint(1, n_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "        \n",
    "        val_obs[i, :n, :dx] = (n_ids/t_steps).unsqueeze(1)\n",
    "        val_obs[i, :n, dx:dx+dg] = feat.repeat(n, 1)\n",
    "        val_obs[i, :n, dx+dg:] = traj[n_ids]\n",
    "        val_obs_mask[i, :n] = True\n",
    "        \n",
    "        val_tar_x[i, :dx] = (m_ids/t_steps).unsqueeze(1)\n",
    "        val_tar_x[i, dx:] = feat.repeat(t_steps, 1)\n",
    "        val_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnep: 177055650\n",
      "cnmp: 177055376\n"
     ]
    }
   ],
   "source": [
    "cnep_ = CNEP(dx+dg, dy, n_max, n_max, [512,128], num_decoders=2, decoder_hidden_dims=[128, 128], batch_size=batch_size, scale_coefs=True, device=device)\n",
    "optimizer_cnep = torch.optim.Adam(lr=3e-4, params=cnep_.parameters())\n",
    "\n",
    "cnmp_ = CNMP(dx+dg, dy, n_max, m_max, [512,128], decoder_hidden_dims=[256, 256], batch_size=batch_size, device=device)\n",
    "optimizer_cnmp = torch.optim.Adam(lr=3e-4, params=cnmp_.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"cnep:\", get_parameter_count(cnep_))\n",
    "print(\"cnmp:\", get_parameter_count(cnmp_))\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    cnep, cnmp = torch.compile(cnep_), torch.compile(cnmp_)\n",
    "else:\n",
    "    cnep, cnmp = cnep_, cnmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacty of 5.77 GiB of which 99.12 MiB is free. Including non-PyTorch memory, this process has 5.66 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 252.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss, nll \u001b[38;5;241m=\u001b[39m cnep\u001b[38;5;241m.\u001b[39mloss(pred, gate, tar_y, tar_mask)\n\u001b[1;32m     49\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m     \u001b[43moptimizer_cnep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     epoch_loss_cnep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nll\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     54\u001b[0m epoch_loss_cnmp \u001b[38;5;241m=\u001b[39m epoch_loss_cnmp\u001b[38;5;241m/\u001b[39mepoch_iter\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:154\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:111\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    109\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacty of 5.77 GiB of which 99.12 MiB is free. Including non-PyTorch memory, this process has 5.66 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 252.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/baxter/cnmp_cnep/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "\n",
    "epochs = 1_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = v_num_demos//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_cnmp, avg_loss_cnep = 0, 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_vl_cnmp, min_vl_cnep = 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "tl_cnmp, tl_cnep = [], []\n",
    "ve_cnmp, ve_cnep = [], []\n",
    "\n",
    "cnmp_tl_path, cnep_tl_path = f'{root_folder}cnmp_training_loss.pt', f'{root_folder}cnep_training_loss.pt'\n",
    "cnmp_ve_path, cnep_ve_path = f'{root_folder}cnmp_validation_error.pt', f'{root_folder}cnep_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_cnmp, epoch_loss_cnep = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(traj_ids[i])\n",
    "\n",
    "        optimizer_cnmp.zero_grad()\n",
    "        pred = cnmp(obs, tar_x, obs_mask)\n",
    "        loss = cnmp.loss(pred, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer_cnmp.step()\n",
    "\n",
    "        epoch_loss_cnmp += loss.item()\n",
    "\n",
    "        optimizer_cnep.zero_grad()\n",
    "        pred, gate = cnep(obs, tar_x, obs_mask)\n",
    "        loss, nll = cnep.loss(pred, gate, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer_cnep.step()\n",
    "\n",
    "        epoch_loss_cnep += nll.item()\n",
    "\n",
    "    epoch_loss_cnmp = epoch_loss_cnmp/epoch_iter\n",
    "    tl_cnmp.append(epoch_loss_cnmp)\n",
    "    epoch_loss_cnep = epoch_loss_cnep/epoch_iter\n",
    "    tl_cnep.append(epoch_loss_cnep)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(v_num_demos)[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_err_cnmp, val_err_cnep = 0, 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                prepare_masked_val_batch(v_traj_ids[j])\n",
    "\n",
    "                p = cnmp.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                vp_means = p[:, :, :dy]\n",
    "                val_err_cnmp += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "                p, g = cnep.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                dec_id = torch.argmax(g.squeeze(1), dim=-1)\n",
    "                vp_means = p[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_err_cnep += mse_loss(vp_means, val_tar_y).item()\n",
    "\n",
    "            val_err_cnmp = val_err_cnmp/v_epoch_iter\n",
    "            val_err_cnep = val_err_cnep/v_epoch_iter\n",
    "\n",
    "            if val_err_cnmp < min_vl_cnmp:\n",
    "                min_vl_cnmp = val_err_cnmp\n",
    "                print(f'CNMP New best: {min_vl_cnmp}')\n",
    "                torch.save(cnmp_.state_dict(), f'{root_folder}saved_models/cnmp.pt')\n",
    "\n",
    "            if val_err_cnep < min_vl_cnep:\n",
    "                min_vl_cnep = val_err_cnep\n",
    "                print(f'CNEP New best: {min_vl_cnep}')\n",
    "                torch.save(cnep_.state_dict(), f'{root_folder}saved_models/cnep.pt')\n",
    "\n",
    "            ve_cnmp.append(val_err_cnmp)\n",
    "            ve_cnep.append(val_err_cnep)\n",
    "\n",
    "    avg_loss_cnmp += epoch_loss_cnmp\n",
    "    avg_loss_cnep += epoch_loss_cnep\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, {}, Min Err: {}, {}\".format(epoch, avg_loss_cnmp/val_per_epoch, avg_loss_cnep/val_per_epoch, min_vl_cnmp, min_vl_cnep))\n",
    "        avg_loss_cnmp, avg_loss_cnep = 0, 0\n",
    "\n",
    "torch.save(torch.Tensor(tl_cnmp), cnmp_tl_path)\n",
    "torch.save(torch.Tensor(ve_cnmp), cnmp_ve_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
