{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 698, 62])\n"
     ]
    }
   ],
   "source": [
    "# from dm_control import suite\n",
    "# from dm_control.locomotion.examples import cmu_2020_tracking\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# env = cmu_2020_tracking.cmu_humanoid_tracking()\n",
    "# time_step = env.reset()\n",
    "\n",
    "# action_spec = env.action_spec()\n",
    "\n",
    "# print('action_spec:', action_spec)\n",
    "\n",
    "# max_frame = 90\n",
    "\n",
    "# width = 480\n",
    "# height = 480\n",
    "# video = np.zeros((90, height, 2 * width, 3), dtype=np.uint8)\n",
    "\n",
    "# # Load one task:\n",
    "# # env = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "\n",
    "# # Step through an episode and print out reward, discount and observation.\n",
    "# action_spec = env.action_spec()\n",
    "# time_step = env.reset()\n",
    "# while not time_step.last():\n",
    "#   for i in range(max_frame):\n",
    "#     action = np.random.uniform(action_spec.minimum,\n",
    "#                              action_spec.maximum,\n",
    "#                              size=action_spec.shape)\n",
    "#     time_step = env.step(action)\n",
    "#     video[i] = np.hstack([env.physics.render(height, width, camera_id=0),\n",
    "#                           env.physics.render(height, width, camera_id=1)])\n",
    "#     #print(time_step.reward, time_step.discount, time_step.observation)\n",
    "#   for i in range(max_frame):\n",
    "#     img = plt.imshow(video[i])\n",
    "#     plt.pause(0.01)  # Need min display time > 0.0.\n",
    "#     plt.draw()\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_frames_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    frames = []\n",
    "    current_frame = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().isdigit():\n",
    "            # New frame, save the previous one\n",
    "            if current_frame:\n",
    "                frames.append(current_frame)\n",
    "            current_frame = []\n",
    "        else:\n",
    "            # Extract numerical values from each line\n",
    "            values = [float(val) for val in line.strip().split()[1:]]\n",
    "            current_frame.extend(values)\n",
    "\n",
    "    # Add the last frame\n",
    "    if current_frame:\n",
    "        frames.append(current_frame)\n",
    "\n",
    "    return torch.tensor(frames)\n",
    "\n",
    "file_path = 'experimental/data/0.txt'\n",
    "data0 = read_frames_from_file(file_path)\n",
    "file_path = 'experimental/data/1.txt'\n",
    "data1 = read_frames_from_file(file_path)\n",
    "file_path = 'experimental/data/2.txt'\n",
    "data2 = read_frames_from_file(file_path)\n",
    "\n",
    "min_frames = min(data0.shape[0], data1.shape[0], data2.shape[0])\n",
    "\n",
    "data = torch.zeros((3, min_frames, data0.shape[1]))\n",
    "data[0] = data0[torch.linspace(0, data0.shape[0] - 1, min_frames).long()]\n",
    "data[1] = data1[torch.linspace(0, data1.shape[0] - 1, min_frames).long()]\n",
    "data[2] = data2[torch.linspace(0, data2.shape[0] - 1, min_frames).long()]\n",
    "\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.wta_cnp import WTA_CNP\n",
    "\n",
    "def get_available_gpu_with_most_memory():\n",
    "    gpu_memory = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch to the GPU to accurately measure memory\n",
    "        gpu_memory.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "\n",
    "    gpu_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return gpu_memory[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_available_gpu_with_most_memory()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)\n",
    "\n",
    "###\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_max_obs, n_max_tar = 20, 20\n",
    "\n",
    "t_steps = data.shape[1]\n",
    "num_demos = 3\n",
    "num_classes = 1\n",
    "num_indiv = num_demos//num_classes  # number of demos per class\n",
    "noise_clip = 0.0\n",
    "dx, dy = 1, data.shape[2]\n",
    "\n",
    "num_val = 3\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['tomato', 'aqua', 'limegreen', 'gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([3, 698, 1]) Y: torch.Size([3, 698, 62]) VX: torch.Size([3, 698, 1]) VY: torch.Size([3, 698, 62])\n"
     ]
    }
   ],
   "source": [
    "def transform_data(data):\n",
    "    num_dimensions = data.shape[2]\n",
    "\n",
    "    transformation_matrix = torch.zeros((num_dimensions, 2))\n",
    "    transformed_data = torch.zeros_like(data)\n",
    "\n",
    "    # Apply transformations to each dimension\n",
    "    for i in range(num_dimensions):\n",
    "        dim_data = data[:, :, i]\n",
    "\n",
    "        min_val = dim_data.min()\n",
    "        max_val = dim_data.max()\n",
    "\n",
    "        transformation_matrix[i, 0] = min_val\n",
    "        transformation_matrix[i, 1] = max_val\n",
    "\n",
    "        transformed_dim = 2 * (dim_data - min_val) / (max_val - min_val) - 1\n",
    "        transformed_data[:, :, i] = transformed_dim\n",
    "\n",
    "    return transformed_data, transformation_matrix\n",
    "\n",
    "def reconstruct_data(transformed_data, transformation_matrix):\n",
    "    num_dimensions = transformed_data.shape[2]\n",
    "\n",
    "    reconstructed_data = torch.zeros_like(transformed_data)\n",
    "\n",
    "    for i in range(num_dimensions):\n",
    "        transformed_dim = transformed_data[:, :, i]\n",
    "        min_val, max_val = transformation_matrix[i, 0], transformation_matrix[i, 1]\n",
    "\n",
    "        reconstructed_dim = ((transformed_dim + 1) / 2) * (max_val - min_val) + min_val\n",
    "        reconstructed_data[:, :, i] = reconstructed_dim\n",
    "\n",
    "    return reconstructed_data\n",
    "\n",
    "y, normalization_transforms = transform_data(data.clone().detach().float().to(device))\n",
    "x = torch.unsqueeze(torch.linspace(0, 1, t_steps).repeat(num_demos, 1), -1).to(device)\n",
    "\n",
    "vx = x.clone()\n",
    "noise = torch.clamp(torch.randn(x.shape)*1e-4**0.5, min=0).to(device)\n",
    "vy = y.clone() + noise\n",
    "\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wta = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [512, 512, 512], num_decoders=1, decoder_hidden_dims=[512, 512, 512], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer_wta = torch.optim.Adam(lr=1e-4, params=model_wta.parameters())\n",
    "\n",
    "# if torch.__version__ >= \"2.0\":\n",
    "#     model_wta = torch.compile(model_wta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WTA)New best: 9842.971435546875\n",
      "Epoch: 0, WTA-Loss: 18.457646484375\n",
      "(WTA)New best: 6497.5723876953125\n",
      "Epoch: 1000, WTA-Loss: 40.74451359081268\n",
      "(WTA)New best: 6259.87841796875\n",
      "Epoch: 2000, WTA-Loss: 11.419576657056808\n",
      "Epoch: 3000, WTA-Loss: 11.023506655216217\n",
      "(WTA)New best: 6234.3880615234375\n",
      "Epoch: 4000, WTA-Loss: 10.66275277209282\n",
      "(WTA)New best: 6200.83642578125\n",
      "Epoch: 5000, WTA-Loss: 10.66713676929474\n",
      "(WTA)New best: 6099.6934814453125\n",
      "Epoch: 6000, WTA-Loss: 10.35142412018776\n",
      "Epoch: 7000, WTA-Loss: 10.115800751924514\n",
      "(WTA)New best: 6037.448974609375\n",
      "Epoch: 8000, WTA-Loss: 10.002525838136673\n",
      "(WTA)New best: 5971.0841064453125\n",
      "Epoch: 9000, WTA-Loss: 9.850870716571809\n",
      "(WTA)New best: 5867.5325927734375\n",
      "Epoch: 10000, WTA-Loss: 9.804976699113846\n",
      "(WTA)New best: 5711.67626953125\n",
      "Epoch: 11000, WTA-Loss: 9.742762154102325\n",
      "(WTA)New best: 5608.576904296875\n",
      "Epoch: 12000, WTA-Loss: 9.684214379787445\n",
      "(WTA)New best: 5552.9656982421875\n",
      "Epoch: 13000, WTA-Loss: 9.635151788473129\n",
      "Epoch: 14000, WTA-Loss: 9.603637376785278\n",
      "(WTA)New best: 5345.1204833984375\n",
      "Epoch: 15000, WTA-Loss: 9.548267970323563\n",
      "(WTA)New best: 5244.9884033203125\n",
      "Epoch: 16000, WTA-Loss: 9.52931715965271\n",
      "(WTA)New best: 5167.623291015625\n",
      "Epoch: 17000, WTA-Loss: 9.472084393739701\n",
      "(WTA)New best: 4998.8251953125\n",
      "Epoch: 18000, WTA-Loss: 9.427051389217377\n",
      "(WTA)New best: 4993.647216796875\n",
      "Epoch: 19000, WTA-Loss: 9.379852696180343\n",
      "(WTA)New best: 4818.372802734375\n",
      "Epoch: 20000, WTA-Loss: 9.340022717475891\n",
      "(WTA)New best: 4802.838623046875\n",
      "Epoch: 21000, WTA-Loss: 9.276287517547608\n",
      "(WTA)New best: 4703.9149169921875\n",
      "Epoch: 22000, WTA-Loss: 9.273097580432891\n",
      "(WTA)New best: 4685.5130615234375\n",
      "Epoch: 23000, WTA-Loss: 9.138882406949996\n",
      "Epoch: 24000, WTA-Loss: 8.995700402975082\n",
      "(WTA)New best: 4597.9366455078125\n",
      "Epoch: 25000, WTA-Loss: 8.81693479371071\n",
      "(WTA)New best: 4401.8450927734375\n",
      "Epoch: 26000, WTA-Loss: 8.513731083393097\n",
      "(WTA)New best: 4290.5665283203125\n",
      "Epoch: 27000, WTA-Loss: 8.307013256371022\n",
      "(WTA)New best: 4251.5286865234375\n",
      "Epoch: 28000, WTA-Loss: 8.060307292699814\n",
      "(WTA)New best: 4236.4871826171875\n",
      "Epoch: 29000, WTA-Loss: 7.904456980109215\n",
      "(WTA)New best: 4136.7601318359375\n",
      "Epoch: 30000, WTA-Loss: 7.75538300549984\n",
      "Epoch: 31000, WTA-Loss: 7.597075355291366\n",
      "(WTA)New best: 4064.7962646484375\n",
      "Epoch: 32000, WTA-Loss: 7.397593565970659\n",
      "Epoch: 33000, WTA-Loss: 7.842617793381214\n",
      "(WTA)New best: 3945.3350830078125\n",
      "Epoch: 34000, WTA-Loss: 7.42356501531601\n",
      "(WTA)New best: 3941.5814208984375\n",
      "Epoch: 35000, WTA-Loss: 7.2020423638820645\n",
      "Epoch: 36000, WTA-Loss: 7.135224531173706\n",
      "(WTA)New best: 3937.564453125\n",
      "Epoch: 37000, WTA-Loss: 7.083411187589169\n",
      "(WTA)New best: 3852.14111328125\n",
      "Epoch: 38000, WTA-Loss: 7.0252779390513895\n",
      "Epoch: 39000, WTA-Loss: 6.968558583498001\n",
      "(WTA)New best: 3779.3192138671875\n",
      "Epoch: 40000, WTA-Loss: 6.971471723645926\n",
      "(WTA)New best: 3748.2318115234375\n",
      "Epoch: 41000, WTA-Loss: 6.8901934234201905\n",
      "(WTA)New best: 3710.197998046875\n",
      "Epoch: 42000, WTA-Loss: 6.895275561749935\n",
      "(WTA)New best: 3596.0099487304688\n",
      "Epoch: 43000, WTA-Loss: 6.715558346003294\n",
      "(WTA)New best: 3527.3869018554688\n",
      "Epoch: 44000, WTA-Loss: 6.710156479686499\n",
      "Epoch: 45000, WTA-Loss: 6.711269570797682\n",
      "Epoch: 46000, WTA-Loss: 6.694120153605938\n",
      "Epoch: 47000, WTA-Loss: 6.640376601040363\n",
      "(WTA)New best: 3492.6801147460938\n",
      "Epoch: 48000, WTA-Loss: 6.514200699955225\n",
      "Epoch: 49000, WTA-Loss: 6.573390472054482\n",
      "Epoch: 50000, WTA-Loss: 6.620307368695736\n",
      "Epoch: 51000, WTA-Loss: 6.684697233021259\n",
      "(WTA)New best: 3422.185546875\n",
      "Epoch: 52000, WTA-Loss: 6.480839922428131\n",
      "(WTA)New best: 3387.6083374023438\n",
      "Epoch: 53000, WTA-Loss: 6.337339423894882\n",
      "Epoch: 54000, WTA-Loss: 14.097004667669534\n",
      "Epoch: 55000, WTA-Loss: 9.518049947738648\n",
      "Epoch: 56000, WTA-Loss: 9.220112349033355\n",
      "Epoch: 57000, WTA-Loss: 9.194066864490509\n",
      "Epoch: 58000, WTA-Loss: 9.132792580366134\n",
      "Epoch: 59000, WTA-Loss: 8.990331289052964\n",
      "Epoch: 60000, WTA-Loss: 8.812248693466186\n",
      "Epoch: 61000, WTA-Loss: 8.480154616475104\n",
      "Epoch: 62000, WTA-Loss: 8.040220318377019\n",
      "Epoch: 63000, WTA-Loss: 7.628843301653862\n",
      "Epoch: 64000, WTA-Loss: 7.409987172663212\n",
      "Epoch: 65000, WTA-Loss: 7.019744467020034\n",
      "Epoch: 66000, WTA-Loss: 6.831909865438938\n",
      "Epoch: 67000, WTA-Loss: 6.710179066747427\n",
      "Epoch: 68000, WTA-Loss: 6.578450913637877\n",
      "Epoch: 69000, WTA-Loss: 6.520002042889595\n",
      "Epoch: 70000, WTA-Loss: 6.426425778448582\n",
      "Epoch: 71000, WTA-Loss: 6.370959420502186\n",
      "Epoch: 72000, WTA-Loss: 6.357750148415565\n",
      "Epoch: 73000, WTA-Loss: 6.207258823603391\n",
      "Epoch: 74000, WTA-Loss: 6.091602690830827\n",
      "Epoch: 75000, WTA-Loss: 6.189179758712649\n",
      "Epoch: 76000, WTA-Loss: 6.060083520516753\n",
      "Epoch: 77000, WTA-Loss: 5.9171545681282876\n",
      "Epoch: 78000, WTA-Loss: 5.877903288694099\n",
      "Epoch: 79000, WTA-Loss: 5.831622587434947\n",
      "Epoch: 80000, WTA-Loss: 5.8293512255698445\n",
      "Epoch: 81000, WTA-Loss: 5.7884712384194135\n",
      "Epoch: 82000, WTA-Loss: 6.3599943895488975\n",
      "Epoch: 83000, WTA-Loss: 5.824897858612239\n",
      "Epoch: 84000, WTA-Loss: 5.982827086910605\n",
      "Epoch: 85000, WTA-Loss: 5.834692722568288\n",
      "Epoch: 86000, WTA-Loss: 5.702116619093344\n",
      "(WTA)New best: 3351.6222534179688\n",
      "Epoch: 87000, WTA-Loss: 5.654314299052581\n",
      "(WTA)New best: 3331.0745239257812\n",
      "Epoch: 88000, WTA-Loss: 5.533084550693165\n",
      "(WTA)New best: 3320.387451171875\n",
      "Epoch: 89000, WTA-Loss: 5.514941012792638\n",
      "Epoch: 90000, WTA-Loss: 5.381095506708836\n",
      "Epoch: 91000, WTA-Loss: 5.356444334664615\n",
      "(WTA)New best: 3234.3974609375\n",
      "Epoch: 92000, WTA-Loss: 5.2935212501809\n",
      "(WTA)New best: 3231.5714111328125\n",
      "Epoch: 93000, WTA-Loss: 5.251569563896861\n",
      "(WTA)New best: 3198.6123046875\n",
      "Epoch: 94000, WTA-Loss: 5.246792999780446\n",
      "(WTA)New best: 3197.6212158203125\n",
      "Epoch: 95000, WTA-Loss: 5.203929063668591\n",
      "(WTA)New best: 3155.367431640625\n",
      "Epoch: 96000, WTA-Loss: 5.07157883198699\n",
      "(WTA)New best: 3148.7327270507812\n",
      "Epoch: 97000, WTA-Loss: 5.136609172224999\n",
      "(WTA)New best: 3143.8640747070312\n",
      "Epoch: 98000, WTA-Loss: 5.066966957881144\n",
      "(WTA)New best: 3071.822021484375\n",
      "Epoch: 99000, WTA-Loss: 6.041682737634518\n",
      "Epoch: 100000, WTA-Loss: 5.948632725127507\n",
      "(WTA)New best: 3071.1859741210938\n",
      "Epoch: 101000, WTA-Loss: 5.422361069427337\n",
      "Epoch: 102000, WTA-Loss: 5.141176489984617\n",
      "Epoch: 103000, WTA-Loss: 5.468109032480744\n",
      "(WTA)New best: 3030.5933227539062\n",
      "Epoch: 104000, WTA-Loss: 5.688416495040059\n",
      "Epoch: 105000, WTA-Loss: 5.300403316651005\n",
      "Epoch: 106000, WTA-Loss: 5.303916948257712\n",
      "(WTA)New best: 3027.9863891601562\n",
      "Epoch: 107000, WTA-Loss: 5.134734632994514\n",
      "Epoch: 108000, WTA-Loss: 4.999837391234003\n",
      "Epoch: 109000, WTA-Loss: 4.920496051899158\n",
      "Epoch: 110000, WTA-Loss: 5.8525813695713875\n",
      "(WTA)New best: 2990.2835693359375\n",
      "Epoch: 111000, WTA-Loss: 4.988728774413699\n",
      "(WTA)New best: 2985.5823364257812\n",
      "Epoch: 112000, WTA-Loss: 4.856927391528152\n",
      "Epoch: 113000, WTA-Loss: 71.31215153899649\n",
      "(WTA)New best: 2964.3571166992188\n",
      "Epoch: 114000, WTA-Loss: 17.036840983198957\n",
      "(WTA)New best: 2902.521484375\n",
      "Epoch: 115000, WTA-Loss: 5.150519821080379\n",
      "Epoch: 116000, WTA-Loss: 5.162706175270723\n",
      "Epoch: 117000, WTA-Loss: 5.09779164412868\n",
      "Epoch: 118000, WTA-Loss: 5.2369408848322925\n",
      "Epoch: 119000, WTA-Loss: 5.108978503820021\n",
      "(WTA)New best: 2876.9700317382812\n",
      "Epoch: 120000, WTA-Loss: 5.199879724308616\n",
      "Epoch: 121000, WTA-Loss: 4.985231226479635\n",
      "Epoch: 122000, WTA-Loss: 4.9836653506543955\n",
      "(WTA)New best: 2872.729736328125\n",
      "Epoch: 123000, WTA-Loss: 5.017323631772073\n",
      "Epoch: 124000, WTA-Loss: 4.815959603038384\n",
      "(WTA)New best: 2863.7158203125\n",
      "Epoch: 125000, WTA-Loss: 4.7226477405147165\n",
      "Epoch: 126000, WTA-Loss: 9.827270550073823\n",
      "Epoch: 127000, WTA-Loss: 4.824742844356224\n",
      "Epoch: 128000, WTA-Loss: 4.66077655018866\n",
      "(WTA)New best: 2833.0912475585938\n",
      "Epoch: 129000, WTA-Loss: 4.751310130488127\n",
      "(WTA)New best: 2825.3971557617188\n",
      "Epoch: 130000, WTA-Loss: 4.774716346646659\n",
      "(WTA)New best: 2808.7645263671875\n",
      "Epoch: 131000, WTA-Loss: 4.743899532273994\n",
      "Epoch: 132000, WTA-Loss: 4.75296757962252\n",
      "Epoch: 133000, WTA-Loss: 17.26016631729342\n",
      "(WTA)New best: 2799.6153564453125\n",
      "Epoch: 134000, WTA-Loss: 6.034787074445747\n",
      "Epoch: 135000, WTA-Loss: 11.163803318463732\n",
      "(WTA)New best: 2717.5718383789062\n",
      "Epoch: 136000, WTA-Loss: 6.039627245634328\n",
      "Epoch: 137000, WTA-Loss: 12837607452.492525\n",
      "Epoch: 138000, WTA-Loss: 7.763372974157333\n",
      "Epoch: 139000, WTA-Loss: 6.643479047060013\n",
      "Epoch: 140000, WTA-Loss: 14.626770893841982\n",
      "Epoch: 141000, WTA-Loss: 6.642223210692405\n",
      "Epoch: 142000, WTA-Loss: 7.762309003144503\n",
      "Epoch: 143000, WTA-Loss: 813.5909583054483\n",
      "Epoch: 144000, WTA-Loss: 9.98460874724388\n",
      "Epoch: 145000, WTA-Loss: 8.129861396193505\n",
      "Epoch: 146000, WTA-Loss: 907.9030014355183\n",
      "Epoch: 147000, WTA-Loss: 1818471582.6958728\n",
      "Epoch: 148000, WTA-Loss: 9.724915188550948\n",
      "Epoch: 149000, WTA-Loss: 9.146965497851372\n",
      "Epoch: 150000, WTA-Loss: 321.27788120675086\n",
      "Epoch: 151000, WTA-Loss: 9.313769253730774\n",
      "Epoch: 152000, WTA-Loss: 8.974749928951264\n",
      "Epoch: 153000, WTA-Loss: 8.609065020442008\n",
      "Epoch: 154000, WTA-Loss: 8.727374693870544\n",
      "Epoch: 155000, WTA-Loss: 8.327342857003211\n",
      "Epoch: 156000, WTA-Loss: 8.3115931879282\n",
      "Epoch: 157000, WTA-Loss: 8.272410065948963\n",
      "Epoch: 158000, WTA-Loss: 8.007686197936534\n",
      "Epoch: 159000, WTA-Loss: 7.851601919293404\n",
      "Epoch: 160000, WTA-Loss: 7.99073259985447\n",
      "Epoch: 161000, WTA-Loss: 7.745699404776096\n",
      "Epoch: 162000, WTA-Loss: 7.462480117082595\n",
      "Epoch: 163000, WTA-Loss: 7.216975917756558\n",
      "Epoch: 164000, WTA-Loss: 7.180869576573372\n",
      "Epoch: 165000, WTA-Loss: 6.716108618617058\n",
      "Epoch: 166000, WTA-Loss: 6.530556816056371\n",
      "Epoch: 167000, WTA-Loss: 6.3319180863499644\n",
      "Epoch: 168000, WTA-Loss: 6.119700246274471\n",
      "Epoch: 169000, WTA-Loss: 5.993913736216724\n",
      "Epoch: 170000, WTA-Loss: 5.907141653679311\n",
      "Epoch: 171000, WTA-Loss: 5.683957015642896\n",
      "Epoch: 172000, WTA-Loss: 5.631328678337857\n",
      "Epoch: 173000, WTA-Loss: 5.55044235027954\n",
      "Epoch: 174000, WTA-Loss: 5.471671231921762\n",
      "Epoch: 175000, WTA-Loss: 5.37809319551778\n",
      "Epoch: 176000, WTA-Loss: 5.35430021643173\n",
      "Epoch: 177000, WTA-Loss: 5.281696876482339\n",
      "Epoch: 178000, WTA-Loss: 5.245934308779775\n",
      "Epoch: 179000, WTA-Loss: 5.159978393019177\n",
      "Epoch: 180000, WTA-Loss: 5.117388959125615\n",
      "Epoch: 181000, WTA-Loss: 5.085774946756661\n",
      "Epoch: 182000, WTA-Loss: 5.0228200050783345\n",
      "Epoch: 183000, WTA-Loss: 4.990162344216369\n",
      "Epoch: 184000, WTA-Loss: 4.944669983369531\n",
      "Epoch: 185000, WTA-Loss: 4.878439383398276\n",
      "Epoch: 186000, WTA-Loss: 4.900044090742012\n",
      "Epoch: 187000, WTA-Loss: 4.76577166530781\n",
      "Epoch: 188000, WTA-Loss: 4.752575909726322\n",
      "Epoch: 189000, WTA-Loss: 4.800275884154835\n",
      "Epoch: 190000, WTA-Loss: 4.7572342262686\n",
      "Epoch: 191000, WTA-Loss: 4.7132081930766585\n",
      "Epoch: 192000, WTA-Loss: 4.662446903721429\n",
      "Epoch: 193000, WTA-Loss: 4.707377353152028\n",
      "Epoch: 194000, WTA-Loss: 4.5880453380155375\n",
      "Epoch: 195000, WTA-Loss: 4.5765843571034495\n",
      "Epoch: 196000, WTA-Loss: 4.562009676180314\n",
      "Epoch: 197000, WTA-Loss: 4.517322912270785\n",
      "Epoch: 198000, WTA-Loss: 4.554428443974816\n",
      "Epoch: 199000, WTA-Loss: 4.512604768398218\n",
      "Epoch: 200000, WTA-Loss: 4.478116297730245\n",
      "Epoch: 201000, WTA-Loss: 4.423867762269452\n",
      "Epoch: 202000, WTA-Loss: 4.434171279139817\n",
      "Epoch: 203000, WTA-Loss: 4.46046699341014\n",
      "Epoch: 204000, WTA-Loss: 4.39699098684825\n",
      "Epoch: 205000, WTA-Loss: 4.547583128771745\n",
      "Epoch: 206000, WTA-Loss: 4.590505325695209\n",
      "Epoch: 207000, WTA-Loss: 4.431558288711589\n",
      "Epoch: 208000, WTA-Loss: 4.358895618813345\n",
      "Epoch: 209000, WTA-Loss: 4.324347143924795\n",
      "Epoch: 210000, WTA-Loss: 4.463187916184077\n",
      "Epoch: 211000, WTA-Loss: 4.467535287925042\n",
      "Epoch: 212000, WTA-Loss: 4.458718808413483\n",
      "Epoch: 213000, WTA-Loss: 4.319385931067634\n",
      "Epoch: 214000, WTA-Loss: 4.310665467824787\n",
      "Epoch: 215000, WTA-Loss: 4.2116781614553185\n",
      "Epoch: 216000, WTA-Loss: 4.53575959459506\n",
      "Epoch: 217000, WTA-Loss: 4.258982118595392\n",
      "Epoch: 218000, WTA-Loss: 4.310462593567558\n",
      "Epoch: 219000, WTA-Loss: 4.335893805998378\n",
      "Epoch: 220000, WTA-Loss: 4.26177649214305\n",
      "Epoch: 221000, WTA-Loss: 4.138831937316805\n",
      "Epoch: 222000, WTA-Loss: 4.183028290975373\n",
      "Epoch: 223000, WTA-Loss: 4.187523263205309\n",
      "Epoch: 224000, WTA-Loss: 4.160057993860042\n",
      "Epoch: 225000, WTA-Loss: 4.178459960158914\n",
      "Epoch: 226000, WTA-Loss: 4.086846255501732\n",
      "Epoch: 227000, WTA-Loss: 4.2692673692838286\n",
      "Epoch: 228000, WTA-Loss: 4.148556393795647\n",
      "Epoch: 229000, WTA-Loss: 4.169042901408859\n",
      "Epoch: 230000, WTA-Loss: 4.1136257777302525\n",
      "Epoch: 231000, WTA-Loss: 4.037920693638283\n",
      "Epoch: 232000, WTA-Loss: 4.105699955201301\n",
      "Epoch: 233000, WTA-Loss: 4.041835228048032\n",
      "Epoch: 234000, WTA-Loss: 3.9473437708215786\n",
      "Epoch: 235000, WTA-Loss: 4.023602415943518\n",
      "Epoch: 236000, WTA-Loss: 4.011837825622584\n",
      "Epoch: 237000, WTA-Loss: 3.9365249351534293\n",
      "Epoch: 238000, WTA-Loss: 3.943624718738254\n",
      "(WTA)New best: 2715.25927734375\n",
      "Epoch: 239000, WTA-Loss: 3.927051633025985\n",
      "(WTA)New best: 2705.6114501953125\n",
      "Epoch: 240000, WTA-Loss: 3.9630459008305334\n",
      "(WTA)New best: 2676.3685913085938\n",
      "Epoch: 241000, WTA-Loss: 3.9239500622437338\n",
      "Epoch: 242000, WTA-Loss: 3.901964559292886\n",
      "(WTA)New best: 2674.7977294921875\n",
      "Epoch: 243000, WTA-Loss: 3.8920264759785494\n",
      "Epoch: 244000, WTA-Loss: 3.9149499808583643\n",
      "(WTA)New best: 2673.4537963867188\n",
      "Epoch: 245000, WTA-Loss: 3.893952794468554\n",
      "(WTA)New best: 2659.3329467773438\n",
      "Epoch: 246000, WTA-Loss: 3.8662020202087004\n",
      "(WTA)New best: 2654.3002319335938\n",
      "Epoch: 247000, WTA-Loss: 3.8995910434289836\n",
      "(WTA)New best: 2644.6629638671875\n",
      "Epoch: 248000, WTA-Loss: 3.892583332948852\n",
      "(WTA)New best: 2635.8194580078125\n",
      "Epoch: 249000, WTA-Loss: 3.809956265444052\n",
      "Epoch: 250000, WTA-Loss: 3.759864201412303\n",
      "(WTA)New best: 2627.51123046875\n",
      "Epoch: 251000, WTA-Loss: 3.846140799015062\n",
      "Epoch: 252000, WTA-Loss: 125411597.7354029\n",
      "Epoch: 253000, WTA-Loss: 4.487925018623239\n",
      "(WTA)New best: 2596.10595703125\n",
      "Epoch: 254000, WTA-Loss: 4.264559454148635\n",
      "(WTA)New best: 2573.962158203125\n",
      "Epoch: 255000, WTA-Loss: 4.13361944954656\n",
      "Epoch: 256000, WTA-Loss: 4.020483445393155\n",
      "Epoch: 257000, WTA-Loss: 3.9324910160543514\n",
      "(WTA)New best: 2567.4501953125\n",
      "Epoch: 258000, WTA-Loss: 3.837624485915527\n",
      "Epoch: 259000, WTA-Loss: 3.823475958856172\n",
      "Epoch: 260000, WTA-Loss: 3.830325942289521\n",
      "(WTA)New best: 2562.56787109375\n",
      "Epoch: 261000, WTA-Loss: 3.8108724941122274\n",
      "Epoch: 262000, WTA-Loss: 3.7706272338111884\n",
      "(WTA)New best: 2557.5213623046875\n",
      "Epoch: 263000, WTA-Loss: 3.797315698911378\n",
      "Epoch: 264000, WTA-Loss: 3.7683843934969046\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/experimental/{dy}D/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "if not os.path.exists(f'{root_folder}img/'):\n",
    "    os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 5_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_wta = 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_wta, min_val_loss_cnp = 1000000, 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_wta, validation_error_wta = [], []\n",
    "training_loss_cnp, validation_error_cnp = [], []\n",
    "\n",
    "wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_wta = 0\n",
    "\n",
    "    traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_wta.zero_grad()\n",
    "\n",
    "        obs_wta, tar_x_wta, tar_y_wta = get_batch(x, y, traj_ids[i], device)\n",
    "        pred_wta, gate_wta = model_wta(obs_wta, tar_x_wta)\n",
    "        loss_wta, wta_nll = model_wta.loss(pred_wta, gate_wta, tar_y_wta)\n",
    "        loss_wta.backward()\n",
    "        optimizer_wta.step()\n",
    "\n",
    "        epoch_loss_wta += wta_nll.item()\n",
    "\n",
    "    training_loss_wta.append(epoch_loss_wta)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss_wta = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_wta, g_wta = model_wta(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss_wta += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "            validation_error_wta.append(val_loss_wta)\n",
    "            if val_loss_wta < min_val_loss_wta:\n",
    "                min_val_loss_wta = val_loss_wta\n",
    "                print(f'(WTA)New best: {min_val_loss_wta}')\n",
    "                torch.save(model_wta.state_dict(), f'{root_folder}saved_models/wta_on_synth.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss_wta += epoch_loss_wta\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, WTA-Loss: {}\".format(epoch, avg_loss_wta/val_per_epoch))\n",
    "        avg_loss_wta = 0\n",
    "\n",
    "torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
