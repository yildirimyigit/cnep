{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from models.cnp import CNP\n",
    "import torch\n",
    "\n",
    "def get_available_gpu_with_most_memory():\n",
    "    gpu_memory = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch to the GPU to accurately measure memory\n",
    "        gpu_memory.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "\n",
    "    gpu_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return gpu_memory[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_available_gpu_with_most_memory()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)\n",
    "\n",
    "###\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Calculate the mean and standard deviation across all data points\n",
    "    mean = np.mean(data, axis=(0, 1))\n",
    "    std = np.std(data, axis=(0, 1))\n",
    "    \n",
    "    # Handle the case where std is zero (to avoid division by zero)\n",
    "    std_replaced = np.where(std == 0, 1, std)\n",
    "    \n",
    "    # Normalize the data\n",
    "    normalized_data = (data - mean) / std_replaced\n",
    "    return normalized_data, mean, std_replaced\n",
    "\n",
    "\n",
    "walk_heavy_actions, (walk_heavy_observations, _, _) = np.load(\"data/mocapact/awh.npy\"), normalize_data(np.load(\"data/mocapact/owh.npy\"))\n",
    "run_circle_actions, (run_circle_observations, _, _) = np.load(\"data/mocapact/arc.npy\"), normalize_data(np.load(\"data/mocapact/orc.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "n_max_obs, n_max_tar = 32, 32\n",
    "\n",
    "num_indiv, t_steps, dx = walk_heavy_observations.shape\n",
    "_, _, dy = walk_heavy_actions.shape\n",
    "num_indiv -= 6\n",
    "\n",
    "num_val = 12\n",
    "num_classes = 2\n",
    "num_demos = num_indiv*num_classes\n",
    "\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['tomato', 'aqua']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([62, 208, 287]) Y: torch.Size([62, 208, 56]) VX: torch.Size([12, 208, 287]) VY: torch.Size([12, 208, 56])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(num_demos, t_steps, dx, device=device)\n",
    "y = torch.zeros(num_demos, t_steps, dy, device=device)\n",
    "vx = torch.zeros(num_val, t_steps, dx, device=device)\n",
    "vy = torch.zeros(num_val, t_steps, dy, device=device)\n",
    "\n",
    "vind = torch.randint(0, num_indiv+num_val_indiv, (num_val_indiv, 1))\n",
    "tr_ctr, val_ctr = 0, 0\n",
    "for cur_vind in vind:\n",
    "    vx[val_ctr] = torch.from_numpy(walk_heavy_observations[cur_vind]).to(device)\n",
    "    vx[val_ctr+1] = torch.from_numpy(run_circle_observations[cur_vind]).to(device)\n",
    "    vy[val_ctr] = torch.from_numpy(walk_heavy_actions[cur_vind]).to(device)\n",
    "    vy[val_ctr+1] = torch.from_numpy(run_circle_actions[cur_vind]).to(device)\n",
    "\n",
    "    val_ctr += 2\n",
    "\n",
    "i=0\n",
    "while i*2 < num_val_indiv+num_indiv:\n",
    "    if i in vind:\n",
    "       pass\n",
    "    else:\n",
    "        x[tr_ctr] = torch.from_numpy(walk_heavy_observations[i]).to(device)\n",
    "        y[tr_ctr] = torch.from_numpy(walk_heavy_actions[i]).to(device)\n",
    "        x[tr_ctr+num_indiv] = torch.from_numpy(run_circle_observations[i]).to(device)\n",
    "        y[tr_ctr+num_indiv] = torch.from_numpy(run_circle_actions[i]).to(device)\n",
    "        tr_ctr += 1\n",
    "    i += 1\n",
    "\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNP: 1956464\n"
     ]
    }
   ],
   "source": [
    "model_cnp_ = CNP(input_dim=dx, hidden_dim=512, output_dim=dy, n_max_obs=n_max_obs, n_max_tar=n_max_tar, num_layers=4, batch_size=batch_size).to(device)\n",
    "optimizer_cnp = torch.optim.Adam(lr=3e-5, params=model_cnp_.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"CNP:\", get_parameter_count(model_cnp_))\n",
    "\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model_cnp = torch.compile(model_cnp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "[2024-01-18 14:46:48,226] [0/4] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.20279599726200104\n",
      "Epoch: 0, cnp-Loss: 0.03854569447040558\n",
      "Validation loss: 0.010029243305325508\n",
      "Epoch: 1000, cnp-Loss: 17.658447007417678\n",
      "Validation loss: 0.009185771457850933\n",
      "Epoch: 2000, cnp-Loss: 17.32927520042658\n",
      "Validation loss: 0.009483774192631245\n",
      "Epoch: 3000, cnp-Loss: 17.275837319433688\n",
      "Validation loss: 0.009795695543289185\n",
      "Epoch: 4000, cnp-Loss: 17.241886497676372\n",
      "Validation loss: 0.01035606674849987\n",
      "Epoch: 5000, cnp-Loss: 17.218094841182232\n",
      "Validation loss: 0.010936410166323185\n",
      "Epoch: 6000, cnp-Loss: 17.199477543771266\n",
      "Validation loss: 0.011406578123569489\n",
      "Epoch: 7000, cnp-Loss: 17.18525534069538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-19 07:06:19,963] [0/6] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.012042392045259476\n",
      "Epoch: 8000, cnp-Loss: 17.174116916954517\n",
      "Validation loss: 0.012432021088898182\n",
      "Epoch: 9000, cnp-Loss: 17.16534583592415\n",
      "Validation loss: 0.012798115611076355\n",
      "Epoch: 10000, cnp-Loss: 17.158465910375117\n",
      "Validation loss: 0.013360819779336452\n",
      "Epoch: 11000, cnp-Loss: 17.152914523780346\n",
      "Validation loss: 0.013575556688010693\n",
      "Epoch: 12000, cnp-Loss: 17.1484868516922\n",
      "Validation loss: 0.01390207652002573\n",
      "Epoch: 13000, cnp-Loss: 17.144867695569992\n",
      "Validation loss: 0.014200146310031414\n",
      "Epoch: 14000, cnp-Loss: 17.142008959412575\n",
      "Validation loss: 0.014411796815693378\n",
      "Epoch: 15000, cnp-Loss: 17.13975729238987\n",
      "Validation loss: 0.014683328568935394\n",
      "Epoch: 16000, cnp-Loss: 17.137812535464764\n",
      "Validation loss: 0.01483162958174944\n",
      "Epoch: 17000, cnp-Loss: 17.13644407939911\n",
      "Validation loss: 0.015044915489852428\n",
      "Epoch: 18000, cnp-Loss: 17.135176597595215\n",
      "Validation loss: 0.01521748024970293\n",
      "Epoch: 19000, cnp-Loss: 17.134154054522515\n",
      "Validation loss: 0.015250771306455135\n",
      "Epoch: 20000, cnp-Loss: 17.133413684487344\n",
      "Validation loss: 0.015331954695284367\n",
      "Epoch: 21000, cnp-Loss: 17.132712265133858\n",
      "Validation loss: 0.015444725751876831\n",
      "Epoch: 22000, cnp-Loss: 17.132185663819314\n",
      "Validation loss: 0.015584979206323624\n",
      "Epoch: 23000, cnp-Loss: 17.131663811802863\n",
      "Validation loss: 0.015665525570511818\n",
      "Epoch: 24000, cnp-Loss: 17.1313492410779\n",
      "Validation loss: 0.01560500729829073\n",
      "Epoch: 25000, cnp-Loss: 17.131009833335877\n",
      "Validation loss: 0.015827922150492668\n",
      "Epoch: 26000, cnp-Loss: 17.130618335425854\n",
      "Validation loss: 0.015707271173596382\n",
      "Epoch: 27000, cnp-Loss: 17.130530522823335\n",
      "Validation loss: 0.015785392373800278\n",
      "Epoch: 28000, cnp-Loss: 17.130214244663716\n",
      "Validation loss: 0.0157278124243021\n",
      "Epoch: 29000, cnp-Loss: 17.129956388652325\n",
      "Validation loss: 0.015873244032263756\n",
      "Epoch: 30000, cnp-Loss: 17.129805933237076\n",
      "Validation loss: 0.01601945236325264\n",
      "Epoch: 31000, cnp-Loss: 17.129679937958716\n",
      "Validation loss: 0.0158833134919405\n",
      "Epoch: 32000, cnp-Loss: 17.129458123624325\n",
      "Validation loss: 0.01597677916288376\n",
      "Epoch: 33000, cnp-Loss: 17.12937993389368\n",
      "Validation loss: 0.015792207792401314\n",
      "Epoch: 34000, cnp-Loss: 17.12930003374815\n",
      "Validation loss: 0.01595696620643139\n",
      "Epoch: 35000, cnp-Loss: 17.129194636523724\n",
      "Validation loss: 0.015825005248188972\n",
      "Epoch: 36000, cnp-Loss: 17.12914774209261\n",
      "Validation loss: 0.015778228640556335\n",
      "Epoch: 37000, cnp-Loss: 17.128987105607987\n",
      "Validation loss: 0.015863800421357155\n",
      "Epoch: 38000, cnp-Loss: 17.128885540246962\n",
      "Validation loss: 0.01579572819173336\n",
      "Epoch: 39000, cnp-Loss: 17.128834489881992\n",
      "Validation loss: 0.015735561028122902\n",
      "Epoch: 40000, cnp-Loss: 17.12876556533575\n",
      "Validation loss: 0.015674080699682236\n",
      "Epoch: 41000, cnp-Loss: 17.128688856065274\n",
      "Validation loss: 0.015842951834201813\n",
      "Epoch: 42000, cnp-Loss: 17.12862353605032\n",
      "Validation loss: 0.015815794467926025\n",
      "Epoch: 43000, cnp-Loss: 17.12860445046425\n",
      "Validation loss: 0.0157844927161932\n",
      "Epoch: 44000, cnp-Loss: 17.128536621272563\n",
      "Validation loss: 0.01569739170372486\n",
      "Epoch: 45000, cnp-Loss: 17.128414360761642\n",
      "Validation loss: 0.015740538015961647\n",
      "Epoch: 46000, cnp-Loss: 17.128460837185383\n",
      "Validation loss: 0.01586129702627659\n",
      "Epoch: 47000, cnp-Loss: 17.12846068382263\n",
      "Validation loss: 0.0157447662204504\n",
      "Epoch: 48000, cnp-Loss: 17.128390115976334\n",
      "Validation loss: 0.01581539213657379\n",
      "Epoch: 49000, cnp-Loss: 17.128331756055356\n",
      "Validation loss: 0.01579958200454712\n",
      "Epoch: 50000, cnp-Loss: 17.12827324998379\n",
      "Validation loss: 0.015579323284327984\n",
      "Epoch: 51000, cnp-Loss: 17.12820537126064\n",
      "Validation loss: 0.015751266852021217\n",
      "Epoch: 52000, cnp-Loss: 17.12817315071821\n",
      "Validation loss: 0.01564793474972248\n",
      "Epoch: 53000, cnp-Loss: 17.128119244098663\n",
      "Validation loss: 0.015428740531206131\n",
      "Epoch: 54000, cnp-Loss: 17.128067531466485\n",
      "Validation loss: 0.015632031485438347\n",
      "Epoch: 55000, cnp-Loss: 17.128053378641606\n",
      "Validation loss: 0.015577174723148346\n",
      "Epoch: 56000, cnp-Loss: 17.12803404843807\n",
      "Validation loss: 0.015525631606578827\n",
      "Epoch: 57000, cnp-Loss: 17.127975486278533\n",
      "Validation loss: 0.015594818629324436\n",
      "Epoch: 58000, cnp-Loss: 17.127961788237094\n",
      "Validation loss: 0.015601268969476223\n",
      "Epoch: 59000, cnp-Loss: 17.127920958936215\n",
      "Validation loss: 0.015511521138250828\n",
      "Epoch: 60000, cnp-Loss: 17.127983954548835\n",
      "Validation loss: 0.015569419600069523\n",
      "Epoch: 61000, cnp-Loss: 17.127871476590634\n",
      "Validation loss: 0.015515447594225407\n",
      "Epoch: 62000, cnp-Loss: 17.12786424535513\n",
      "Validation loss: 0.015605808235704899\n",
      "Epoch: 63000, cnp-Loss: 17.127827756643295\n",
      "Validation loss: 0.015446245670318604\n",
      "Epoch: 64000, cnp-Loss: 17.12785978871584\n",
      "Validation loss: 0.015507563948631287\n",
      "Epoch: 65000, cnp-Loss: 17.127762408435345\n",
      "Validation loss: 0.015587232075631618\n",
      "Epoch: 66000, cnp-Loss: 17.12774452191591\n",
      "Validation loss: 0.015566126443445683\n",
      "Epoch: 67000, cnp-Loss: 17.12773154860735\n",
      "Validation loss: 0.01546341460198164\n",
      "Epoch: 68000, cnp-Loss: 17.1277182661891\n",
      "Validation loss: 0.015461951494216919\n",
      "Epoch: 69000, cnp-Loss: 17.12766028922796\n",
      "Validation loss: 0.01536079216748476\n",
      "Epoch: 70000, cnp-Loss: 17.127668580293655\n",
      "Validation loss: 0.015376575291156769\n",
      "Epoch: 71000, cnp-Loss: 17.127661803722383\n",
      "Validation loss: 0.01540016382932663\n",
      "Epoch: 72000, cnp-Loss: 17.127627512037755\n",
      "Validation loss: 0.015519186854362488\n",
      "Epoch: 73000, cnp-Loss: 17.12758774250746\n",
      "Validation loss: 0.0154978446662426\n",
      "Epoch: 74000, cnp-Loss: 17.1275552585721\n",
      "Validation loss: 0.015445786528289318\n",
      "Epoch: 75000, cnp-Loss: 17.12755316609144\n",
      "Validation loss: 0.015369574539363384\n",
      "Epoch: 76000, cnp-Loss: 17.12754082018137\n",
      "Validation loss: 0.01525877695530653\n",
      "Epoch: 77000, cnp-Loss: 17.127532109200953\n",
      "Validation loss: 0.01536830049008131\n",
      "Epoch: 78000, cnp-Loss: 17.127470067977907\n",
      "Validation loss: 0.015352781862020493\n",
      "Epoch: 79000, cnp-Loss: 17.12751734417677\n",
      "Validation loss: 0.015328940004110336\n",
      "Epoch: 80000, cnp-Loss: 17.12752066028118\n",
      "Validation loss: 0.015219469554722309\n",
      "Epoch: 81000, cnp-Loss: 17.12748145097494\n",
      "Validation loss: 0.015329251997172832\n",
      "Epoch: 82000, cnp-Loss: 17.12739787298441\n",
      "Validation loss: 0.015344414860010147\n",
      "Epoch: 83000, cnp-Loss: 17.127391377329825\n",
      "Validation loss: 0.015326426364481449\n",
      "Epoch: 84000, cnp-Loss: 17.127394678652287\n",
      "Validation loss: 0.015297153033316135\n",
      "Epoch: 85000, cnp-Loss: 17.127373494386674\n",
      "Validation loss: 0.015192844904959202\n",
      "Epoch: 86000, cnp-Loss: 17.127342625916004\n",
      "Validation loss: 0.015111456625163555\n",
      "Epoch: 87000, cnp-Loss: 17.12729559004307\n",
      "Validation loss: 0.01529811043292284\n",
      "Epoch: 88000, cnp-Loss: 17.127406134426593\n",
      "Validation loss: 0.015286390669643879\n",
      "Epoch: 89000, cnp-Loss: 17.127296267092227\n",
      "Validation loss: 0.015117420814931393\n",
      "Epoch: 90000, cnp-Loss: 17.12728330755234\n",
      "Validation loss: 0.015301507897675037\n",
      "Epoch: 91000, cnp-Loss: 17.127263392150404\n",
      "Validation loss: 0.015382252633571625\n",
      "Epoch: 92000, cnp-Loss: 17.12726846754551\n",
      "Validation loss: 0.015382777899503708\n",
      "Epoch: 93000, cnp-Loss: 17.127249239265918\n",
      "Validation loss: 0.015225276350975037\n",
      "Epoch: 94000, cnp-Loss: 17.127236494660377\n",
      "Validation loss: 0.01526611763983965\n",
      "Epoch: 95000, cnp-Loss: 17.12718972814083\n",
      "Validation loss: 0.015144921839237213\n",
      "Epoch: 96000, cnp-Loss: 17.127222689926626\n",
      "Validation loss: 0.01516890898346901\n",
      "Epoch: 97000, cnp-Loss: 17.127176765859126\n",
      "Validation loss: 0.015330466441810131\n",
      "Epoch: 98000, cnp-Loss: 17.127169525682927\n",
      "Validation loss: 0.01522092055529356\n",
      "Epoch: 99000, cnp-Loss: 17.127149931490422\n",
      "Validation loss: 0.015217222273349762\n",
      "Epoch: 100000, cnp-Loss: 17.127150500774384\n",
      "Validation loss: 0.015120181255042553\n",
      "New best: 0.015120181255042553\n",
      "Epoch: 101000, cnp-Loss: 17.127131895661353\n",
      "Validation loss: 0.0152534618973732\n",
      "Epoch: 102000, cnp-Loss: 17.127110702216626\n",
      "Validation loss: 0.015190166421234608\n",
      "Epoch: 103000, cnp-Loss: 17.127125586628914\n",
      "Validation loss: 0.015134986490011215\n",
      "Epoch: 104000, cnp-Loss: 17.127111849963665\n",
      "Validation loss: 0.015085835009813309\n",
      "New best: 0.015085835009813309\n",
      "Epoch: 105000, cnp-Loss: 17.12703658759594\n",
      "Validation loss: 0.015188578516244888\n",
      "Epoch: 106000, cnp-Loss: 17.127096562981606\n",
      "Validation loss: 0.015154220163822174\n",
      "Epoch: 107000, cnp-Loss: 17.127087257385252\n",
      "Validation loss: 0.01512051373720169\n",
      "Epoch: 108000, cnp-Loss: 17.12705681794882\n",
      "Validation loss: 0.01498464960604906\n",
      "New best: 0.01498464960604906\n",
      "Epoch: 109000, cnp-Loss: 17.12704618281126\n",
      "Validation loss: 0.015003443695604801\n",
      "Epoch: 110000, cnp-Loss: 17.126994392216204\n",
      "Validation loss: 0.015091297216713428\n",
      "Epoch: 111000, cnp-Loss: 17.12700130790472\n",
      "Validation loss: 0.015039953403174877\n",
      "Epoch: 112000, cnp-Loss: 17.127045603990556\n",
      "Validation loss: 0.015025429427623749\n",
      "Epoch: 113000, cnp-Loss: 17.127030496776104\n",
      "Validation loss: 0.015084739774465561\n",
      "Epoch: 114000, cnp-Loss: 17.127004040539266\n",
      "Validation loss: 0.015064702369272709\n",
      "Epoch: 115000, cnp-Loss: 17.1269983420372\n",
      "Validation loss: 0.015109055675566196\n",
      "Epoch: 116000, cnp-Loss: 17.12697161281109\n",
      "Validation loss: 0.01496603712439537\n",
      "New best: 0.01496603712439537\n",
      "Epoch: 117000, cnp-Loss: 17.12696732920408\n",
      "Validation loss: 0.014899703674018383\n",
      "New best: 0.014899703674018383\n",
      "Epoch: 118000, cnp-Loss: 17.126984857857227\n",
      "Validation loss: 0.014986406080424786\n",
      "Epoch: 119000, cnp-Loss: 17.126963875710963\n",
      "Validation loss: 0.01493957731872797\n",
      "Epoch: 120000, cnp-Loss: 17.12696279257536\n",
      "Validation loss: 0.014976263977587223\n",
      "Epoch: 121000, cnp-Loss: 17.126919410586357\n",
      "Validation loss: 0.01509547233581543\n",
      "Epoch: 122000, cnp-Loss: 17.12687523782253\n",
      "Validation loss: 0.01516558974981308\n",
      "Epoch: 123000, cnp-Loss: 17.126956586122514\n",
      "Validation loss: 0.015035686083137989\n",
      "Epoch: 124000, cnp-Loss: 17.126893596827983\n",
      "Validation loss: 0.015029177069664001\n",
      "Epoch: 125000, cnp-Loss: 17.12685750323534\n",
      "Validation loss: 0.015097816474735737\n",
      "Epoch: 126000, cnp-Loss: 17.12687219041586\n",
      "Validation loss: 0.015097369439899921\n",
      "Epoch: 127000, cnp-Loss: 17.12685586106777\n",
      "Validation loss: 0.014905310235917568\n",
      "Epoch: 128000, cnp-Loss: 17.126863242268563\n",
      "Validation loss: 0.01513658743351698\n",
      "Epoch: 129000, cnp-Loss: 17.126848946392535\n",
      "Validation loss: 0.014896721579134464\n",
      "New best: 0.014896721579134464\n",
      "Epoch: 130000, cnp-Loss: 17.126858964800835\n",
      "Validation loss: 0.015089720487594604\n",
      "Epoch: 131000, cnp-Loss: 17.12684914159775\n",
      "Validation loss: 0.015007912181317806\n",
      "Epoch: 132000, cnp-Loss: 17.12685552340746\n",
      "Validation loss: 0.014980584383010864\n",
      "Epoch: 133000, cnp-Loss: 17.126820368528367\n",
      "Validation loss: 0.014912008307874203\n",
      "Epoch: 134000, cnp-Loss: 17.126828517913818\n",
      "Validation loss: 0.015011340379714966\n",
      "Epoch: 135000, cnp-Loss: 17.12679379582405\n",
      "Validation loss: 0.01483714859932661\n",
      "New best: 0.01483714859932661\n",
      "Epoch: 136000, cnp-Loss: 17.12683914923668\n",
      "Validation loss: 0.01499928068369627\n",
      "Epoch: 137000, cnp-Loss: 17.126823070168495\n",
      "Validation loss: 0.015061193145811558\n",
      "Epoch: 138000, cnp-Loss: 17.126807234942913\n",
      "Validation loss: 0.014888090081512928\n",
      "Epoch: 139000, cnp-Loss: 17.126803314685823\n",
      "Validation loss: 0.015027149580419064\n",
      "Epoch: 140000, cnp-Loss: 17.12676889961958\n",
      "Validation loss: 0.015058203600347042\n",
      "Epoch: 141000, cnp-Loss: 17.126807250857354\n",
      "Validation loss: 0.014860902912914753\n",
      "Epoch: 142000, cnp-Loss: 17.126781567692756\n",
      "Validation loss: 0.014842632226645947\n",
      "Epoch: 143000, cnp-Loss: 17.12678875041008\n",
      "Validation loss: 0.014981663785874844\n",
      "Epoch: 144000, cnp-Loss: 17.126757140815258\n",
      "Validation loss: 0.014868016354739666\n",
      "Epoch: 145000, cnp-Loss: 17.126756249547004\n",
      "Validation loss: 0.014788332395255566\n",
      "New best: 0.014788332395255566\n",
      "Epoch: 146000, cnp-Loss: 17.126740458846093\n",
      "Validation loss: 0.01471638958901167\n",
      "New best: 0.01471638958901167\n",
      "Epoch: 147000, cnp-Loss: 17.12674760377407\n",
      "Validation loss: 0.014728453010320663\n",
      "Epoch: 148000, cnp-Loss: 17.126746219038964\n",
      "Validation loss: 0.014792592264711857\n",
      "Epoch: 149000, cnp-Loss: 17.12671792423725\n",
      "Validation loss: 0.014811848290264606\n",
      "Epoch: 150000, cnp-Loss: 17.126758114635944\n",
      "Validation loss: 0.014730758033692837\n",
      "Epoch: 151000, cnp-Loss: 17.12670859313011\n",
      "Validation loss: 0.014868238009512424\n",
      "Epoch: 152000, cnp-Loss: 17.126709580600263\n",
      "Validation loss: 0.015020127408206463\n",
      "Epoch: 153000, cnp-Loss: 17.12671352428198\n",
      "Validation loss: 0.014697704464197159\n",
      "New best: 0.014697704464197159\n",
      "Epoch: 154000, cnp-Loss: 17.126717997670173\n",
      "Validation loss: 0.014904291369020939\n",
      "Epoch: 155000, cnp-Loss: 17.12670645624399\n",
      "Validation loss: 0.014813932590186596\n",
      "Epoch: 156000, cnp-Loss: 17.126727024614812\n",
      "Validation loss: 0.014818462543189526\n",
      "Epoch: 157000, cnp-Loss: 17.126655999183654\n",
      "Validation loss: 0.014817786403000355\n",
      "Epoch: 158000, cnp-Loss: 17.126682965934275\n",
      "Validation loss: 0.014780865050852299\n",
      "Epoch: 159000, cnp-Loss: 17.126661500573157\n",
      "Validation loss: 0.014674830250442028\n",
      "New best: 0.014674830250442028\n",
      "Epoch: 160000, cnp-Loss: 17.126672515273093\n",
      "Validation loss: 0.014664425514638424\n",
      "New best: 0.014664425514638424\n",
      "Epoch: 161000, cnp-Loss: 17.126695540904997\n",
      "Validation loss: 0.014871194958686829\n",
      "Epoch: 162000, cnp-Loss: 17.12667654109001\n",
      "Validation loss: 0.014667517505586147\n",
      "Epoch: 163000, cnp-Loss: 17.126648822009564\n",
      "Validation loss: 0.014853380620479584\n",
      "Epoch: 164000, cnp-Loss: 17.126620180785658\n",
      "Validation loss: 0.014776636846363544\n",
      "Epoch: 165000, cnp-Loss: 17.126633416950703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m pred_cnp, _ \u001b[38;5;241m=\u001b[39m model_cnp(obs_cnp, tar_x_cnp)\n\u001b[1;32m     55\u001b[0m loss_cnp \u001b[38;5;241m=\u001b[39m model_cnp\u001b[38;5;241m.\u001b[39mloss(pred_cnp, tar_y_cnp)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss_cnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer_cnp\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m epoch_loss_cnp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_cnp\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3232\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.backward\u001b[0;34m(ctx, *flat_args)\u001b[0m\n\u001b[1;32m   3230\u001b[0m     out \u001b[38;5;241m=\u001b[39m CompiledFunctionBackward\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39mall_args)\n\u001b[1;32m   3231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3232\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcall_compiled_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3204\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.backward.<locals>.call_compiled_backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(saved_context), context(), track_graph_compiling(aot_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   3200\u001b[0m         CompiledFunction\u001b[38;5;241m.\u001b[39mcompiled_bw \u001b[38;5;241m=\u001b[39m aot_config\u001b[38;5;241m.\u001b[39mbw_compiler(\n\u001b[1;32m   3201\u001b[0m             bw_module, placeholder_list\n\u001b[1;32m   3202\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3207\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3211\u001b[0m out \u001b[38;5;241m=\u001b[39m functionalized_rng_runtime_epilogue(CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata, out)\n\u001b[1;32m   3212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:374\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:401\u001b[0m, in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[1;32m    393\u001b[0m     compiled_graph\u001b[38;5;241m.\u001b[39mcompiled_artifact \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mload_by_key_path(\n\u001b[1;32m    394\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mcache_key,\n\u001b[1;32m    395\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39martifact_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m (),\n\u001b[1;32m    399\u001b[0m     )\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_yigit/4q/c4qjx43p4zwgelzmotprcwweiei6nlwkgf65tuo73brhyygnqlin.py:384\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    382\u001b[0m buf1 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m512\u001b[39m), (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Source Nodes: [], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m \u001b[43mextern_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtangents_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m112\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ms1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m112\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_15\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m view_15\n\u001b[1;32m    386\u001b[0m buf2 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m112\u001b[39m), (\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/mocapact/{dy}D/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "# if not os.path.exists(f'{root_folder}img/'):\n",
    "#     os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 1_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_cnp = 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_cnp = 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_cnp, validation_error_cnp = [], []\n",
    "\n",
    "cnp_tr_loss_path = f'{root_folder}cnp_training_loss.pt'\n",
    "cnp_val_err_path = f'{root_folder}cnp_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_cnp = 0\n",
    "\n",
    "    # traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "    traj_ids, v_traj_ids = [], []\n",
    "    inds = torch.randperm(num_indiv)\n",
    "    vinds = torch.randperm(num_val)[:num_val_indiv]\n",
    "\n",
    "    for i in range(inds.shape[0]):\n",
    "        first = inds[i] * torch.randint(1,3,(1,1)).item()  # randint changes the order if it returns 2. for input randomization\n",
    "        second = num_demos-first-1\n",
    "        traj_ids.append([first, second])\n",
    "\n",
    "    for i in range(vinds.shape[0]):\n",
    "        v_traj_ids.append([vinds[i], num_val-vinds[i]-1])\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_cnp.zero_grad()\n",
    "\n",
    "        obs_cnp, tar_x_cnp, tar_y_cnp = get_batch(x, y, traj_ids[i], device)\n",
    "        pred_cnp, _ = model_cnp(obs_cnp, tar_x_cnp)\n",
    "        loss_cnp = model_cnp.loss(pred_cnp, tar_y_cnp)\n",
    "        loss_cnp.backward()\n",
    "        optimizer_cnp.step()\n",
    "\n",
    "        epoch_loss_cnp += loss_cnp.item()\n",
    "\n",
    "    training_loss_cnp.append(epoch_loss_cnp)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            # v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss_cnp = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_cnp, t_cnp, tr_cnp = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_cnp, _ = model_cnp(o_cnp, t_cnp)\n",
    "                val_loss_cnp += mse_loss(p_cnp[:, :, :model_cnp.output_dim], tr_cnp)\n",
    "\n",
    "            val_loss_cnp /= num_val\n",
    "            validation_error_cnp.append(val_loss_cnp)\n",
    "            print(f'Validation loss: {val_loss_cnp}')\n",
    "            if val_loss_cnp < min_val_loss_cnp and epoch > 1e5:\n",
    "                min_val_loss_cnp = val_loss_cnp\n",
    "                print(f'New best: {min_val_loss_cnp}')\n",
    "                torch.save(model_cnp_.state_dict(), f'{root_folder}saved_models/cnp_on_synth.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss_cnp += epoch_loss_cnp\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, cnp-Loss: {}\".format(epoch, avg_loss_cnp/val_per_epoch))\n",
    "        avg_loss_cnp = 0\n",
    "\n",
    "torch.save(torch.Tensor(training_loss_cnp), cnp_tr_loss_path)\n",
    "torch.save(torch.Tensor(validation_error_cnp), cnp_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
