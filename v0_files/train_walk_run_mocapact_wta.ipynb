{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from models.wta_cnp import WTA_CNP\n",
    "import torch\n",
    "\n",
    "def get_available_gpu_with_most_memory():\n",
    "    gpu_memory = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch to the GPU to accurately measure memory\n",
    "        gpu_memory.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "\n",
    "    gpu_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return gpu_memory[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_available_gpu_with_most_memory()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)\n",
    "\n",
    "###\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Calculate the mean and standard deviation across all data points\n",
    "    mean = np.mean(data, axis=(0, 1))\n",
    "    std = np.std(data, axis=(0, 1))\n",
    "    \n",
    "    # Handle the case where std is zero (to avoid division by zero)\n",
    "    std_replaced = np.where(std == 0, 1, std)\n",
    "    \n",
    "    # Normalize the data\n",
    "    normalized_data = (data - mean) / std_replaced\n",
    "    return normalized_data, mean, std_replaced\n",
    "\n",
    "\n",
    "walk_heavy_actions, (walk_heavy_observations, _, _) = np.load(\"data/mocapact/awh.npy\"), normalize_data(np.load(\"data/mocapact/owh.npy\"))\n",
    "run_circle_actions, (run_circle_observations, _, _) = np.load(\"data/mocapact/arc.npy\"), normalize_data(np.load(\"data/mocapact/orc.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "n_max_obs, n_max_tar = 32, 32\n",
    "\n",
    "num_indiv, t_steps, dx = walk_heavy_observations.shape\n",
    "_, _, dy = walk_heavy_actions.shape\n",
    "num_indiv -= 6\n",
    "\n",
    "num_val = 12\n",
    "num_classes = 2\n",
    "num_demos = num_indiv*num_classes\n",
    "\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['tomato', 'aqua']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([62, 208, 287]) Y: torch.Size([62, 208, 56]) VX: torch.Size([12, 208, 287]) VY: torch.Size([12, 208, 56])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(num_demos, t_steps, dx, device=device)\n",
    "y = torch.zeros(num_demos, t_steps, dy, device=device)\n",
    "vx = torch.zeros(num_val, t_steps, dx, device=device)\n",
    "vy = torch.zeros(num_val, t_steps, dy, device=device)\n",
    "\n",
    "vind = torch.randint(0, num_indiv+num_val_indiv, (num_val_indiv, 1))\n",
    "tr_ctr, val_ctr = 0, 0\n",
    "for cur_vind in vind:\n",
    "    vx[val_ctr] = torch.from_numpy(walk_heavy_observations[cur_vind]).to(device)\n",
    "    vx[val_ctr+1] = torch.from_numpy(run_circle_observations[cur_vind]).to(device)\n",
    "    vy[val_ctr] = torch.from_numpy(walk_heavy_actions[cur_vind]).to(device)\n",
    "    vy[val_ctr+1] = torch.from_numpy(run_circle_actions[cur_vind]).to(device)\n",
    "\n",
    "    val_ctr += 2\n",
    "\n",
    "i=0\n",
    "while i*2 < num_val_indiv+num_indiv:\n",
    "    if i in vind:\n",
    "       pass\n",
    "    else:\n",
    "        x[tr_ctr] = torch.from_numpy(walk_heavy_observations[i]).to(device)\n",
    "        y[tr_ctr] = torch.from_numpy(walk_heavy_actions[i]).to(device)\n",
    "        x[tr_ctr+num_indiv] = torch.from_numpy(run_circle_observations[i]).to(device)\n",
    "        y[tr_ctr+num_indiv] = torch.from_numpy(run_circle_actions[i]).to(device)\n",
    "        tr_ctr += 1\n",
    "    i += 1\n",
    "\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WTA-CNP: 1741362\n"
     ]
    }
   ],
   "source": [
    "model_wta_ = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [1024, 512, 256], num_decoders=2, decoder_hidden_dims=[360, 360, 360], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer_wta = torch.optim.Adam(lr=3e-5, params=model_wta_.parameters())\n",
    "\n",
    "def get_parameter_count(model):\n",
    "    total_num = 0\n",
    "    for param in model.parameters():\n",
    "        total_num += param.shape.numel()\n",
    "    return total_num\n",
    "\n",
    "print(\"WTA-CNP:\", get_parameter_count(model_wta_))\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model_wta = torch.compile(model_wta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/yigit/.local/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "[2024-01-18 14:46:53,286] [0/5] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WTA)Validation loss: 0.4004123757282893\n",
      "Epoch: 0, WTA-Loss: 0.03878955090045929\n",
      "(WTA)Validation loss: 0.020542152225971222\n",
      "Epoch: 1000, WTA-Loss: 17.698264050126078\n",
      "(WTA)Validation loss: 0.019522906591494877\n",
      "Epoch: 2000, WTA-Loss: 17.396631286382675\n",
      "(WTA)Validation loss: 0.01881197684754928\n",
      "Epoch: 3000, WTA-Loss: 17.35177454453707\n",
      "(WTA)Validation loss: 0.018003832238415878\n",
      "Epoch: 4000, WTA-Loss: 17.326031472444534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-18 21:00:44,749] [0/7] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WTA)Validation loss: 0.01806862683345874\n",
      "Epoch: 5000, WTA-Loss: 17.30832500445843\n",
      "(WTA)Validation loss: 0.017290004373838503\n",
      "Epoch: 6000, WTA-Loss: 17.294547849714757\n",
      "(WTA)Validation loss: 0.01782176100338499\n",
      "Epoch: 7000, WTA-Loss: 17.28356212031841\n",
      "(WTA)Validation loss: 0.020119949554403622\n",
      "Epoch: 8000, WTA-Loss: 17.274070960342883\n",
      "(WTA)Validation loss: 0.01832819435124596\n",
      "Epoch: 9000, WTA-Loss: 17.265990211308\n",
      "(WTA)Validation loss: 0.019862435292452574\n",
      "Epoch: 10000, WTA-Loss: 17.258642950177194\n",
      "(WTA)Validation loss: 0.02109148974219958\n",
      "Epoch: 11000, WTA-Loss: 17.252478099286556\n",
      "(WTA)Validation loss: 0.02088058413937688\n",
      "Epoch: 12000, WTA-Loss: 17.2469499669075\n",
      "(WTA)Validation loss: 0.02078925088668863\n",
      "Epoch: 13000, WTA-Loss: 17.241587978124617\n",
      "(WTA)Validation loss: 0.02217521828909715\n",
      "Epoch: 14000, WTA-Loss: 17.237081035494803\n",
      "(WTA)Validation loss: 0.02007465964804093\n",
      "Epoch: 15000, WTA-Loss: 17.232904402375222\n",
      "(WTA)Validation loss: 0.021700883905092876\n",
      "Epoch: 16000, WTA-Loss: 17.229143583834173\n",
      "(WTA)Validation loss: 0.0220158143589894\n",
      "Epoch: 17000, WTA-Loss: 17.225463337659836\n",
      "(WTA)Validation loss: 0.021314214294155438\n",
      "Epoch: 18000, WTA-Loss: 17.222332395672797\n",
      "(WTA)Validation loss: 0.022650543910761673\n",
      "Epoch: 19000, WTA-Loss: 17.21918940514326\n",
      "(WTA)Validation loss: 0.022756127019723255\n",
      "Epoch: 20000, WTA-Loss: 17.216616922676565\n",
      "(WTA)Validation loss: 0.021792476996779442\n",
      "Epoch: 21000, WTA-Loss: 17.214045675754548\n",
      "(WTA)Validation loss: 0.02359896618872881\n",
      "Epoch: 22000, WTA-Loss: 17.21165482866764\n",
      "(WTA)Validation loss: 0.022664876654744148\n",
      "Epoch: 23000, WTA-Loss: 17.209360710680485\n",
      "(WTA)Validation loss: 0.024373449385166168\n",
      "Epoch: 24000, WTA-Loss: 17.207324290633203\n",
      "(WTA)Validation loss: 0.024101012386381626\n",
      "Epoch: 25000, WTA-Loss: 17.20528501945734\n",
      "(WTA)Validation loss: 0.02444557597239812\n",
      "Epoch: 26000, WTA-Loss: 17.203627157390116\n",
      "(WTA)Validation loss: 0.02452044654637575\n",
      "Epoch: 27000, WTA-Loss: 17.201962579607965\n",
      "(WTA)Validation loss: 0.024763773816327255\n",
      "Epoch: 28000, WTA-Loss: 17.20033644682169\n",
      "(WTA)Validation loss: 0.024988941227396328\n",
      "Epoch: 29000, WTA-Loss: 17.198900086581705\n",
      "(WTA)Validation loss: 0.02671529756238063\n",
      "Epoch: 30000, WTA-Loss: 17.19743719369173\n",
      "(WTA)Validation loss: 0.025659235194325447\n",
      "Epoch: 31000, WTA-Loss: 17.196023221433162\n",
      "(WTA)Validation loss: 0.025976828299462795\n",
      "Epoch: 32000, WTA-Loss: 17.194961108982564\n",
      "(WTA)Validation loss: 0.02594008358816306\n",
      "Epoch: 33000, WTA-Loss: 17.193528866648673\n",
      "(WTA)Validation loss: 0.02597684847811858\n",
      "Epoch: 34000, WTA-Loss: 17.19240580445528\n",
      "(WTA)Validation loss: 0.024990844540297985\n",
      "Epoch: 35000, WTA-Loss: 17.191385938465594\n",
      "(WTA)Validation loss: 0.02648235484957695\n",
      "Epoch: 36000, WTA-Loss: 17.190200355947017\n",
      "(WTA)Validation loss: 0.025321781324843567\n",
      "Epoch: 37000, WTA-Loss: 17.18930600488186\n",
      "(WTA)Validation loss: 0.02684734823803107\n",
      "Epoch: 38000, WTA-Loss: 17.188265088021755\n",
      "(WTA)Validation loss: 0.027156299600998562\n",
      "Epoch: 39000, WTA-Loss: 17.187314045250417\n",
      "(WTA)Validation loss: 0.027025133992234867\n",
      "Epoch: 40000, WTA-Loss: 17.186445682883264\n",
      "(WTA)Validation loss: 0.02724799110243718\n",
      "Epoch: 41000, WTA-Loss: 17.185491652190684\n",
      "(WTA)Validation loss: 0.029301609223087628\n",
      "Epoch: 42000, WTA-Loss: 17.1848305554986\n",
      "(WTA)Validation loss: 0.027629874025781948\n",
      "Epoch: 43000, WTA-Loss: 17.1839771733284\n",
      "(WTA)Validation loss: 0.02923783039053281\n",
      "Epoch: 44000, WTA-Loss: 17.183317904949188\n",
      "(WTA)Validation loss: 0.029603082686662674\n",
      "Epoch: 45000, WTA-Loss: 17.182595589876176\n",
      "(WTA)Validation loss: 0.026797556318342686\n",
      "Epoch: 46000, WTA-Loss: 17.181850688159464\n",
      "(WTA)Validation loss: 0.026746835249165695\n",
      "Epoch: 47000, WTA-Loss: 17.181277596354484\n",
      "(WTA)Validation loss: 0.027053869950274628\n",
      "Epoch: 48000, WTA-Loss: 17.18055314028263\n",
      "(WTA)Validation loss: 0.030187302579482395\n",
      "Epoch: 49000, WTA-Loss: 17.179891135811804\n",
      "(WTA)Validation loss: 0.028716060953835647\n",
      "Epoch: 50000, WTA-Loss: 17.179329488158228\n",
      "(WTA)Validation loss: 0.02717013160387675\n",
      "Epoch: 51000, WTA-Loss: 17.178916633605958\n",
      "(WTA)Validation loss: 0.029106739287575085\n",
      "Epoch: 52000, WTA-Loss: 17.178260326504706\n",
      "(WTA)Validation loss: 0.02912056539207697\n",
      "Epoch: 53000, WTA-Loss: 17.177758854985235\n",
      "(WTA)Validation loss: 0.029169083883364994\n",
      "Epoch: 54000, WTA-Loss: 17.17722687846422\n",
      "(WTA)Validation loss: 0.029254551666478317\n",
      "Epoch: 55000, WTA-Loss: 17.176736877858637\n",
      "(WTA)Validation loss: 0.031275082690020405\n",
      "Epoch: 56000, WTA-Loss: 17.176247557282448\n",
      "(WTA)Validation loss: 0.029559399311741192\n",
      "Epoch: 57000, WTA-Loss: 17.175803799986838\n",
      "(WTA)Validation loss: 0.03138351161032915\n",
      "Epoch: 58000, WTA-Loss: 17.175337570250033\n",
      "(WTA)Validation loss: 0.029921294189989567\n",
      "Epoch: 59000, WTA-Loss: 17.1747180583477\n",
      "(WTA)Validation loss: 0.02994159422814846\n",
      "Epoch: 60000, WTA-Loss: 17.174401376008987\n",
      "(WTA)Validation loss: 0.030278479990859825\n",
      "Epoch: 61000, WTA-Loss: 17.17400239998102\n",
      "(WTA)Validation loss: 0.031909617905815445\n",
      "Epoch: 62000, WTA-Loss: 17.173412371456624\n",
      "(WTA)Validation loss: 0.030365068775912125\n",
      "Epoch: 63000, WTA-Loss: 17.173159970641137\n",
      "(WTA)Validation loss: 0.02872384525835514\n",
      "Epoch: 64000, WTA-Loss: 17.172741160988807\n",
      "(WTA)Validation loss: 0.028743076759080093\n",
      "Epoch: 65000, WTA-Loss: 17.17241464805603\n",
      "(WTA)Validation loss: 0.030622672600050766\n",
      "Epoch: 66000, WTA-Loss: 17.172092536330222\n",
      "(WTA)Validation loss: 0.03059909337510665\n",
      "Epoch: 67000, WTA-Loss: 17.171563857734203\n",
      "(WTA)Validation loss: 0.029214270412921906\n",
      "Epoch: 68000, WTA-Loss: 17.171319213688374\n",
      "(WTA)Validation loss: 0.03076550519714753\n",
      "Epoch: 69000, WTA-Loss: 17.171062982320784\n",
      "(WTA)Validation loss: 0.030955990465978783\n",
      "Epoch: 70000, WTA-Loss: 17.170727420389653\n",
      "(WTA)Validation loss: 0.03134825577338537\n",
      "Epoch: 71000, WTA-Loss: 17.17030289363861\n",
      "(WTA)Validation loss: 0.02918763862301906\n",
      "Epoch: 72000, WTA-Loss: 17.170028385818004\n",
      "(WTA)Validation loss: 0.03150611153493325\n",
      "Epoch: 73000, WTA-Loss: 17.169617635071276\n",
      "(WTA)Validation loss: 0.031148746609687805\n",
      "Epoch: 74000, WTA-Loss: 17.169275249004365\n",
      "(WTA)Validation loss: 0.031413040744761624\n",
      "Epoch: 75000, WTA-Loss: 17.16912684774399\n",
      "(WTA)Validation loss: 0.03135728122045597\n",
      "Epoch: 76000, WTA-Loss: 17.168801340818405\n",
      "(WTA)Validation loss: 0.02986577618867159\n",
      "Epoch: 77000, WTA-Loss: 17.16857090628147\n",
      "(WTA)Validation loss: 0.0335098405679067\n",
      "Epoch: 78000, WTA-Loss: 17.16826789999008\n",
      "(WTA)Validation loss: 0.03190686088055372\n",
      "Epoch: 79000, WTA-Loss: 17.167955078661443\n",
      "(WTA)Validation loss: 0.030318547661105793\n",
      "Epoch: 80000, WTA-Loss: 17.167760929405688\n",
      "(WTA)Validation loss: 0.030081707052886486\n",
      "Epoch: 81000, WTA-Loss: 17.16750274449587\n",
      "(WTA)Validation loss: 0.032155960177381836\n",
      "Epoch: 82000, WTA-Loss: 17.16724590986967\n",
      "(WTA)Validation loss: 0.03207589810093244\n",
      "Epoch: 83000, WTA-Loss: 17.166965461671353\n",
      "(WTA)Validation loss: 0.030536514706909657\n",
      "Epoch: 84000, WTA-Loss: 17.166803961694242\n",
      "(WTA)Validation loss: 0.03205966825286547\n",
      "Epoch: 85000, WTA-Loss: 17.16648944824934\n",
      "(WTA)Validation loss: 0.03260273269067208\n",
      "Epoch: 86000, WTA-Loss: 17.166293401420116\n",
      "(WTA)Validation loss: 0.03062973978618781\n",
      "Epoch: 87000, WTA-Loss: 17.16606307911873\n",
      "(WTA)Validation loss: 0.030943057499825954\n",
      "Epoch: 88000, WTA-Loss: 17.165753895938398\n",
      "(WTA)Validation loss: 0.03238955264290174\n",
      "Epoch: 89000, WTA-Loss: 17.165563448250293\n",
      "(WTA)Validation loss: 0.03057013979802529\n",
      "Epoch: 90000, WTA-Loss: 17.1654165686965\n",
      "(WTA)Validation loss: 0.0328609279046456\n",
      "Epoch: 91000, WTA-Loss: 17.16523906815052\n",
      "(WTA)Validation loss: 0.031236964277923107\n",
      "Epoch: 92000, WTA-Loss: 17.164984989881514\n",
      "(WTA)Validation loss: 0.03112895010660092\n",
      "Epoch: 93000, WTA-Loss: 17.164798378527163\n",
      "(WTA)Validation loss: 0.032760502149661384\n",
      "Epoch: 94000, WTA-Loss: 17.164667847275734\n",
      "(WTA)Validation loss: 0.0330782321592172\n",
      "Epoch: 95000, WTA-Loss: 17.164399833202364\n",
      "(WTA)Validation loss: 0.03325095493346453\n",
      "Epoch: 96000, WTA-Loss: 17.164180616259575\n",
      "(WTA)Validation loss: 0.03315171195815007\n",
      "Epoch: 97000, WTA-Loss: 17.16401039433479\n",
      "(WTA)Validation loss: 0.033474099511901535\n",
      "Epoch: 98000, WTA-Loss: 17.163951807141306\n",
      "(WTA)Validation loss: 0.033421373615662255\n",
      "Epoch: 99000, WTA-Loss: 17.163672322034834\n",
      "(WTA)Validation loss: 0.03529630911846956\n",
      "Epoch: 100000, WTA-Loss: 17.16335945546627\n",
      "(WTA)Validation loss: 0.0334640946239233\n",
      "(WTA)New best: 0.0334640946239233\n",
      "Epoch: 101000, WTA-Loss: 17.16342746722698\n",
      "(WTA)Validation loss: 0.032007948805888496\n",
      "(WTA)New best: 0.032007948805888496\n",
      "Epoch: 102000, WTA-Loss: 17.16307032352686\n",
      "(WTA)Validation loss: 0.033810073199371495\n",
      "Epoch: 103000, WTA-Loss: 17.16289247739315\n",
      "(WTA)Validation loss: 0.03346627547095219\n",
      "Epoch: 104000, WTA-Loss: 17.16279221379757\n",
      "(WTA)Validation loss: 0.035618613163630165\n",
      "Epoch: 105000, WTA-Loss: 17.16262888467312\n",
      "(WTA)Validation loss: 0.03392866191764673\n",
      "Epoch: 106000, WTA-Loss: 17.162456385433675\n",
      "(WTA)Validation loss: 0.03206914166609446\n",
      "Epoch: 107000, WTA-Loss: 17.162275655806063\n",
      "(WTA)Validation loss: 0.033867730448643364\n",
      "Epoch: 108000, WTA-Loss: 17.162081072926522\n",
      "(WTA)Validation loss: 0.03397988403836886\n",
      "Epoch: 109000, WTA-Loss: 17.16193845540285\n",
      "(WTA)Validation loss: 0.03414576345433792\n",
      "Epoch: 110000, WTA-Loss: 17.161837545633315\n",
      "(WTA)Validation loss: 0.034199076083799206\n",
      "Epoch: 111000, WTA-Loss: 17.161795774757863\n",
      "(WTA)Validation loss: 0.03414593388636907\n",
      "Epoch: 112000, WTA-Loss: 17.16156446492672\n",
      "(WTA)Validation loss: 0.03436028119176626\n",
      "Epoch: 113000, WTA-Loss: 17.161373096346857\n",
      "(WTA)Validation loss: 0.03393662596742312\n",
      "Epoch: 114000, WTA-Loss: 17.16136368918419\n",
      "(WTA)Validation loss: 0.03278483791897694\n",
      "Epoch: 115000, WTA-Loss: 17.161253160655498\n",
      "(WTA)Validation loss: 0.034564211033284664\n",
      "Epoch: 116000, WTA-Loss: 17.16090096718073\n",
      "(WTA)Validation loss: 0.034386332146823406\n",
      "Epoch: 117000, WTA-Loss: 17.160973203539847\n",
      "(WTA)Validation loss: 0.034880246967077255\n",
      "Epoch: 118000, WTA-Loss: 17.16073646378517\n",
      "(WTA)Validation loss: 0.034621168238421283\n",
      "Epoch: 119000, WTA-Loss: 17.160541265547277\n",
      "(WTA)Validation loss: 0.03268786302457253\n",
      "Epoch: 120000, WTA-Loss: 17.160511139929294\n",
      "(WTA)Validation loss: 0.034849148554106556\n",
      "Epoch: 121000, WTA-Loss: 17.160440561890603\n",
      "(WTA)Validation loss: 0.034775017450253166\n",
      "Epoch: 122000, WTA-Loss: 17.16018061763048\n",
      "(WTA)Validation loss: 0.036800011371572815\n",
      "Epoch: 123000, WTA-Loss: 17.160138081371784\n",
      "(WTA)Validation loss: 0.03480691431711117\n",
      "Epoch: 124000, WTA-Loss: 17.159934502601622\n",
      "(WTA)Validation loss: 0.03527498617768288\n",
      "Epoch: 125000, WTA-Loss: 17.15986623030901\n",
      "(WTA)Validation loss: 0.034752484100560345\n",
      "Epoch: 126000, WTA-Loss: 17.159702459037305\n",
      "(WTA)Validation loss: 0.03471804317086935\n",
      "Epoch: 127000, WTA-Loss: 17.159593906521796\n",
      "(WTA)Validation loss: 0.03307884093374014\n",
      "Epoch: 128000, WTA-Loss: 17.15943816435337\n",
      "(WTA)Validation loss: 0.03520274752130111\n",
      "Epoch: 129000, WTA-Loss: 17.15942573326826\n",
      "(WTA)Validation loss: 0.03704028017818928\n",
      "Epoch: 130000, WTA-Loss: 17.159292292654513\n",
      "(WTA)Validation loss: 0.035133847345908485\n",
      "Epoch: 131000, WTA-Loss: 17.159163429677488\n",
      "(WTA)Validation loss: 0.03740410630901655\n",
      "Epoch: 132000, WTA-Loss: 17.159094287276268\n",
      "(WTA)Validation loss: 0.03678350461026033\n",
      "Epoch: 133000, WTA-Loss: 17.158982149362565\n",
      "(WTA)Validation loss: 0.035154325577119984\n",
      "Epoch: 134000, WTA-Loss: 17.15886033219099\n",
      "(WTA)Validation loss: 0.03740196923414866\n",
      "Epoch: 135000, WTA-Loss: 17.158725290119648\n",
      "(WTA)Validation loss: 0.035607051414748035\n",
      "Epoch: 136000, WTA-Loss: 17.158675360918046\n",
      "(WTA)Validation loss: 0.03343993859986464\n",
      "Epoch: 137000, WTA-Loss: 17.158509829342364\n",
      "(WTA)Validation loss: 0.03346858515093724\n",
      "Epoch: 138000, WTA-Loss: 17.1583790115118\n",
      "(WTA)Validation loss: 0.03551695588976145\n",
      "Epoch: 139000, WTA-Loss: 17.158282366156577\n",
      "(WTA)Validation loss: 0.03368827234953642\n",
      "Epoch: 140000, WTA-Loss: 17.158268573641777\n",
      "(WTA)Validation loss: 0.03572174906730652\n",
      "Epoch: 141000, WTA-Loss: 17.158117613315582\n",
      "(WTA)Validation loss: 0.03403336523721615\n",
      "Epoch: 142000, WTA-Loss: 17.158135976731778\n",
      "(WTA)Validation loss: 0.03757242610057195\n",
      "Epoch: 143000, WTA-Loss: 17.15789620101452\n",
      "(WTA)Validation loss: 0.035622408613562584\n",
      "Epoch: 144000, WTA-Loss: 17.157953266739845\n",
      "(WTA)Validation loss: 0.03587665967643261\n",
      "Epoch: 145000, WTA-Loss: 17.15780418294668\n",
      "(WTA)Validation loss: 0.035757746237019696\n",
      "Epoch: 146000, WTA-Loss: 17.157584245145323\n",
      "(WTA)Validation loss: 0.03567623719573021\n",
      "Epoch: 147000, WTA-Loss: 17.15758606827259\n",
      "(WTA)Validation loss: 0.035789416482051216\n",
      "Epoch: 148000, WTA-Loss: 17.157451582491397\n",
      "(WTA)Validation loss: 0.03379212785512209\n",
      "Epoch: 149000, WTA-Loss: 17.157446132838725\n",
      "(WTA)Validation loss: 0.03637028858065605\n",
      "Epoch: 150000, WTA-Loss: 17.15733565980196\n",
      "(WTA)Validation loss: 0.03604953673978647\n",
      "Epoch: 151000, WTA-Loss: 17.15725558048487\n",
      "(WTA)Validation loss: 0.0363224238778154\n",
      "Epoch: 152000, WTA-Loss: 17.15719505751133\n",
      "(WTA)Validation loss: 0.037861245994766556\n",
      "Epoch: 153000, WTA-Loss: 17.15706588816643\n",
      "(WTA)Validation loss: 0.036215963773429394\n",
      "Epoch: 154000, WTA-Loss: 17.157054211616515\n",
      "(WTA)Validation loss: 0.036338046503563724\n",
      "Epoch: 155000, WTA-Loss: 17.156949681520462\n",
      "(WTA)Validation loss: 0.03861448106666406\n",
      "Epoch: 156000, WTA-Loss: 17.16532325989008\n",
      "(WTA)Validation loss: 0.036280001824100815\n",
      "Epoch: 157000, WTA-Loss: 17.158733595132826\n",
      "(WTA)Validation loss: 0.03449594912429651\n",
      "Epoch: 158000, WTA-Loss: 17.156190202236175\n",
      "(WTA)Validation loss: 0.03689758976300558\n",
      "Epoch: 159000, WTA-Loss: 17.155868128418923\n",
      "(WTA)Validation loss: 0.03664874533812205\n",
      "Epoch: 160000, WTA-Loss: 17.155640399158003\n",
      "(WTA)Validation loss: 0.03651776692519585\n",
      "Epoch: 161000, WTA-Loss: 17.155466115951537\n",
      "(WTA)Validation loss: 0.03468923643231392\n",
      "Epoch: 162000, WTA-Loss: 17.155338729679585\n",
      "(WTA)Validation loss: 0.03887205012142658\n",
      "Epoch: 163000, WTA-Loss: 17.15523140156269\n",
      "(WTA)Validation loss: 0.03654994970808426\n",
      "Epoch: 164000, WTA-Loss: 17.155134147047995\n",
      "(WTA)Validation loss: 0.03482817392796278\n",
      "Epoch: 165000, WTA-Loss: 17.154929570734502\n",
      "(WTA)Validation loss: 0.03682057714710633\n",
      "Epoch: 166000, WTA-Loss: 17.154826264858247\n",
      "(WTA)Validation loss: 0.03688691618541876\n",
      "Epoch: 167000, WTA-Loss: 17.15476847141981\n",
      "(WTA)Validation loss: 0.0388988039145867\n",
      "Epoch: 168000, WTA-Loss: 17.154580174922945\n",
      "(WTA)Validation loss: 0.03512902340541283\n",
      "Epoch: 169000, WTA-Loss: 17.154589369118213\n",
      "(WTA)Validation loss: 0.03936824264625708\n",
      "Epoch: 170000, WTA-Loss: 17.15440751260519\n",
      "(WTA)Validation loss: 0.03701986465603113\n",
      "Epoch: 171000, WTA-Loss: 17.15436709201336\n",
      "(WTA)Validation loss: 0.03938870628674825\n",
      "Epoch: 172000, WTA-Loss: 17.154261870205403\n",
      "(WTA)Validation loss: 0.03852665858964125\n",
      "Epoch: 173000, WTA-Loss: 17.15417549175024\n",
      "(WTA)Validation loss: 0.038258075093229614\n",
      "Epoch: 174000, WTA-Loss: 17.154049554526807\n",
      "(WTA)Validation loss: 0.03826656316717466\n",
      "Epoch: 175000, WTA-Loss: 17.1539323464036\n",
      "(WTA)Validation loss: 0.03835419254998366\n",
      "Epoch: 176000, WTA-Loss: 17.153889643669128\n",
      "(WTA)Validation loss: 0.038727400824427605\n",
      "Epoch: 177000, WTA-Loss: 17.15381862425804\n",
      "(WTA)Validation loss: 0.038649254788955055\n",
      "Epoch: 178000, WTA-Loss: 17.15378508269787\n",
      "(WTA)Validation loss: 0.039270732551813126\n",
      "Epoch: 179000, WTA-Loss: 17.153635604143144\n",
      "(WTA)Validation loss: 0.038924974078933396\n",
      "Epoch: 180000, WTA-Loss: 17.153604877531528\n",
      "(WTA)Validation loss: 0.03936621422568957\n",
      "Epoch: 181000, WTA-Loss: 17.153430192053317\n",
      "(WTA)Validation loss: 0.03967153777678808\n",
      "Epoch: 182000, WTA-Loss: 17.15339022552967\n",
      "(WTA)Validation loss: 0.03883824062844118\n",
      "Epoch: 183000, WTA-Loss: 17.153408759534358\n",
      "(WTA)Validation loss: 0.03956685774028301\n",
      "Epoch: 184000, WTA-Loss: 17.153255066394806\n",
      "(WTA)Validation loss: 0.04052926662067572\n",
      "Epoch: 185000, WTA-Loss: 17.15318185532093\n",
      "(WTA)Validation loss: 0.039150202025969826\n",
      "Epoch: 186000, WTA-Loss: 17.15308810132742\n",
      "(WTA)Validation loss: 0.03725105430930853\n",
      "Epoch: 187000, WTA-Loss: 17.153078692257406\n",
      "(WTA)Validation loss: 0.03776813422640165\n",
      "Epoch: 188000, WTA-Loss: 17.152899714529514\n",
      "(WTA)Validation loss: 0.039145482083161674\n",
      "Epoch: 189000, WTA-Loss: 17.152870907425882\n",
      "(WTA)Validation loss: 0.037769899082680546\n",
      "Epoch: 190000, WTA-Loss: 17.152797325849534\n",
      "(WTA)Validation loss: 0.039954609548052154\n",
      "Epoch: 191000, WTA-Loss: 17.152810954391956\n",
      "(WTA)Validation loss: 0.04003259104986986\n",
      "Epoch: 192000, WTA-Loss: 17.152715839207172\n",
      "(WTA)Validation loss: 0.04089764878153801\n",
      "Epoch: 193000, WTA-Loss: 17.152554436683655\n",
      "(WTA)Validation loss: 0.04061012280484041\n",
      "Epoch: 194000, WTA-Loss: 17.152586153745652\n",
      "(WTA)Validation loss: 0.04014731508990129\n",
      "Epoch: 195000, WTA-Loss: 17.152507321953774\n",
      "(WTA)Validation loss: 0.039541990806659065\n",
      "Epoch: 196000, WTA-Loss: 17.152491959512233\n",
      "(WTA)Validation loss: 0.038841995100180306\n",
      "Epoch: 197000, WTA-Loss: 17.152420093536378\n",
      "(WTA)Validation loss: 0.03702477210511764\n",
      "Epoch: 198000, WTA-Loss: 17.15239497691393\n",
      "(WTA)Validation loss: 0.03977070872982343\n",
      "Epoch: 199000, WTA-Loss: 17.152306076705457\n",
      "(WTA)Validation loss: 0.04068211652338505\n",
      "Epoch: 200000, WTA-Loss: 17.15215491628647\n",
      "(WTA)Validation loss: 0.037859909857312836\n",
      "Epoch: 201000, WTA-Loss: 17.152156006157398\n",
      "(WTA)Validation loss: 0.038267938420176506\n",
      "Epoch: 202000, WTA-Loss: 17.152061357796192\n",
      "(WTA)Validation loss: 0.039231677850087486\n",
      "Epoch: 203000, WTA-Loss: 17.152161995828152\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m     loss_wta, wta_nll \u001b[38;5;241m=\u001b[39m model_wta\u001b[38;5;241m.\u001b[39mloss(pred_wta, gate_wta, tar_y_wta)\n\u001b[1;32m     56\u001b[0m     loss_wta\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 57\u001b[0m     \u001b[43moptimizer_wta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     epoch_loss_wta \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m wta_nll\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m training_loss_wta\u001b[38;5;241m.\u001b[39mappend(epoch_loss_wta)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/mocapact/{dy}D/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "# if not os.path.exists(f'{root_folder}img/'):\n",
    "#     os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 1_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_wta = 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_wta = 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_wta, validation_error_wta = [], []\n",
    "\n",
    "wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_wta = 0\n",
    "\n",
    "    # traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "    traj_ids, v_traj_ids = [], []\n",
    "    inds = torch.randperm(num_indiv)\n",
    "    vinds = torch.randperm(num_val)[:num_val_indiv]\n",
    "\n",
    "    for i in range(inds.shape[0]):\n",
    "        first = inds[i] * torch.randint(1,3,(1,1)).item()  # randint changes the order if it returns 2. for input randomization\n",
    "        second = num_demos-first-1\n",
    "        traj_ids.append([first, second])\n",
    "\n",
    "    for i in range(vinds.shape[0]):\n",
    "        v_traj_ids.append([vinds[i], num_val-vinds[i]-1])\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_wta.zero_grad()\n",
    "\n",
    "        obs_wta, tar_x_wta, tar_y_wta = get_batch(x, y, traj_ids[i], device)\n",
    "        pred_wta, gate_wta = model_wta(obs_wta, tar_x_wta)\n",
    "        loss_wta, wta_nll = model_wta.loss(pred_wta, gate_wta, tar_y_wta)\n",
    "        loss_wta.backward()\n",
    "        optimizer_wta.step()\n",
    "\n",
    "        epoch_loss_wta += wta_nll.item()\n",
    "\n",
    "    training_loss_wta.append(epoch_loss_wta)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            # v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss_wta = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_wta, g_wta = model_wta(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss_wta += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "            val_loss_wta /= v_epoch_iter\n",
    "            validation_error_wta.append(val_loss_wta)\n",
    "            print(f'(WTA)Validation loss: {val_loss_wta}')\n",
    "            if val_loss_wta < min_val_loss_wta and epoch > 1e5:\n",
    "                min_val_loss_wta = val_loss_wta\n",
    "                print(f'(WTA)New best: {min_val_loss_wta}')\n",
    "                torch.save(model_wta_.state_dict(), f'{root_folder}saved_models/wta_on_synth.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss_wta += epoch_loss_wta\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, WTA-Loss: {}\".format(epoch, avg_loss_wta/val_per_epoch))\n",
    "        avg_loss_wta = 0\n",
    "\n",
    "torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
