{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CMU_016_40.hdf5', 'CMU_007_12.hdf5']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "root = '/home/yigit/projects/mbcnp/data/raw/mocapact/'\n",
    "files = []\n",
    "\n",
    "# Iterate directory\n",
    "for file_path in os.listdir(root):\n",
    "    if file_path.endswith('.hdf5') and os.path.isfile(os.path.join(root, file_path)):\n",
    "        if 'CMU_016_40.hdf5' in file_path or 'CMU_007_12.hdf5' in file_path:\n",
    "            # add filename to list\n",
    "            files.append(file_path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_observables = ['actuator_activation', 'joints_pos', 'joints_vel', 'sensors_gyro', 'end_effectors_pos', \n",
    "                       'sensors_torque', 'sensors_touch', 'sensors_velocimeter']\n",
    "\n",
    "def get_obs_indices(path):\n",
    "    indices = []\n",
    "\n",
    "    f = h5py.File(path, 'r+')\n",
    "    walker_obs_dict = f['observable_indices']['walker']\n",
    "    for k in walker_obs_dict.keys():\n",
    "        if k in desired_observables:\n",
    "            dum = walker_obs_dict[k][:]\n",
    "            indices.extend(dum)\n",
    "    f.close()\n",
    "\n",
    "    return np.array(indices)\n",
    "\n",
    "# Get indices\n",
    "indices = get_obs_indices(os.path.join(root, 'CMU_016_40.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    }
   ],
   "source": [
    "#region read mocapact data\n",
    "full_obs, full_act = [], []\n",
    "\n",
    "for file in files:\n",
    "    fp = os.path.join(root, file)\n",
    "    # Open file\n",
    "    f = h5py.File(fp, 'r+')\n",
    "\n",
    "    demos = {}\n",
    "\n",
    "    num_start_rollouts = f['n_start_rollouts'][()]  # concatenate snippets to create this many rollouts\n",
    "    for i in range(num_start_rollouts):\n",
    "        demos.update({i: {}})\n",
    "        demos[i].update({'obs': {}})\n",
    "        demos[i].update({'act': {}})\n",
    "    \n",
    "    num_snippets = 0\n",
    "    for key in f.keys():\n",
    "        if key.startswith('CMU_'):\n",
    "            num_snippets += 1\n",
    "\n",
    "    for key in f.keys():\n",
    "        if key.startswith('CMU_'):\n",
    "            start, end = int(key.split('-')[-2]), int(key.split('-')[-1])\n",
    "            for i in range(num_start_rollouts):\n",
    "                obs = np.array(f[key][str(i)]['observations']['proprioceptive'])\n",
    "                act = np.array(f[key][str(i)]['actions'])\n",
    "                for j in range(len(act)):\n",
    "                    demos[i]['obs'].update({start+j: obs[j, indices]})\n",
    "                    demos[i]['act'].update({start+j: act[j]})\n",
    "\n",
    "    for key in f.keys():\n",
    "        for i in range(num_start_rollouts):\n",
    "            if key.startswith('CMU_') and f[key]['early_termination'][i] == True:\n",
    "                if i in demos.keys():\n",
    "                    demos.pop(i)\n",
    "\n",
    "    for key in demos.keys():\n",
    "        full_obs.append(np.array(list(demos[key]['obs'].values())))\n",
    "        full_act.append(np.array(list(demos[key]['act'].values())))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "print(len(full_obs), len(full_act))\n",
    "#endregion\n",
    "min_length = 1000\n",
    "for i in range(len(full_obs)):\n",
    "    if len(full_obs[i]) < min_length:\n",
    "        min_length = len(full_obs[i])\n",
    "\n",
    "processed_obs, processed_act = [], []\n",
    "for i in range(len(full_obs)):\n",
    "    processed_obs.append(full_obs[i][np.linspace(0, len(full_obs[i])-1, min_length, dtype=int)])\n",
    "    processed_act.append(full_act[i][np.linspace(0, len(full_obs[i])-1, min_length, dtype=int)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.wta_cnp import WTA_CNP\n",
    "import torch\n",
    "\n",
    "def get_available_gpu_with_most_memory():\n",
    "    gpu_memory = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch to the GPU to accurately measure memory\n",
    "        gpu_memory.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "\n",
    "    gpu_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return gpu_memory[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_available_gpu_with_most_memory()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)\n",
    "\n",
    "###\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "n_max_obs, n_max_tar = 5, 5\n",
    "\n",
    "t_steps = min_length\n",
    "num_val = 4\n",
    "num_demos = len(full_obs)-num_val\n",
    "num_classes = 2\n",
    "num_indiv = num_demos//num_classes  # number of demos per class\n",
    "\n",
    "dx, dy = 1, len(full_act[0][0])\n",
    "\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['tomato', 'aqua']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([16, 45, 1]) Y: torch.Size([16, 45, 56]) VX: torch.Size([4, 45, 1]) VY: torch.Size([4, 45, 56])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(num_demos, t_steps, dx, device=device)\n",
    "y = torch.zeros(num_demos, t_steps, dy, device=device)\n",
    "vx = torch.zeros(num_val, t_steps, dx, device=device)\n",
    "vy = torch.zeros(num_val, t_steps, dy, device=device)\n",
    "\n",
    "ind = torch.randperm(len(full_obs))\n",
    "vind = torch.cat((torch.randint(0, num_indiv, (num_val_indiv, 1)), torch.randint(num_indiv, num_demos, (num_val_indiv, 1))), dim=0)\n",
    "tr_ctr, val_ctr = 0, 0\n",
    "\n",
    "for i in range(len(full_obs)):\n",
    "    if i in vind:\n",
    "        # vx[val_ctr] = torch.tensor(processed_obs[i], dtype=torch.float32)\n",
    "        vx[val_ctr] = torch.linspace(0, 1, t_steps, dtype=torch.float32).view_as(vx[val_ctr])\n",
    "        vy[val_ctr] = torch.tensor(processed_act[i], dtype=torch.float32)\n",
    "        val_ctr += 1\n",
    "    else:\n",
    "        # x[tr_ctr] = torch.tensor(processed_obs[i], dtype=torch.float32)\n",
    "        x[tr_ctr] = torch.linspace(0, 1, t_steps, dtype=torch.float32).view_as(x[tr_ctr])\n",
    "        y[tr_ctr] = torch.tensor(processed_act[i], dtype=torch.float32)\n",
    "        tr_ctr += 1\n",
    "\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wta_ = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [1024, 1024, 1024], num_decoders=2, decoder_hidden_dims=[512, 512, 512], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer_wta = torch.optim.Adam(lr=1e-4, params=model_wta_.parameters())\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model_wta = torch.compile(model_wta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WTA)New best: 0.7975544929504395\n",
      "Epoch: 0, WTA-Loss: 0.008055461645126343\n",
      "(WTA)New best: 0.061571910977363586\n",
      "Epoch: 1000, WTA-Loss: -3.4550922681065277\n",
      "(WTA)New best: 0.05285011604428291\n",
      "Epoch: 2000, WTA-Loss: -6.927817085117102\n",
      "(WTA)New best: 0.05199756287038326\n",
      "Epoch: 3000, WTA-Loss: -8.845476897746325\n",
      "Epoch: 4000, WTA-Loss: -10.397426395207644\n",
      "Epoch: 5000, WTA-Loss: -11.668039237648248\n",
      "Epoch: 6000, WTA-Loss: -12.753860163927078\n",
      "Epoch: 7000, WTA-Loss: -13.696185856878758\n",
      "Epoch: 8000, WTA-Loss: -14.404736883461476\n",
      "Epoch: 9000, WTA-Loss: -15.055532764852048\n",
      "Epoch: 10000, WTA-Loss: -15.657686552882195\n",
      "Epoch: 11000, WTA-Loss: -16.117637781977653\n",
      "Epoch: 12000, WTA-Loss: -16.529323966920376\n",
      "Epoch: 13000, WTA-Loss: -16.91634295386076\n",
      "Epoch: 14000, WTA-Loss: -17.29427516967058\n",
      "Epoch: 15000, WTA-Loss: -17.576423897475003\n",
      "Epoch: 16000, WTA-Loss: -17.88244921001792\n",
      "Epoch: 17000, WTA-Loss: -18.067207051753996\n",
      "Epoch: 18000, WTA-Loss: -18.36862338787317\n",
      "Epoch: 19000, WTA-Loss: -18.576259548425675\n",
      "Epoch: 20000, WTA-Loss: -18.752542068362235\n",
      "Epoch: 21000, WTA-Loss: -18.910876978874207\n",
      "Epoch: 22000, WTA-Loss: -19.209436400413512\n",
      "Epoch: 23000, WTA-Loss: -19.296861251711846\n",
      "Epoch: 24000, WTA-Loss: -19.430435297369957\n",
      "Epoch: 25000, WTA-Loss: -19.565261769384147\n",
      "Epoch: 26000, WTA-Loss: -19.723084061861037\n",
      "Epoch: 27000, WTA-Loss: -19.81894050526619\n",
      "Epoch: 28000, WTA-Loss: -19.95388967782259\n",
      "Epoch: 29000, WTA-Loss: -20.053563211798668\n",
      "Epoch: 30000, WTA-Loss: -20.17739215731621\n",
      "Epoch: 31000, WTA-Loss: -20.293904533147813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_iter):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     optimizer_wta\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     obs_wta, tar_x_wta, tar_y_wta \u001b[39m=\u001b[39m get_batch(x, y, traj_ids[i], device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     pred_wta, gate_wta \u001b[39m=\u001b[39m model_wta(obs_wta, tar_x_wta)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     loss_wta, wta_nll \u001b[39m=\u001b[39m model_wta\u001b[39m.\u001b[39mloss(pred_wta, gate_wta, tar_y_wta)\n",
      "\u001b[1;32m/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m o_ids \u001b[39m=\u001b[39m random_query_ids[:n_o]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m t_ids \u001b[39m=\u001b[39m random_query_ids[n_o:n_o\u001b[39m+\u001b[39mn_t]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m obs[i, :, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tar[i, :, :] \u001b[39m=\u001b[39m x[traj_ids[i], t_ids]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m tar_val[i, :, :] \u001b[39m=\u001b[39m y[traj_ids[i], t_ids]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/experimental/{dy}D/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "# if not os.path.exists(f'{root_folder}img/'):\n",
    "#     os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 5_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_wta = 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_wta = 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_wta, validation_error_wta = [], []\n",
    "\n",
    "wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_wta = 0\n",
    "\n",
    "    # traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "    traj_ids, v_traj_ids = [], []\n",
    "    inds = torch.randperm(num_indiv)\n",
    "    vinds = torch.randperm(num_val_indiv)\n",
    "    for i in inds:\n",
    "        traj_ids.append([inds[i], num_demos-inds[i]-1])\n",
    "\n",
    "    for i in vinds:\n",
    "        v_traj_ids.append([vinds[i], num_val-vinds[i]-1])\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_wta.zero_grad()\n",
    "\n",
    "        obs_wta, tar_x_wta, tar_y_wta = get_batch(x, y, traj_ids[i], device)\n",
    "        pred_wta, gate_wta = model_wta(obs_wta, tar_x_wta)\n",
    "        loss_wta, wta_nll = model_wta.loss(pred_wta, gate_wta, tar_y_wta)\n",
    "        loss_wta.backward()\n",
    "        optimizer_wta.step()\n",
    "\n",
    "        epoch_loss_wta += wta_nll.item()\n",
    "\n",
    "    training_loss_wta.append(epoch_loss_wta)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            # v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss_wta = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_wta, g_wta = model_wta(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss_wta += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "            validation_error_wta.append(val_loss_wta)\n",
    "            if val_loss_wta < min_val_loss_wta:\n",
    "                min_val_loss_wta = val_loss_wta\n",
    "                print(f'(WTA)New best: {min_val_loss_wta}')\n",
    "                torch.save(model_wta_.state_dict(), f'{root_folder}saved_models/wta_on_synth.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss_wta += epoch_loss_wta\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, WTA-Loss: {}\".format(epoch, avg_loss_wta/val_per_epoch))\n",
    "        avg_loss_wta = 0\n",
    "\n",
    "torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WTA_CNP(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=261, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=1229, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=112, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gate): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.wta_cnp import WTA_CNP\n",
    "\n",
    "root_folder = f'outputs/experimental/56D/1701871167/'\n",
    "wta_model_path = f'{root_folder}saved_models/wta_on_synth.pt'\n",
    "\n",
    "y = torch.load(f'{root_folder}y.pt').cpu()\n",
    "num_samples, t_steps, dy = y.shape\n",
    "dx = 205\n",
    "batch_size = 1\n",
    "n_max_obs, n_max_tar = 6, 6\n",
    "\n",
    "wta = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [1024, 1024, 1024], num_decoders=2, decoder_hidden_dims=[512, 512, 512], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "\n",
    "wta.load_state_dict(torch.load(wta_model_path))\n",
    "wta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dm_control import viewer\n",
    "from dm_control import composer\n",
    "from dm_control.locomotion import arenas\n",
    "from dm_control.locomotion.tasks import go_to_target\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "from dm_control_wrapper import StandInitializer\n",
    "from dm_control.composer import ObservationPadding\n",
    "\n",
    "seed = 0\n",
    "dind = 13\n",
    "\n",
    "def prepare_obs(obs, ind, a):\n",
    "    vv = []\n",
    "    for k in obs.keys():\n",
    "        real_key = k.split('/')[1]\n",
    "        if real_key in desired_observables:\n",
    "            vals = obs[k].flatten()\n",
    "            vv.extend([vals])\n",
    "    if ind == 0:\n",
    "        vv.extend([y[dind, ind].numpy()])\n",
    "    else:\n",
    "        vv.extend([a])\n",
    "    v = np.concatenate(vv).reshape(-1)\n",
    "    return torch.from_numpy(v).view(1, 1, dx+dy).float().to(device)\n",
    "\n",
    "\n",
    "def prepare_tar(ind):\n",
    "    return x[dind, ind].view(1, 1, dx).float().to(device)\n",
    "\n",
    "initializer = StandInitializer()\n",
    "walker = cmu_humanoid.CMUHumanoidPositionControlledV2020(initializer=initializer)\n",
    "\n",
    "# Build an empty arena.\n",
    "arena = arenas.Floor()\n",
    "\n",
    "# Build a task that rewards the agent for tracking motion capture reference\n",
    "# data.\n",
    "task = go_to_target.GoToTarget(walker=walker, arena=arena, physics_timestep=0.005, control_timestep=0.03)\n",
    "env = composer.Environment(task=task, random_state=seed)\n",
    "# print(env.control_timestep())\n",
    "\n",
    "ind = -1\n",
    "inst_a = None\n",
    "\n",
    "def tst(ts):\n",
    "    global ind, inst_a\n",
    "    ind += 1\n",
    "    # dm_obs, dm_tar = prepare_obs(ts.observation, ind, inst_a), prepare_tar(ind+1)\n",
    "    # p_wta, g_wta = wta(dm_obs, dm_tar)\n",
    "    # # print(g_wta.squeeze(1))\n",
    "    # inst_a = p_wta[torch.argmax(g_wta.squeeze(1), dim=-1), 0, 0, :dy].cpu().detach().numpy().squeeze()\n",
    "    # return inst_a\n",
    "    return y[dind, ind].numpy()\n",
    "    # return full_act[dind][ind]\n",
    "\n",
    "# Viewer for visualization\n",
    "viewer.launch(env, policy=tst)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dm_obs = prepare_obs(env.reset()[3], 0)\n",
    "# dm_tar = prepare_tar(1)\n",
    "\n",
    "# for i in range(1, t_steps):\n",
    "#     # p_wta, g_wta = wta(dm_obs, dm_tar)\n",
    "#     # a = p_wta[torch.argmax(g_wta.squeeze(1), dim=-1), 0, 0, :dy].cpu().detach().numpy().squeeze()\n",
    "#     # print(a)\n",
    "#     a = y[0, i].numpy()\n",
    "#     s = env.step(a)\n",
    "#     # dm_obs, dm_tar = prepare_obs(s.observation, i-1), prepare_tar(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuator_activation <class 'numpy.ndarray'>\n",
      "appendages_pos <class 'numpy.ndarray'>\n",
      "body_height <class 'numpy.ndarray'>\n",
      "end_effectors_pos <class 'numpy.ndarray'>\n",
      "gyro_anticlockwise_spin <class 'numpy.ndarray'>\n",
      "gyro_backward_roll <class 'numpy.ndarray'>\n",
      "gyro_control <class 'numpy.ndarray'>\n",
      "gyro_rightward_roll <class 'numpy.ndarray'>\n",
      "head_height <class 'numpy.ndarray'>\n",
      "joints_pos <class 'numpy.ndarray'>\n",
      "joints_vel <class 'numpy.ndarray'>\n",
      "joints_vel_control <class 'numpy.ndarray'>\n",
      "orientation <class 'numpy.ndarray'>\n",
      "position <class 'numpy.ndarray'>\n",
      "sensors_accelerometer <class 'numpy.ndarray'>\n",
      "sensors_gyro <class 'numpy.ndarray'>\n",
      "sensors_torque <class 'numpy.ndarray'>\n",
      "sensors_touch <class 'numpy.ndarray'>\n",
      "sensors_velocimeter <class 'numpy.ndarray'>\n",
      "time_in_clip <class 'numpy.ndarray'>\n",
      "torso_xvel <class 'numpy.ndarray'>\n",
      "torso_yvel <class 'numpy.ndarray'>\n",
      "veloc_forward <class 'numpy.ndarray'>\n",
      "veloc_strafe <class 'numpy.ndarray'>\n",
      "veloc_up <class 'numpy.ndarray'>\n",
      "velocimeter_control <class 'numpy.ndarray'>\n",
      "world_zaxis <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for k in f['observable_indices']['walker']:\n",
    "    if 'reference' not in k:\n",
    "        print(k, type(f['observable_indices']['walker'][k][()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuator_activation (1, 56)\n",
      "appendages_pos (1, 15)\n",
      "body_height (1,)\n",
      "end_effectors_pos (1, 12)\n",
      "joints_pos (1, 56)\n",
      "joints_vel (1, 56)\n",
      "sensors_accelerometer (1, 3)\n",
      "sensors_force (1, 0)\n",
      "sensors_gyro (1, 3)\n",
      "sensors_torque (1, 6)\n",
      "sensors_touch (1, 10)\n",
      "sensors_velocimeter (1, 3)\n",
      "world_zaxis (1, 3)\n",
      "target (1, 3)\n"
     ]
    }
   ],
   "source": [
    "for k in s[3].keys():\n",
    "    print(k.replace('/', '.').split('.')[-1], s[3][k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([1, 781, 1]) Y: torch.Size([1, 781, 62]) VX: torch.Size([1, 781, 1]) VY: torch.Size([1, 781, 62])\n"
     ]
    }
   ],
   "source": [
    "# def transform_data(data):\n",
    "#     num_dimensions = data.shape[2]\n",
    "\n",
    "#     transformation_matrix = torch.zeros((num_dimensions, 2))\n",
    "#     transformed_data = torch.zeros_like(data)\n",
    "\n",
    "#     # Apply transformations to each dimension\n",
    "#     for i in range(num_dimensions):\n",
    "#         dim_data = data[:, :, i]\n",
    "\n",
    "#         min_val = dim_data.min()\n",
    "#         max_val = dim_data.max()\n",
    "\n",
    "#         transformation_matrix[i, 0] = min_val\n",
    "#         transformation_matrix[i, 1] = max_val\n",
    "\n",
    "#         interval = max_val - min_val\n",
    "#         if interval < 1e-6:\n",
    "#             interval = 1\n",
    "\n",
    "#         transformed_dim = 2 * (dim_data - min_val) / interval - 1\n",
    "#         transformed_data[:, :, i] = transformed_dim\n",
    "\n",
    "#     return transformed_data, transformation_matrix\n",
    "\n",
    "# def reconstruct_data(transformed_data, transformation_matrix):\n",
    "#     num_dimensions = transformed_data.shape[2]\n",
    "\n",
    "#     reconstructed_data = torch.zeros_like(transformed_data)\n",
    "\n",
    "#     for i in range(num_dimensions):\n",
    "#         transformed_dim = transformed_data[:, :, i]\n",
    "#         min_val, max_val = transformation_matrix[i, 0], transformation_matrix[i, 1]\n",
    "\n",
    "#         reconstructed_dim = ((transformed_dim + 1) / 2) * (max_val - min_val) + min_val\n",
    "#         reconstructed_data[:, :, i] = reconstructed_dim\n",
    "\n",
    "#     return reconstructed_data\n",
    "\n",
    "# y = data.clone().to(device)\n",
    "# x = torch.unsqueeze(torch.linspace(0, 1, t_steps).repeat(num_demos, 1), -1).to(device)\n",
    "\n",
    "# vx = x.clone()\n",
    "# noise = torch.clamp(torch.randn(x.shape)*1e-4**0.5, min=0).to(device)\n",
    "# vy = y.clone() + noise\n",
    "\n",
    "# print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
