{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CMU_049_06.hdf5', 'CMU_049_07.hdf5']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "root = '/home/yigit/projects/mbcnp/data/raw/mocapact/'\n",
    "files = []\n",
    "\n",
    "# Iterate directory\n",
    "for file_path in os.listdir(root):\n",
    "    if file_path.endswith('.hdf5') and os.path.isfile(os.path.join(root, file_path)):\n",
    "        # add filename to list\n",
    "        files.append(file_path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_obs_indices(path):\n",
    "    desired_observables = ['actuator_activation', 'appendages_pos', 'body_height', 'end_effectors_pos', 'joints_pos', 'joints_vel',\n",
    "                           'sensors_accelerometer', 'sensors_gyro', 'sensors_torque', 'sensors_touch', 'sensors_velocimeter', 'world_zaxis']\n",
    "    indices = []\n",
    "\n",
    "    f = h5py.File(path, 'r+')\n",
    "    walker_obs_dict = f['observable_indices']['walker']\n",
    "    for k in walker_obs_dict.keys():\n",
    "        if k in desired_observables:\n",
    "            indices.extend(walker_obs_dict[k][:])\n",
    "    f.close()\n",
    "\n",
    "    return np.array(indices)\n",
    "\n",
    "# Get indices\n",
    "indices = get_obs_indices(os.path.join(root, 'CMU_049_06.hdf5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region read mocapact data\n",
    "full_obs, full_act = [], []\n",
    "\n",
    "for file in files:\n",
    "    fp = os.path.join(root, file)\n",
    "    # Open file\n",
    "    f = h5py.File(fp, 'r+')\n",
    "\n",
    "    demos = {}\n",
    "\n",
    "    num_start_rollouts = f['n_start_rollouts'][()]  # concatenate snippets to create this many rollouts\n",
    "    for i in range(num_start_rollouts):\n",
    "        demos.update({i: {}})\n",
    "        demos[i].update({'obs': {}})\n",
    "        demos[i].update({'act': {}})\n",
    "    \n",
    "    num_snippets = 0\n",
    "    for key in f.keys():\n",
    "        if key.startswith('CMU_'):\n",
    "            num_snippets += 1\n",
    "\n",
    "    for key in f.keys():\n",
    "        if key.startswith('CMU_'):\n",
    "            start, end = int(key.split('-')[-2]), int(key.split('-')[-1])\n",
    "            for i in range(num_start_rollouts):\n",
    "                obs = np.array(f[key][str(i)]['observations']['proprioceptive'])\n",
    "                act = np.array(f[key][str(i)]['actions'])\n",
    "                for j in range(len(act)):\n",
    "                    demos[i]['obs'].update({start+j: obs[j, indices]})\n",
    "                    demos[i]['act'].update({start+j: act[j]})\n",
    "\n",
    "    for key in f.keys():\n",
    "        for i in range(num_start_rollouts):\n",
    "            if key.startswith('CMU_') and f[key]['early_termination'][i] == True:\n",
    "                if i in demos.keys():\n",
    "                    demos.pop(i)\n",
    "\n",
    "    for key in demos.keys():\n",
    "        full_obs.append(np.array(list(demos[key]['obs'].values())))\n",
    "        full_act.append(np.array(list(demos[key]['act'].values())))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "# print(len(full_obs), len(full_act))\n",
    "#endregion\n",
    "min_length = 1000\n",
    "for i in range(len(full_obs)):\n",
    "    if len(full_obs[i]) < min_length:\n",
    "        min_length = len(full_obs[i])\n",
    "\n",
    "processed_obs, processed_act = [], []\n",
    "for i in range(len(full_obs)):\n",
    "    processed_obs.append(full_obs[i][np.linspace(0, len(full_obs[i])-1, min_length, dtype=int)])\n",
    "    processed_act.append(full_act[i][np.linspace(0, len(full_obs[i])-1, min_length, dtype=int)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.wta_cnp import WTA_CNP\n",
    "import torch\n",
    "\n",
    "def get_available_gpu_with_most_memory():\n",
    "    gpu_memory = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch to the GPU to accurately measure memory\n",
    "        gpu_memory.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "\n",
    "    gpu_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return gpu_memory[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_available_gpu_with_most_memory()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)\n",
    "\n",
    "###\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_max_obs, n_max_tar = 10, 10\n",
    "\n",
    "t_steps = min_length\n",
    "num_val = 2\n",
    "num_demos = len(full_obs)-num_val\n",
    "num_classes = 1\n",
    "num_indiv = num_demos//num_classes  # number of demos per class\n",
    "\n",
    "dx, dy = len(indices), len(full_act[0][0])\n",
    "\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['tomato', 'aqua']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([18, 122, 224]) Y: torch.Size([18, 122, 56]) VX: torch.Size([2, 122, 224]) VY: torch.Size([2, 122, 56])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(num_demos, t_steps, dx, device=device)\n",
    "y = torch.zeros(num_demos, t_steps, dy, device=device)\n",
    "vx = torch.zeros(num_val, t_steps, dx, device=device)\n",
    "vy = torch.zeros(num_val, t_steps, dy, device=device)\n",
    "\n",
    "ind = torch.randperm(len(full_obs))\n",
    "\n",
    "for i in range(len(ind)):\n",
    "    if i < num_demos:\n",
    "        x[i] = torch.tensor(processed_obs[ind[i]], dtype=torch.float32)\n",
    "        y[i] = torch.tensor(processed_act[ind[i]], dtype=torch.float32)\n",
    "    else:\n",
    "        vx[i-num_demos] = torch.tensor(processed_obs[ind[i]], dtype=torch.float32)\n",
    "        vy[i-num_demos] = torch.tensor(processed_act[ind[i]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wta_ = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [1024, 1024, 1024], num_decoders=1, decoder_hidden_dims=[1024, 1024, 1024], batch_size=batch_size).to(device)\n",
    "optimizer_wta = torch.optim.Adam(lr=1e-4, params=model_wta_.parameters())\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model_wta = torch.compile(model_wta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m pred_wta, gate_wta \u001b[39m=\u001b[39m model_wta(obs_wta, tar_x_wta)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss_wta, wta_nll \u001b[39m=\u001b[39m model_wta\u001b[39m.\u001b[39mloss(pred_wta, gate_wta, tar_y_wta)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss_wta\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X23sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m optimizer_wta\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yigit/projects/mbcnp/train_exp_online_cnep.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m epoch_loss_wta \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m wta_nll\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:89\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/experimental/{dy}D/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "if not os.path.exists(f'{root_folder}img/'):\n",
    "    os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 10_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_wta = 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_wta = 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_wta, validation_error_wta = [], []\n",
    "\n",
    "wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_wta = 0\n",
    "\n",
    "    traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_wta.zero_grad()\n",
    "\n",
    "        obs_wta, tar_x_wta, tar_y_wta = get_batch(x, y, traj_ids[i], device)\n",
    "        pred_wta, gate_wta = model_wta(obs_wta, tar_x_wta)\n",
    "        loss_wta, wta_nll = model_wta.loss(pred_wta, gate_wta, tar_y_wta)\n",
    "        loss_wta.backward()\n",
    "        optimizer_wta.step()\n",
    "\n",
    "        epoch_loss_wta += wta_nll.item()\n",
    "\n",
    "    training_loss_wta.append(epoch_loss_wta)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss_wta = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_wta, g_wta = model_wta(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss_wta += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "            validation_error_wta.append(val_loss_wta)\n",
    "            if val_loss_wta < min_val_loss_wta:\n",
    "                min_val_loss_wta = val_loss_wta\n",
    "                print(f'(WTA)New best: {min_val_loss_wta}')\n",
    "                torch.save(model_wta_.state_dict(), f'{root_folder}saved_models/wta_on_synth.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss_wta += epoch_loss_wta\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, WTA-Loss: {}\".format(epoch, avg_loss_wta/val_per_epoch))\n",
    "        avg_loss_wta = 0\n",
    "\n",
    "torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control.locomotion.mocap import cmu_mocap_data\n",
    "from dm_control.locomotion.mocap import loader\n",
    "from dm_control.locomotion.tasks.reference_pose import tracking\n",
    "from dm_control.locomotion.tasks.reference_pose import utils\n",
    "from dm_control.locomotion.walkers import initializers\n",
    "import numpy as np\n",
    "import tree\n",
    "import mujoco\n",
    "\n",
    "class StandInitializer(initializers.WalkerInitializer):\n",
    "    def __init__(self):\n",
    "        ref_path = cmu_mocap_data.get_path_for_cmu(version='2020')\n",
    "        mocap_loader = loader.HDF5TrajectoryLoader(ref_path)\n",
    "        trajectory = mocap_loader.get_trajectory('CMU_049_06')\n",
    "        clip_reference_features = trajectory.as_dict()\n",
    "        clip_reference_features = tracking._strip_reference_prefix(clip_reference_features, 'walker/')\n",
    "        self._stand_features = tree.map_structure(lambda x: x[0], clip_reference_features)\n",
    "\n",
    "    def initialize_pose(self, physics, walker, random_state):\n",
    "        del random_state\n",
    "        utils.set_walker_from_features(physics, walker, self._stand_features)\n",
    "        mujoco.mj_kinematics(physics.model.ptr, physics.data.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dm_control import viewer\n",
    "from dm_control import composer\n",
    "from dm_control.locomotion import arenas\n",
    "from dm_control.locomotion.tasks import go_to_target\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "\n",
    "\n",
    "def prepare_obs(obs):\n",
    "    values_list = [v.flatten() for v in obs][:-1]  # exclude world_zaxis\n",
    "    v = np.concatenate(values_list)\n",
    "    return torch.from_numpy(v).view(1, 1, dx).float().to(device)\n",
    "\n",
    "initializer = StandInitializer()\n",
    "walker = cmu_humanoid.CMUHumanoidPositionControlledV2020(initializer=initializer)\n",
    "\n",
    "# Build an empty arena.\n",
    "arena = arenas.Floor()\n",
    "\n",
    "# Build a task that rewards the agent for tracking motion capture reference\n",
    "# data.\n",
    "task = go_to_target.GoToTarget(walker=walker, arena=arena)\n",
    "env = composer.Environment(task=task, random_state=None)\n",
    "\n",
    "# Viewer for visualization\n",
    "viewer.launch(env)\n",
    "dm_obs = prepare_obs(env.reset()[3].values())\n",
    "# print(te[3])\n",
    "\n",
    "for _ in range(200):\n",
    "    p_wta, g_wta = model_wta(obs, t_wta)\n",
    "    a = np.random.uniform(env.action_spec().minimum, env.action_spec().maximum)\n",
    "    s = env.step(a)\n",
    "    # print(s)\n",
    "    # print(s[3].shape)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuator_activation <class 'numpy.ndarray'>\n",
      "appendages_pos <class 'numpy.ndarray'>\n",
      "body_height <class 'numpy.ndarray'>\n",
      "end_effectors_pos <class 'numpy.ndarray'>\n",
      "gyro_anticlockwise_spin <class 'numpy.ndarray'>\n",
      "gyro_backward_roll <class 'numpy.ndarray'>\n",
      "gyro_control <class 'numpy.ndarray'>\n",
      "gyro_rightward_roll <class 'numpy.ndarray'>\n",
      "head_height <class 'numpy.ndarray'>\n",
      "joints_pos <class 'numpy.ndarray'>\n",
      "joints_vel <class 'numpy.ndarray'>\n",
      "joints_vel_control <class 'numpy.ndarray'>\n",
      "orientation <class 'numpy.ndarray'>\n",
      "position <class 'numpy.ndarray'>\n",
      "sensors_accelerometer <class 'numpy.ndarray'>\n",
      "sensors_gyro <class 'numpy.ndarray'>\n",
      "sensors_torque <class 'numpy.ndarray'>\n",
      "sensors_touch <class 'numpy.ndarray'>\n",
      "sensors_velocimeter <class 'numpy.ndarray'>\n",
      "time_in_clip <class 'numpy.ndarray'>\n",
      "torso_xvel <class 'numpy.ndarray'>\n",
      "torso_yvel <class 'numpy.ndarray'>\n",
      "veloc_forward <class 'numpy.ndarray'>\n",
      "veloc_strafe <class 'numpy.ndarray'>\n",
      "veloc_up <class 'numpy.ndarray'>\n",
      "velocimeter_control <class 'numpy.ndarray'>\n",
      "world_zaxis <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for k in f['observable_indices']['walker']:\n",
    "    if 'reference' not in k:\n",
    "        print(k, type(f['observable_indices']['walker'][k][()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuator_activation (1, 56)\n",
      "appendages_pos (1, 15)\n",
      "body_height (1,)\n",
      "end_effectors_pos (1, 12)\n",
      "joints_pos (1, 56)\n",
      "joints_vel (1, 56)\n",
      "sensors_accelerometer (1, 3)\n",
      "sensors_force (1, 0)\n",
      "sensors_gyro (1, 3)\n",
      "sensors_torque (1, 6)\n",
      "sensors_touch (1, 10)\n",
      "sensors_velocimeter (1, 3)\n",
      "world_zaxis (1, 3)\n",
      "target (1, 3)\n"
     ]
    }
   ],
   "source": [
    "for k in s[3].keys():\n",
    "    print(k.replace('/', '.').split('.')[-1], s[3][k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([1, 781, 1]) Y: torch.Size([1, 781, 62]) VX: torch.Size([1, 781, 1]) VY: torch.Size([1, 781, 62])\n"
     ]
    }
   ],
   "source": [
    "# def transform_data(data):\n",
    "#     num_dimensions = data.shape[2]\n",
    "\n",
    "#     transformation_matrix = torch.zeros((num_dimensions, 2))\n",
    "#     transformed_data = torch.zeros_like(data)\n",
    "\n",
    "#     # Apply transformations to each dimension\n",
    "#     for i in range(num_dimensions):\n",
    "#         dim_data = data[:, :, i]\n",
    "\n",
    "#         min_val = dim_data.min()\n",
    "#         max_val = dim_data.max()\n",
    "\n",
    "#         transformation_matrix[i, 0] = min_val\n",
    "#         transformation_matrix[i, 1] = max_val\n",
    "\n",
    "#         interval = max_val - min_val\n",
    "#         if interval < 1e-6:\n",
    "#             interval = 1\n",
    "\n",
    "#         transformed_dim = 2 * (dim_data - min_val) / interval - 1\n",
    "#         transformed_data[:, :, i] = transformed_dim\n",
    "\n",
    "#     return transformed_data, transformation_matrix\n",
    "\n",
    "# def reconstruct_data(transformed_data, transformation_matrix):\n",
    "#     num_dimensions = transformed_data.shape[2]\n",
    "\n",
    "#     reconstructed_data = torch.zeros_like(transformed_data)\n",
    "\n",
    "#     for i in range(num_dimensions):\n",
    "#         transformed_dim = transformed_data[:, :, i]\n",
    "#         min_val, max_val = transformation_matrix[i, 0], transformation_matrix[i, 1]\n",
    "\n",
    "#         reconstructed_dim = ((transformed_dim + 1) / 2) * (max_val - min_val) + min_val\n",
    "#         reconstructed_data[:, :, i] = reconstructed_dim\n",
    "\n",
    "#     return reconstructed_data\n",
    "\n",
    "# y = data.clone().to(device)\n",
    "# x = torch.unsqueeze(torch.linspace(0, 1, t_steps).repeat(num_demos, 1), -1).to(device)\n",
    "\n",
    "# vx = x.clone()\n",
    "# noise = torch.clamp(torch.randn(x.shape)*1e-4**0.5, min=0).to(device)\n",
    "# vy = y.clone() + noise\n",
    "\n",
    "# print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
