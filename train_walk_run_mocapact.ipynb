{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.wta_cnp import WTA_CNP\n",
    "import torch\n",
    "\n",
    "def get_available_gpu_with_most_memory():\n",
    "    gpu_memory = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch to the GPU to accurately measure memory\n",
    "        gpu_memory.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "\n",
    "    gpu_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return gpu_memory[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_available_gpu_with_most_memory()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)\n",
    "\n",
    "###\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load trajectories\n",
    "walk_heavy_actions, walk_heavy_observations = np.load(\"data/mocapact/awh.npy\"), np.load(\"data/mocapact/owh.npy\")\n",
    "run_circle_actions, run_circle_observations = np.load(\"data/mocapact/arc.npy\"), np.load(\"data/mocapact/orc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "n_max_obs, n_max_tar = 10, 10\n",
    "\n",
    "num_indiv, t_steps, dx = walk_heavy_observations.shape\n",
    "_, _, dy = walk_heavy_actions.shape\n",
    "num_val = 2\n",
    "num_classes = 2\n",
    "num_demos = num_indiv*num_classes - num_val\n",
    "\n",
    "num_val_indiv = num_val//num_classes\n",
    "\n",
    "colors = ['tomato', 'aqua']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([4, 208, 287]) Y: torch.Size([4, 208, 56]) VX: torch.Size([2, 208, 287]) VY: torch.Size([2, 208, 56])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(num_demos, t_steps, dx, device=device)\n",
    "y = torch.zeros(num_demos, t_steps, dy, device=device)\n",
    "vx = torch.zeros(num_val, t_steps, dx, device=device)\n",
    "vy = torch.zeros(num_val, t_steps, dy, device=device)\n",
    "\n",
    "vind = torch.randint(0, num_indiv, (num_val_indiv, 1))\n",
    "tr_ctr, val_ctr = 0, 0\n",
    "\n",
    "vx[0] = torch.from_numpy(walk_heavy_observations[vind]).to(device)\n",
    "vx[1] = torch.from_numpy(run_circle_observations[vind]).to(device)\n",
    "vy[0] = torch.from_numpy(walk_heavy_actions[vind]).to(device)\n",
    "vy[1] = torch.from_numpy(run_circle_actions[vind]).to(device)\n",
    "\n",
    "for i in range(num_indiv*num_classes):\n",
    "    if i == vind or i == vind + num_indiv:\n",
    "       pass\n",
    "    else:\n",
    "        if i<num_indiv:\n",
    "            x[tr_ctr] = torch.from_numpy(walk_heavy_observations[i]).to(device)\n",
    "            y[tr_ctr] = torch.from_numpy(walk_heavy_actions[i]).to(device)\n",
    "        else:\n",
    "            x[tr_ctr] = torch.from_numpy(run_circle_observations[i-num_indiv]).to(device)\n",
    "            y[tr_ctr] = torch.from_numpy(run_circle_actions[i-num_indiv]).to(device)\n",
    "        tr_ctr += 1\n",
    "\n",
    "print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, traj_ids, device=device):\n",
    "    n_o = torch.randint(1, n_max_obs, (1,)).item()\n",
    "    n_t = torch.randint(1, n_max_tar, (1,)).item()\n",
    "    \n",
    "    tar = torch.zeros(batch_size, n_t, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, n_t, dy, device=device)\n",
    "    obs = torch.zeros(batch_size, n_o, dx+dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        \n",
    "        o_ids = random_query_ids[:n_o]\n",
    "        t_ids = random_query_ids[n_o:n_o+n_t]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((x[traj_ids[i], o_ids], y[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = x[traj_ids[i], t_ids]\n",
    "        tar_val[i, :, :] = y[traj_ids[i], t_ids]\n",
    "\n",
    "    return obs, tar, tar_val\n",
    "\n",
    "def get_validation_batch(vx, vy, traj_ids, device=device):\n",
    "    num_obs = torch.randint(1, n_max_obs, (1,)).item()\n",
    "\n",
    "    obs = torch.zeros(batch_size, num_obs, dx+dy, device=device)\n",
    "    tar = torch.zeros(batch_size, t_steps, dx, device=device)\n",
    "    tar_val = torch.zeros(batch_size, t_steps, dy, device=device)\n",
    "\n",
    "    for i in range(len(traj_ids)):\n",
    "        random_query_ids = torch.randperm(t_steps)\n",
    "        o_ids = random_query_ids[:num_obs]\n",
    "\n",
    "        obs[i, :, :] = torch.cat((vx[traj_ids[i], o_ids], vy[traj_ids[i], o_ids]), dim=-1)\n",
    "        tar[i, :, :] = vx[traj_ids[i]]\n",
    "        tar_val[i, :, :] = vy[traj_ids[i]]\n",
    "\n",
    "    return obs, tar, tar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wta_ = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [1024, 1024, 1024], num_decoders=2, decoder_hidden_dims=[512, 512, 512], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "optimizer_wta = torch.optim.Adam(lr=1e-4, params=model_wta_.parameters())\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model_wta = torch.compile(model_wta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WTA)New best: 0.40390509366989136\n",
      "Epoch: 0, WTA-Loss: 0.0019881385564804077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-22 04:22:01,068] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/yigit/projects/mbcnp/models/wta_cnp.py:64)\n",
      "   reasons:  tensor 'obs' strides mismatch at index 0. expected 1372, actual 2744\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WTA)New best: 0.04118404909968376\n",
      "Epoch: 1000, WTA-Loss: -0.7760251429779456\n",
      "(WTA)New best: 0.038486842066049576\n",
      "Epoch: 2000, WTA-Loss: -2.1332125391364096\n",
      "(WTA)New best: 0.036706820130348206\n",
      "Epoch: 3000, WTA-Loss: -3.046498068004847\n",
      "(WTA)New best: 0.03643212839961052\n",
      "Epoch: 4000, WTA-Loss: -3.621864306330681\n",
      "(WTA)New best: 0.036047786474227905\n",
      "Epoch: 5000, WTA-Loss: -3.9571953736543657\n",
      "(WTA)New best: 0.0359199084341526\n",
      "Epoch: 6000, WTA-Loss: -4.134189720928669\n",
      "Epoch: 7000, WTA-Loss: -4.272029125809669\n",
      "Epoch: 8000, WTA-Loss: -4.4192090091109275\n",
      "(WTA)New best: 0.03587695583701134\n",
      "Epoch: 9000, WTA-Loss: -4.548209491431713\n",
      "Epoch: 10000, WTA-Loss: -4.610962930679321\n",
      "Epoch: 11000, WTA-Loss: -4.6441003808379175\n",
      "Epoch: 12000, WTA-Loss: -4.7123397097587585\n",
      "Epoch: 13000, WTA-Loss: -4.854018047392368\n",
      "Epoch: 14000, WTA-Loss: -4.8868198543190955\n",
      "Epoch: 15000, WTA-Loss: -4.905523990392685\n",
      "Epoch: 16000, WTA-Loss: -4.966825802803039\n",
      "Epoch: 17000, WTA-Loss: -5.014663355410099\n",
      "Epoch: 18000, WTA-Loss: -5.029362983286381\n",
      "Epoch: 19000, WTA-Loss: -5.079644201397896\n",
      "Epoch: 20000, WTA-Loss: -5.061398789763451\n",
      "Epoch: 21000, WTA-Loss: -5.145518361568451\n",
      "Epoch: 22000, WTA-Loss: -5.209980042219162\n",
      "Epoch: 23000, WTA-Loss: -5.183312182307243\n",
      "Epoch: 24000, WTA-Loss: -5.261225992679596\n",
      "Epoch: 25000, WTA-Loss: -5.299341060519218\n",
      "Epoch: 26000, WTA-Loss: -5.259642667889595\n",
      "Epoch: 27000, WTA-Loss: -5.258734107792377\n",
      "Epoch: 28000, WTA-Loss: -5.336138137459755\n",
      "Epoch: 29000, WTA-Loss: -5.388048367917538\n",
      "Epoch: 30000, WTA-Loss: -5.3096597193479536\n",
      "Epoch: 31000, WTA-Loss: -5.35658214366436\n",
      "Epoch: 32000, WTA-Loss: -5.362238652646542\n",
      "Epoch: 33000, WTA-Loss: -5.401727041840553\n",
      "Epoch: 34000, WTA-Loss: -5.395888173282146\n",
      "Epoch: 35000, WTA-Loss: -5.433341857731342\n",
      "Epoch: 36000, WTA-Loss: -5.481559466719627\n",
      "Epoch: 37000, WTA-Loss: -5.518054791808129\n",
      "Epoch: 38000, WTA-Loss: -5.40646737831831\n",
      "Epoch: 39000, WTA-Loss: -5.478842576026916\n",
      "Epoch: 40000, WTA-Loss: -5.486602227091789\n",
      "Epoch: 41000, WTA-Loss: -5.520693253397941\n",
      "Epoch: 42000, WTA-Loss: -5.566456811189651\n",
      "Epoch: 43000, WTA-Loss: -5.596375388085842\n",
      "Epoch: 44000, WTA-Loss: -5.606007756114006\n",
      "Epoch: 45000, WTA-Loss: -5.616933043122292\n",
      "Epoch: 46000, WTA-Loss: -5.612058908820153\n",
      "Epoch: 47000, WTA-Loss: -5.594703233599663\n",
      "Epoch: 48000, WTA-Loss: -5.629121234953404\n",
      "Epoch: 49000, WTA-Loss: -5.59235127311945\n",
      "Epoch: 50000, WTA-Loss: -5.661662047684192\n",
      "Epoch: 51000, WTA-Loss: -5.6343882945179935\n",
      "Epoch: 52000, WTA-Loss: -5.633266254842281\n",
      "Epoch: 53000, WTA-Loss: -5.619230072140693\n",
      "Epoch: 54000, WTA-Loss: -5.703396764338017\n",
      "Epoch: 55000, WTA-Loss: -5.690761619627476\n",
      "Epoch: 56000, WTA-Loss: -5.699229720115661\n",
      "Epoch: 57000, WTA-Loss: -5.672039447128773\n",
      "Epoch: 58000, WTA-Loss: -5.709076241314412\n",
      "Epoch: 59000, WTA-Loss: -5.670953517854214\n",
      "Epoch: 60000, WTA-Loss: -5.7355072356462475\n",
      "Epoch: 61000, WTA-Loss: -5.690305641770363\n",
      "Epoch: 62000, WTA-Loss: -5.7044531894922255\n",
      "Epoch: 63000, WTA-Loss: -5.757954357326031\n",
      "Epoch: 64000, WTA-Loss: -5.759445780038834\n",
      "Epoch: 65000, WTA-Loss: -5.738196240007877\n",
      "Epoch: 66000, WTA-Loss: -5.775088044762612\n",
      "Epoch: 67000, WTA-Loss: -5.603993304073811\n",
      "Epoch: 68000, WTA-Loss: -5.820867109835148\n",
      "Epoch: 69000, WTA-Loss: -5.852741276323795\n",
      "Epoch: 70000, WTA-Loss: -5.819899076759815\n",
      "Epoch: 71000, WTA-Loss: -5.8259548572897915\n",
      "Epoch: 72000, WTA-Loss: -5.742075185894966\n",
      "Epoch: 73000, WTA-Loss: -5.838652182102203\n",
      "Epoch: 74000, WTA-Loss: -5.794616155683994\n",
      "Epoch: 75000, WTA-Loss: -5.8010760370492935\n",
      "Epoch: 76000, WTA-Loss: -5.817696179509163\n",
      "Epoch: 77000, WTA-Loss: -5.770963680267334\n",
      "Epoch: 78000, WTA-Loss: -5.887082607150078\n",
      "Epoch: 79000, WTA-Loss: -5.758439883291722\n",
      "Epoch: 80000, WTA-Loss: -5.808070512831211\n",
      "Epoch: 81000, WTA-Loss: -5.703036578714848\n",
      "Epoch: 82000, WTA-Loss: -5.792631316125393\n",
      "Epoch: 83000, WTA-Loss: -5.796052995741367\n",
      "Epoch: 84000, WTA-Loss: -5.823222723484039\n",
      "Epoch: 85000, WTA-Loss: -5.764514543414116\n",
      "Epoch: 86000, WTA-Loss: -5.851826463580132\n",
      "Epoch: 87000, WTA-Loss: -5.820083554923534\n",
      "Epoch: 88000, WTA-Loss: -5.805916434884072\n",
      "Epoch: 89000, WTA-Loss: -5.753870861649514\n",
      "Epoch: 90000, WTA-Loss: -5.834417957544327\n",
      "Epoch: 91000, WTA-Loss: -5.811460337162018\n",
      "Epoch: 92000, WTA-Loss: -5.8290768436193465\n",
      "Epoch: 93000, WTA-Loss: -5.811445638477802\n",
      "Epoch: 94000, WTA-Loss: -5.870798437148332\n",
      "Epoch: 95000, WTA-Loss: -5.8596353271007535\n",
      "Epoch: 96000, WTA-Loss: -5.766600369691849\n",
      "Epoch: 97000, WTA-Loss: -5.893109676599503\n",
      "Epoch: 98000, WTA-Loss: -5.808913054823876\n",
      "Epoch: 99000, WTA-Loss: -5.855263362467289\n",
      "Epoch: 100000, WTA-Loss: -5.795039767384529\n",
      "Epoch: 101000, WTA-Loss: -5.7922819660902025\n",
      "Epoch: 102000, WTA-Loss: -5.767102760434151\n",
      "Epoch: 103000, WTA-Loss: -5.883601759493351\n",
      "Epoch: 104000, WTA-Loss: -5.8420434923768045\n",
      "Epoch: 105000, WTA-Loss: -5.875938634097576\n",
      "Epoch: 106000, WTA-Loss: -5.7255726518034935\n",
      "Epoch: 107000, WTA-Loss: -5.784084223389626\n",
      "Epoch: 108000, WTA-Loss: -5.817053061306477\n",
      "Epoch: 109000, WTA-Loss: -5.82687944561243\n",
      "Epoch: 110000, WTA-Loss: -5.801196727335453\n",
      "Epoch: 111000, WTA-Loss: -5.80612036806345\n",
      "Epoch: 112000, WTA-Loss: -5.7480316200256345\n",
      "Epoch: 113000, WTA-Loss: -5.825519739985466\n",
      "Epoch: 114000, WTA-Loss: -5.803897314369679\n",
      "Epoch: 115000, WTA-Loss: -5.88648759123683\n",
      "Epoch: 116000, WTA-Loss: -5.796528814554215\n",
      "Epoch: 117000, WTA-Loss: -5.740834156095982\n",
      "Epoch: 118000, WTA-Loss: -5.724946939110756\n",
      "Epoch: 119000, WTA-Loss: -5.809352404952049\n",
      "Epoch: 120000, WTA-Loss: -5.7908300598859785\n",
      "Epoch: 121000, WTA-Loss: -5.82304473567009\n",
      "Epoch: 122000, WTA-Loss: -5.802332568228245\n",
      "Epoch: 123000, WTA-Loss: -5.747674026489258\n",
      "Epoch: 124000, WTA-Loss: -5.797732029736042\n",
      "Epoch: 125000, WTA-Loss: -5.844028353810311\n",
      "Epoch: 126000, WTA-Loss: -5.700146362900734\n",
      "Epoch: 127000, WTA-Loss: -5.789636241018772\n",
      "Epoch: 128000, WTA-Loss: -5.620723319351673\n",
      "Epoch: 129000, WTA-Loss: -5.757725955426693\n",
      "Epoch: 130000, WTA-Loss: -5.65158516317606\n",
      "Epoch: 131000, WTA-Loss: -5.657824532806873\n",
      "Epoch: 132000, WTA-Loss: -5.676338193774224\n",
      "Epoch: 133000, WTA-Loss: -5.665266071677208\n",
      "Epoch: 134000, WTA-Loss: -5.737084469854832\n",
      "Epoch: 135000, WTA-Loss: -5.73025785446167\n",
      "Epoch: 136000, WTA-Loss: -5.781528023958206\n",
      "Epoch: 137000, WTA-Loss: -5.682640182435512\n",
      "Epoch: 138000, WTA-Loss: -5.586466435551643\n",
      "Epoch: 139000, WTA-Loss: -5.717337563872337\n",
      "Epoch: 140000, WTA-Loss: -5.657859034016728\n",
      "Epoch: 141000, WTA-Loss: -5.618316327095032\n",
      "Epoch: 142000, WTA-Loss: -5.622207718193531\n",
      "Epoch: 143000, WTA-Loss: -5.650853132486343\n",
      "Epoch: 144000, WTA-Loss: -5.677136688292027\n",
      "Epoch: 145000, WTA-Loss: -5.647093467026949\n",
      "Epoch: 146000, WTA-Loss: -5.459426750779152\n",
      "Epoch: 147000, WTA-Loss: -5.517158929869533\n",
      "Epoch: 148000, WTA-Loss: -5.561667395353317\n",
      "Epoch: 149000, WTA-Loss: -5.538224369287491\n",
      "Epoch: 150000, WTA-Loss: -5.475154394209385\n",
      "Epoch: 151000, WTA-Loss: -5.5343469235301015\n",
      "Epoch: 152000, WTA-Loss: -5.456786067068577\n",
      "Epoch: 153000, WTA-Loss: -5.482190018475055\n",
      "Epoch: 154000, WTA-Loss: -5.392972363471985\n",
      "Epoch: 155000, WTA-Loss: -5.549976729631424\n",
      "Epoch: 156000, WTA-Loss: -5.4716684273779395\n",
      "Epoch: 157000, WTA-Loss: -5.547709532767534\n",
      "Epoch: 158000, WTA-Loss: -5.444604756653309\n",
      "Epoch: 159000, WTA-Loss: -5.522860093712807\n",
      "Epoch: 160000, WTA-Loss: -5.39116368675232\n",
      "Epoch: 161000, WTA-Loss: -5.480243086218834\n",
      "Epoch: 162000, WTA-Loss: -5.365039128601551\n",
      "Epoch: 163000, WTA-Loss: -5.280381634056568\n",
      "Epoch: 164000, WTA-Loss: -5.346720214962959\n",
      "Epoch: 165000, WTA-Loss: -5.294141160726547\n",
      "Epoch: 166000, WTA-Loss: -5.198476500988007\n",
      "Epoch: 167000, WTA-Loss: -5.299486282601952\n",
      "Epoch: 168000, WTA-Loss: -5.260242040276528\n",
      "Epoch: 169000, WTA-Loss: -5.216589719712735\n",
      "Epoch: 170000, WTA-Loss: -5.1699511094093324\n",
      "Epoch: 171000, WTA-Loss: -5.19265741866827\n",
      "Epoch: 172000, WTA-Loss: -5.193612586915493\n",
      "Epoch: 173000, WTA-Loss: -5.17867453867197\n",
      "Epoch: 174000, WTA-Loss: -5.058047846436501\n",
      "Epoch: 175000, WTA-Loss: -5.107585547745228\n",
      "Epoch: 176000, WTA-Loss: -5.084296259403229\n",
      "Epoch: 177000, WTA-Loss: -4.964422315180301\n",
      "Epoch: 178000, WTA-Loss: -5.142300184726715\n",
      "Epoch: 179000, WTA-Loss: -4.994354828953743\n",
      "Epoch: 180000, WTA-Loss: -5.020453928053379\n",
      "Epoch: 181000, WTA-Loss: -5.059196239173413\n",
      "Epoch: 182000, WTA-Loss: -4.902781906992197\n",
      "Epoch: 183000, WTA-Loss: -4.698980110913515\n",
      "Epoch: 184000, WTA-Loss: -4.817654690682888\n",
      "Epoch: 185000, WTA-Loss: -4.645088568240404\n",
      "Epoch: 186000, WTA-Loss: -4.8217174705863\n",
      "Epoch: 187000, WTA-Loss: -4.842118792772293\n",
      "Epoch: 188000, WTA-Loss: -4.78963301551342\n",
      "Epoch: 189000, WTA-Loss: -4.775613855659961\n",
      "Epoch: 190000, WTA-Loss: -4.779105217576027\n",
      "Epoch: 191000, WTA-Loss: -4.629903059929609\n",
      "Epoch: 192000, WTA-Loss: -4.76169142973423\n",
      "Epoch: 193000, WTA-Loss: -4.827536840975284\n",
      "Epoch: 194000, WTA-Loss: -4.754154904901982\n",
      "Epoch: 195000, WTA-Loss: -4.639180096656084\n",
      "Epoch: 196000, WTA-Loss: -4.767708827018738\n",
      "Epoch: 197000, WTA-Loss: -4.8872386522889135\n",
      "Epoch: 198000, WTA-Loss: -4.666136206269265\n",
      "Epoch: 199000, WTA-Loss: -4.8111073340773585\n",
      "Epoch: 200000, WTA-Loss: -4.791956038832664\n",
      "Epoch: 201000, WTA-Loss: -4.716801633477211\n",
      "Epoch: 202000, WTA-Loss: -4.877295944333077\n",
      "Epoch: 203000, WTA-Loss: -4.801313855975867\n",
      "Epoch: 204000, WTA-Loss: -4.787560514450073\n",
      "Epoch: 205000, WTA-Loss: -4.564672221958637\n",
      "Epoch: 206000, WTA-Loss: -4.306853320717812\n",
      "Epoch: 207000, WTA-Loss: -4.524341774404049\n",
      "Epoch: 208000, WTA-Loss: -4.480890219658614\n",
      "Epoch: 209000, WTA-Loss: -4.596042110830545\n",
      "Epoch: 210000, WTA-Loss: -4.4330055979788305\n",
      "Epoch: 211000, WTA-Loss: -4.314598768830299\n",
      "Epoch: 212000, WTA-Loss: -4.585747332304716\n",
      "Epoch: 213000, WTA-Loss: -4.545143482863903\n",
      "Epoch: 214000, WTA-Loss: -4.591613515138626\n",
      "Epoch: 215000, WTA-Loss: -4.5781043578386305\n",
      "Epoch: 216000, WTA-Loss: -4.474968232780695\n",
      "Epoch: 217000, WTA-Loss: -1.2449468657970428\n",
      "Epoch: 218000, WTA-Loss: -3.97538075581193\n",
      "Epoch: 219000, WTA-Loss: -4.3662940792441365\n",
      "Epoch: 220000, WTA-Loss: -4.566665004849434\n",
      "Epoch: 221000, WTA-Loss: -4.466559866219759\n",
      "Epoch: 222000, WTA-Loss: -4.308601149976253\n",
      "Epoch: 223000, WTA-Loss: -4.250718987971545\n",
      "Epoch: 224000, WTA-Loss: -4.445527285456658\n",
      "Epoch: 225000, WTA-Loss: -4.056146905064582\n",
      "Epoch: 226000, WTA-Loss: -4.314529910326004\n",
      "Epoch: 227000, WTA-Loss: -4.163179603636265\n",
      "Epoch: 228000, WTA-Loss: -4.251303110934794\n",
      "Epoch: 229000, WTA-Loss: -4.222062221854925\n",
      "Epoch: 230000, WTA-Loss: -4.211475833982229\n",
      "Epoch: 231000, WTA-Loss: -4.491127854466439\n",
      "Epoch: 232000, WTA-Loss: -4.346937448099256\n",
      "Epoch: 233000, WTA-Loss: 4009.6910837016553\n",
      "Epoch: 234000, WTA-Loss: -2.1077086627595127\n",
      "Epoch: 235000, WTA-Loss: -2.2553523725382982\n",
      "Epoch: 236000, WTA-Loss: -2.446301380261779\n",
      "Epoch: 237000, WTA-Loss: -2.4737292303144933\n",
      "Epoch: 238000, WTA-Loss: -2.533432713240385\n",
      "Epoch: 239000, WTA-Loss: -2.568382008895278\n",
      "Epoch: 240000, WTA-Loss: -2.677874315775931\n",
      "Epoch: 241000, WTA-Loss: -2.7738075565323235\n",
      "Epoch: 242000, WTA-Loss: -2.7458444294966755\n",
      "Epoch: 243000, WTA-Loss: -2.792276716388762\n",
      "Epoch: 244000, WTA-Loss: -2.7883272811993955\n",
      "Epoch: 245000, WTA-Loss: -2.6680372444093226\n",
      "Epoch: 246000, WTA-Loss: -2.833980355411768\n",
      "Epoch: 247000, WTA-Loss: -2.8462522964179517\n",
      "Epoch: 248000, WTA-Loss: -2.7618750974684954\n",
      "Epoch: 249000, WTA-Loss: -2.729262534663081\n",
      "Epoch: 250000, WTA-Loss: -2.9002493932023645\n",
      "Epoch: 251000, WTA-Loss: -2.8741992333978414\n",
      "Epoch: 252000, WTA-Loss: -2.9578377766907216\n",
      "Epoch: 253000, WTA-Loss: -2.8689695862829687\n",
      "Epoch: 254000, WTA-Loss: -3.0150703991502525\n",
      "Epoch: 255000, WTA-Loss: -2.9228090024888513\n",
      "Epoch: 256000, WTA-Loss: -3.023528563171625\n",
      "Epoch: 257000, WTA-Loss: -2.870796520516276\n",
      "Epoch: 258000, WTA-Loss: -3.0060127703249453\n",
      "Epoch: 259000, WTA-Loss: -3.072581264704466\n",
      "Epoch: 260000, WTA-Loss: -3.1051804398596285\n",
      "Epoch: 261000, WTA-Loss: -3.2417172894775867\n",
      "Epoch: 262000, WTA-Loss: -2.9408779054135086\n",
      "Epoch: 263000, WTA-Loss: -3.1269446488320827\n",
      "Epoch: 264000, WTA-Loss: -3.0205255664289\n",
      "Epoch: 265000, WTA-Loss: -3.1293572817891837\n",
      "Epoch: 266000, WTA-Loss: -3.070245732039213\n",
      "Epoch: 267000, WTA-Loss: -3.245162409067154\n",
      "Epoch: 268000, WTA-Loss: -2.9767585421204568\n",
      "Epoch: 269000, WTA-Loss: -3.095675628423691\n",
      "Epoch: 270000, WTA-Loss: -3.2439211897253992\n",
      "Epoch: 271000, WTA-Loss: -3.3004797773361205\n",
      "Epoch: 272000, WTA-Loss: -3.376645719885826\n",
      "Epoch: 273000, WTA-Loss: -3.3444955599308015\n",
      "Epoch: 274000, WTA-Loss: -3.2341949377954005\n",
      "Epoch: 275000, WTA-Loss: -3.2362675592303276\n",
      "Epoch: 276000, WTA-Loss: -3.27628174123168\n",
      "Epoch: 277000, WTA-Loss: -3.1819657778143884\n",
      "Epoch: 278000, WTA-Loss: -3.191954234749079\n",
      "Epoch: 279000, WTA-Loss: -3.0723570132255555\n",
      "Epoch: 280000, WTA-Loss: -2.6975379292666912\n",
      "Epoch: 281000, WTA-Loss: -3.031472921490669\n",
      "Epoch: 282000, WTA-Loss: -3.2827585528194905\n",
      "Epoch: 283000, WTA-Loss: -3.084357260644436\n",
      "Epoch: 284000, WTA-Loss: -3.011387044519186\n",
      "Epoch: 285000, WTA-Loss: -3.049111219137907\n",
      "Epoch: 286000, WTA-Loss: -2.8546516641378403\n",
      "Epoch: 287000, WTA-Loss: -3.039701508462429\n",
      "Epoch: 288000, WTA-Loss: -2.7068965522646904\n",
      "Epoch: 289000, WTA-Loss: -2.9130783530771733\n",
      "Epoch: 290000, WTA-Loss: -3.087275705009699\n",
      "Epoch: 291000, WTA-Loss: -2.9936214688122273\n",
      "Epoch: 292000, WTA-Loss: -3.064037615299225\n",
      "Epoch: 293000, WTA-Loss: -3.048182749092579\n",
      "Epoch: 294000, WTA-Loss: -2.793097492605448\n",
      "Epoch: 295000, WTA-Loss: -3.1900419352203606\n",
      "Epoch: 296000, WTA-Loss: -3.09731421366334\n",
      "Epoch: 297000, WTA-Loss: -3.2666069306135177\n",
      "Epoch: 298000, WTA-Loss: -3.146693004786968\n",
      "Epoch: 299000, WTA-Loss: -3.214357727319002\n",
      "Epoch: 300000, WTA-Loss: -3.0977399191856385\n",
      "Epoch: 301000, WTA-Loss: -2.9420946972072124\n",
      "Epoch: 302000, WTA-Loss: -2.9949712315797807\n",
      "Epoch: 303000, WTA-Loss: -2.8452580730617045\n",
      "Epoch: 304000, WTA-Loss: -2.4499126576818524\n",
      "Epoch: 305000, WTA-Loss: -2.8902454906105994\n",
      "Epoch: 306000, WTA-Loss: -3.0504544486403464\n",
      "Epoch: 307000, WTA-Loss: -2.930610119253397\n",
      "Epoch: 308000, WTA-Loss: -2.318899381816387\n",
      "Epoch: 309000, WTA-Loss: -2.330860846325755\n",
      "Epoch: 310000, WTA-Loss: -3.0068151377141477\n",
      "Epoch: 311000, WTA-Loss: -2.2462050981689243\n",
      "Epoch: 312000, WTA-Loss: -2.199116518855095\n",
      "Epoch: 313000, WTA-Loss: -1.7012970692813396\n",
      "Epoch: 314000, WTA-Loss: -2.0427640097364783\n",
      "Epoch: 315000, WTA-Loss: -2.464074261367321\n",
      "Epoch: 316000, WTA-Loss: -2.586095509022474\n",
      "Epoch: 317000, WTA-Loss: -1.8060321478545667\n",
      "Epoch: 318000, WTA-Loss: -2.6295350318551063\n",
      "Epoch: 319000, WTA-Loss: -3.057886883944273\n",
      "Epoch: 320000, WTA-Loss: -3.1618268340826035\n",
      "Epoch: 321000, WTA-Loss: -3.2524534473717215\n",
      "Epoch: 322000, WTA-Loss: -3.246562338769436\n",
      "Epoch: 323000, WTA-Loss: -3.580901691496372\n",
      "Epoch: 324000, WTA-Loss: -2.9165867096185685\n",
      "Epoch: 325000, WTA-Loss: -2.984251754641533\n",
      "Epoch: 326000, WTA-Loss: -3.7254366194009783\n",
      "Epoch: 327000, WTA-Loss: 639486.9014530105\n",
      "Epoch: 328000, WTA-Loss: -1.3555662052277475\n",
      "Epoch: 329000, WTA-Loss: -1.9408454493694007\n",
      "Epoch: 330000, WTA-Loss: -2.9781909951865675\n",
      "Epoch: 331000, WTA-Loss: -2.6714530333578588\n",
      "Epoch: 332000, WTA-Loss: -3.481532542794943\n",
      "Epoch: 333000, WTA-Loss: -3.214785708233714\n",
      "Epoch: 334000, WTA-Loss: -3.7792290158867834\n",
      "Epoch: 335000, WTA-Loss: -3.8785758444666865\n",
      "Epoch: 336000, WTA-Loss: -4.1691194845438\n",
      "Epoch: 337000, WTA-Loss: -4.092664240837097\n",
      "Epoch: 338000, WTA-Loss: -4.256558237850666\n",
      "Epoch: 339000, WTA-Loss: -4.152031118452549\n",
      "Epoch: 340000, WTA-Loss: -4.273009746313095\n",
      "Epoch: 341000, WTA-Loss: -4.0502374625206\n",
      "Epoch: 342000, WTA-Loss: -4.413506872057915\n",
      "Epoch: 343000, WTA-Loss: -4.316394568026066\n",
      "Epoch: 344000, WTA-Loss: -4.497177596330642\n",
      "Epoch: 345000, WTA-Loss: -4.438004795014859\n",
      "Epoch: 346000, WTA-Loss: -4.074649504765868\n",
      "Epoch: 347000, WTA-Loss: -4.265137264072895\n",
      "Epoch: 348000, WTA-Loss: -4.319843927651644\n",
      "Epoch: 349000, WTA-Loss: -4.297877312660217\n",
      "Epoch: 350000, WTA-Loss: -4.438320796966552\n",
      "Epoch: 351000, WTA-Loss: -4.414975014925003\n",
      "Epoch: 352000, WTA-Loss: -4.485746066451073\n",
      "Epoch: 353000, WTA-Loss: -2.7325719055086375\n",
      "Epoch: 354000, WTA-Loss: -2.5111630125679074\n",
      "Epoch: 355000, WTA-Loss: -2.6230199827849865\n",
      "Epoch: 356000, WTA-Loss: -2.662609291881323\n",
      "Epoch: 357000, WTA-Loss: -2.7686872689723967\n",
      "Epoch: 358000, WTA-Loss: 170.0778715315163\n",
      "Epoch: 359000, WTA-Loss: -2.6287253621220588\n",
      "Epoch: 360000, WTA-Loss: -2.645371976852417\n",
      "Epoch: 361000, WTA-Loss: -2.743840042874217\n",
      "Epoch: 362000, WTA-Loss: -2.7606138288229705\n",
      "Epoch: 363000, WTA-Loss: -2.8933956845998763\n",
      "Epoch: 364000, WTA-Loss: -2.8436161253899335\n",
      "Epoch: 365000, WTA-Loss: -2.7670167887955905\n",
      "Epoch: 366000, WTA-Loss: -2.9277891515791414\n",
      "Epoch: 367000, WTA-Loss: -2.9505054400265216\n",
      "Epoch: 368000, WTA-Loss: -2.880909597799182\n",
      "Epoch: 369000, WTA-Loss: -2.9091697128117087\n",
      "Epoch: 370000, WTA-Loss: -2.883878185465932\n",
      "Epoch: 371000, WTA-Loss: -2.9264775488972665\n",
      "Epoch: 372000, WTA-Loss: -2.897392321050167\n",
      "Epoch: 373000, WTA-Loss: -3.068752935439348\n",
      "Epoch: 374000, WTA-Loss: -3.097390413939953\n",
      "Epoch: 375000, WTA-Loss: -3.105837872594595\n",
      "Epoch: 376000, WTA-Loss: -0.18941173705458642\n",
      "Epoch: 377000, WTA-Loss: -2.926392072290182\n",
      "Epoch: 378000, WTA-Loss: -3.033321325331926\n",
      "Epoch: 379000, WTA-Loss: -3.131519504010677\n",
      "Epoch: 380000, WTA-Loss: -3.126135213240981\n",
      "Epoch: 381000, WTA-Loss: -3.074538838163018\n",
      "Epoch: 382000, WTA-Loss: -3.171812715888023\n",
      "Epoch: 383000, WTA-Loss: -3.1946106027811765\n",
      "Epoch: 384000, WTA-Loss: -2.5639870281666517\n",
      "Epoch: 385000, WTA-Loss: -3.146360702008009\n",
      "Epoch: 386000, WTA-Loss: -3.396689472436905\n",
      "Epoch: 387000, WTA-Loss: -3.3858682471215724\n",
      "Epoch: 388000, WTA-Loss: -2.93183357898891\n",
      "Epoch: 389000, WTA-Loss: -3.1674294989407064\n",
      "Epoch: 390000, WTA-Loss: -3.226383998528123\n",
      "Epoch: 391000, WTA-Loss: -3.2811268204301594\n",
      "Epoch: 392000, WTA-Loss: -0.7568326042294502\n",
      "Epoch: 393000, WTA-Loss: -3.079337020367384\n",
      "Epoch: 394000, WTA-Loss: -3.130353674799204\n",
      "Epoch: 395000, WTA-Loss: -3.1421713330745695\n",
      "Epoch: 396000, WTA-Loss: -3.2466435950398447\n",
      "Epoch: 397000, WTA-Loss: -3.3211904864311217\n",
      "Epoch: 398000, WTA-Loss: -2.9709947952628135\n",
      "Epoch: 399000, WTA-Loss: -3.2269305643737316\n",
      "Epoch: 400000, WTA-Loss: -3.0250533918738367\n",
      "Epoch: 401000, WTA-Loss: -3.280144784748554\n",
      "Epoch: 402000, WTA-Loss: -3.2755815713107586\n",
      "Epoch: 403000, WTA-Loss: -3.2013358579277993\n",
      "Epoch: 404000, WTA-Loss: -3.2522935605049135\n",
      "Epoch: 405000, WTA-Loss: -3.325836600661278\n",
      "Epoch: 406000, WTA-Loss: -3.2820426326692105\n",
      "Epoch: 407000, WTA-Loss: -3.407548273921013\n",
      "Epoch: 408000, WTA-Loss: -3.366912498831749\n",
      "Epoch: 409000, WTA-Loss: -3.040462141036987\n",
      "Epoch: 410000, WTA-Loss: -3.3803988971710206\n",
      "Epoch: 411000, WTA-Loss: -3.5263668698072435\n",
      "Epoch: 412000, WTA-Loss: -3.6639779354929924\n",
      "Epoch: 413000, WTA-Loss: -3.193719554632902\n",
      "Epoch: 414000, WTA-Loss: -3.277426180720329\n",
      "Epoch: 415000, WTA-Loss: -3.370848072126508\n",
      "Epoch: 416000, WTA-Loss: -3.364460925847292\n",
      "Epoch: 417000, WTA-Loss: -3.390998656272888\n",
      "Epoch: 418000, WTA-Loss: -3.38570413222909\n",
      "Epoch: 419000, WTA-Loss: -3.3564378038048743\n",
      "Epoch: 420000, WTA-Loss: -3.3485733054280282\n",
      "Epoch: 421000, WTA-Loss: -3.181790916353464\n",
      "Epoch: 422000, WTA-Loss: -3.2776932349652053\n",
      "Epoch: 423000, WTA-Loss: -3.3585486907064914\n",
      "Epoch: 424000, WTA-Loss: -3.1902693623900413\n",
      "Epoch: 425000, WTA-Loss: -3.3061378723680974\n",
      "Epoch: 426000, WTA-Loss: -3.24620447576046\n",
      "Epoch: 427000, WTA-Loss: -3.3094673075079917\n",
      "Epoch: 428000, WTA-Loss: -3.2638200443685053\n",
      "Epoch: 429000, WTA-Loss: -3.1402744586765765\n",
      "Epoch: 430000, WTA-Loss: -3.2896231449246405\n",
      "Epoch: 431000, WTA-Loss: -3.370207142531872\n",
      "Epoch: 432000, WTA-Loss: -3.22257143124938\n",
      "Epoch: 433000, WTA-Loss: -3.3473871014118193\n",
      "Epoch: 434000, WTA-Loss: -3.3279433912932874\n",
      "Epoch: 435000, WTA-Loss: -3.3527200295329096\n",
      "Epoch: 436000, WTA-Loss: -3.3857918829619886\n",
      "Epoch: 437000, WTA-Loss: -3.1772745335400105\n",
      "Epoch: 438000, WTA-Loss: -3.288511542856693\n",
      "Epoch: 439000, WTA-Loss: -3.2692696630060674\n",
      "Epoch: 440000, WTA-Loss: -3.3186796078532934\n",
      "Epoch: 441000, WTA-Loss: -3.0962182578742503\n",
      "Epoch: 442000, WTA-Loss: -3.3236811221539972\n",
      "Epoch: 443000, WTA-Loss: -3.3438339019417764\n",
      "Epoch: 444000, WTA-Loss: -3.019994303524494\n",
      "Epoch: 445000, WTA-Loss: -2.21063201995939\n",
      "Epoch: 446000, WTA-Loss: -2.882736073374748\n",
      "Epoch: 447000, WTA-Loss: -3.2088421147465707\n",
      "Epoch: 448000, WTA-Loss: -3.0188090385198594\n",
      "Epoch: 449000, WTA-Loss: -3.3176493665874003\n",
      "Epoch: 450000, WTA-Loss: -3.3213754664063453\n",
      "Epoch: 451000, WTA-Loss: -2.7853493339270354\n",
      "Epoch: 452000, WTA-Loss: 1.2379099759710952\n",
      "Epoch: 453000, WTA-Loss: -1.2166795655861498\n",
      "Epoch: 454000, WTA-Loss: -1.759832642197609\n",
      "Epoch: 455000, WTA-Loss: -1.7677006238251924\n",
      "Epoch: 456000, WTA-Loss: -2.148693279504776\n",
      "Epoch: 457000, WTA-Loss: -2.4249623902738096\n",
      "Epoch: 458000, WTA-Loss: -2.451468895599246\n",
      "Epoch: 459000, WTA-Loss: -2.056936474993825\n",
      "Epoch: 460000, WTA-Loss: -2.705895258888602\n",
      "Epoch: 461000, WTA-Loss: -2.6040747430473568\n",
      "Epoch: 462000, WTA-Loss: -1.9809971654638647\n",
      "Epoch: 463000, WTA-Loss: -2.401437745153904\n",
      "Epoch: 464000, WTA-Loss: -2.3903934386521577\n",
      "Epoch: 465000, WTA-Loss: -2.6214732704758643\n",
      "Epoch: 466000, WTA-Loss: -2.7297817493081094\n",
      "Epoch: 467000, WTA-Loss: -2.30022317263484\n",
      "Epoch: 468000, WTA-Loss: -2.616132969498634\n",
      "Epoch: 469000, WTA-Loss: -2.1704306496605277\n",
      "Epoch: 470000, WTA-Loss: -1.5204550630748273\n",
      "Epoch: 471000, WTA-Loss: -1.2433916457695886\n",
      "Epoch: 472000, WTA-Loss: -1.9272325704693793\n",
      "Epoch: 473000, WTA-Loss: -2.147649216502905\n",
      "Epoch: 474000, WTA-Loss: -2.3244973996579645\n",
      "Epoch: 475000, WTA-Loss: -1.5314206501357257\n",
      "Epoch: 476000, WTA-Loss: -1.3607752090841532\n",
      "Epoch: 477000, WTA-Loss: -1.9451907561570405\n",
      "Epoch: 478000, WTA-Loss: -1.4901903666555882\n",
      "Epoch: 479000, WTA-Loss: -1.3953486553542316\n",
      "Epoch: 480000, WTA-Loss: -0.6065975103154778\n",
      "Epoch: 481000, WTA-Loss: -0.8384740017727017\n",
      "Epoch: 482000, WTA-Loss: -0.960855621509254\n",
      "Epoch: 483000, WTA-Loss: -1.0524240695759655\n",
      "Epoch: 484000, WTA-Loss: -1.155773670822382\n",
      "Epoch: 485000, WTA-Loss: -1.200003756031394\n",
      "Epoch: 486000, WTA-Loss: -1.2519190446957946\n",
      "Epoch: 487000, WTA-Loss: -1.3112381989061832\n",
      "Epoch: 488000, WTA-Loss: -1.3329647073745727\n",
      "Epoch: 489000, WTA-Loss: -1.3586358559206129\n",
      "Epoch: 490000, WTA-Loss: -1.4698175850212574\n",
      "Epoch: 491000, WTA-Loss: -1.476977729693055\n",
      "Epoch: 492000, WTA-Loss: -1.507965908974409\n",
      "Epoch: 493000, WTA-Loss: -1.3246754933390767\n",
      "Epoch: 494000, WTA-Loss: -1.4117473544925452\n",
      "Epoch: 495000, WTA-Loss: -1.5517441178411244\n",
      "Epoch: 496000, WTA-Loss: -1.5498714593052865\n",
      "Epoch: 497000, WTA-Loss: -1.6132779114842415\n",
      "Epoch: 498000, WTA-Loss: -1.6193427955508233\n",
      "Epoch: 499000, WTA-Loss: -1.6469224752634763\n",
      "Epoch: 500000, WTA-Loss: -1.6883527805507184\n",
      "Epoch: 501000, WTA-Loss: -0.6263397676064633\n",
      "Epoch: 502000, WTA-Loss: -1.593753053367138\n",
      "Epoch: 503000, WTA-Loss: -1.647233322545886\n",
      "Epoch: 504000, WTA-Loss: -1.6985519627779722\n",
      "Epoch: 505000, WTA-Loss: -1.693548645824194\n",
      "Epoch: 506000, WTA-Loss: -1.710202557951212\n",
      "Epoch: 507000, WTA-Loss: -1.7904692647755147\n",
      "Epoch: 508000, WTA-Loss: -1.7100105237960816\n",
      "Epoch: 509000, WTA-Loss: -1.6561825649887323\n",
      "Epoch: 510000, WTA-Loss: -1.7879727085530759\n",
      "Epoch: 511000, WTA-Loss: -1.8380021890103817\n",
      "Epoch: 512000, WTA-Loss: -1.7357796466499567\n",
      "Epoch: 513000, WTA-Loss: -1.8402848905175924\n",
      "Epoch: 514000, WTA-Loss: -1.8578143186569214\n",
      "Epoch: 515000, WTA-Loss: -1.8636180824786424\n",
      "Epoch: 516000, WTA-Loss: -1.8956040880680085\n",
      "Epoch: 517000, WTA-Loss: -1.8330494983643293\n",
      "Epoch: 518000, WTA-Loss: -1.842416073590517\n",
      "Epoch: 519000, WTA-Loss: -1.913610859721899\n",
      "Epoch: 520000, WTA-Loss: -1.7578360684514045\n",
      "Epoch: 521000, WTA-Loss: -1.9498574118316174\n",
      "Epoch: 522000, WTA-Loss: -1.9084689443856477\n",
      "Epoch: 523000, WTA-Loss: -1.965171071499586\n",
      "Epoch: 524000, WTA-Loss: -1.9601785248816013\n",
      "Epoch: 525000, WTA-Loss: -1.9609251821488143\n",
      "Epoch: 526000, WTA-Loss: -1.9806752126812934\n",
      "Epoch: 527000, WTA-Loss: -0.9542469775937498\n",
      "Epoch: 528000, WTA-Loss: -1.8168938421458005\n",
      "Epoch: 529000, WTA-Loss: -1.895427990615368\n",
      "Epoch: 530000, WTA-Loss: -1.9398886622786522\n",
      "Epoch: 531000, WTA-Loss: -1.9583326413109898\n",
      "Epoch: 532000, WTA-Loss: -2.0017926104217767\n",
      "Epoch: 533000, WTA-Loss: -2.023586977362633\n",
      "Epoch: 534000, WTA-Loss: -2.0182342605888843\n",
      "Epoch: 535000, WTA-Loss: -1.9955975234806538\n",
      "Epoch: 536000, WTA-Loss: 8067808.481615744\n",
      "Epoch: 537000, WTA-Loss: -1.8712957478165626\n",
      "Epoch: 538000, WTA-Loss: -1.8343332799822092\n",
      "Epoch: 539000, WTA-Loss: -1.823234715297818\n",
      "Epoch: 540000, WTA-Loss: -1.930069617241621\n",
      "Epoch: 541000, WTA-Loss: -1.9111153367459774\n",
      "Epoch: 542000, WTA-Loss: -1.9978305656313897\n",
      "Epoch: 543000, WTA-Loss: -1.947751150071621\n",
      "Epoch: 544000, WTA-Loss: -2.0085375793874265\n",
      "Epoch: 545000, WTA-Loss: -2.0141185972988604\n",
      "Epoch: 546000, WTA-Loss: -1.8242024715840817\n",
      "Epoch: 547000, WTA-Loss: -1.9890696359574795\n",
      "Epoch: 548000, WTA-Loss: -2.037750002577901\n",
      "Epoch: 549000, WTA-Loss: -2.049461040943861\n",
      "Epoch: 550000, WTA-Loss: -2.022147069811821\n",
      "Epoch: 551000, WTA-Loss: -2.0516716809421776\n",
      "Epoch: 552000, WTA-Loss: -2.0328778068870306\n",
      "Epoch: 553000, WTA-Loss: -2.0502210095375775\n",
      "Epoch: 554000, WTA-Loss: -2.080061882928014\n",
      "Epoch: 555000, WTA-Loss: -2.0561483676582575\n",
      "Epoch: 556000, WTA-Loss: -2.0043232706412675\n",
      "Epoch: 557000, WTA-Loss: -2.0522868265509606\n",
      "Epoch: 558000, WTA-Loss: -2.0651201594173907\n",
      "Epoch: 559000, WTA-Loss: -2.092212198138237\n",
      "Epoch: 560000, WTA-Loss: -0.8594921463504434\n",
      "Epoch: 561000, WTA-Loss: -2.0274039685428145\n",
      "Epoch: 562000, WTA-Loss: -2.060518593788147\n",
      "Epoch: 563000, WTA-Loss: -2.1013379320949315\n",
      "Epoch: 564000, WTA-Loss: -2.08153208398819\n",
      "Epoch: 565000, WTA-Loss: -2.1330279381275177\n",
      "Epoch: 566000, WTA-Loss: -2.122761159598827\n",
      "Epoch: 567000, WTA-Loss: -2.012533584780991\n",
      "Epoch: 568000, WTA-Loss: -2.116258802205324\n",
      "Epoch: 569000, WTA-Loss: -1.8704074024371802\n",
      "Epoch: 570000, WTA-Loss: -2.00417421169579\n",
      "Epoch: 571000, WTA-Loss: -2.1165535273104905\n",
      "Epoch: 572000, WTA-Loss: -2.1624889385402204\n",
      "Epoch: 573000, WTA-Loss: -2.12600055937469\n",
      "Epoch: 574000, WTA-Loss: -2.165602444678545\n",
      "Epoch: 575000, WTA-Loss: -2.123665214329958\n",
      "Epoch: 576000, WTA-Loss: -1.97695449029468\n",
      "Epoch: 577000, WTA-Loss: -2.1001680446863173\n",
      "Epoch: 578000, WTA-Loss: -2.1224389312416316\n",
      "Epoch: 579000, WTA-Loss: -2.1906090594530108\n",
      "Epoch: 580000, WTA-Loss: -2.183877096116543\n",
      "Epoch: 581000, WTA-Loss: -2.2046875205636023\n",
      "Epoch: 582000, WTA-Loss: -2.2318759284764527\n",
      "Epoch: 583000, WTA-Loss: -2.189129769399762\n",
      "Epoch: 584000, WTA-Loss: -2.120726380556822\n",
      "Epoch: 585000, WTA-Loss: -2.1869787232875826\n",
      "Epoch: 586000, WTA-Loss: -2.0878553384393452\n",
      "Epoch: 587000, WTA-Loss: -2.225826088801026\n",
      "Epoch: 588000, WTA-Loss: -2.098209423720837\n",
      "Epoch: 589000, WTA-Loss: -2.170690349429846\n",
      "Epoch: 590000, WTA-Loss: -2.2435103685557842\n",
      "Epoch: 591000, WTA-Loss: -2.2304494504332544\n",
      "Epoch: 592000, WTA-Loss: -2.2436039936840535\n",
      "Epoch: 593000, WTA-Loss: -2.236501624584198\n",
      "Epoch: 594000, WTA-Loss: -2.275483720585704\n",
      "Epoch: 595000, WTA-Loss: -2.223306910045445\n",
      "Epoch: 596000, WTA-Loss: -2.274272093594074\n",
      "Epoch: 597000, WTA-Loss: -2.2704787974357603\n",
      "Epoch: 598000, WTA-Loss: -2.2146640340089796\n",
      "Epoch: 599000, WTA-Loss: -2.1476318472027778\n",
      "Epoch: 600000, WTA-Loss: -2.298092812627554\n",
      "Epoch: 601000, WTA-Loss: -2.3058169456720354\n",
      "Epoch: 602000, WTA-Loss: -2.3084609034508468\n",
      "Epoch: 603000, WTA-Loss: -2.176720862150192\n",
      "Epoch: 604000, WTA-Loss: -2.230617507815361\n",
      "Epoch: 605000, WTA-Loss: -2.062490925781429\n",
      "Epoch: 606000, WTA-Loss: -2.1790974023044107\n",
      "Epoch: 607000, WTA-Loss: -2.3033610194027423\n",
      "Epoch: 608000, WTA-Loss: -2.325791947454214\n",
      "Epoch: 609000, WTA-Loss: -2.243540125131607\n",
      "Epoch: 610000, WTA-Loss: -2.298292853295803\n",
      "Epoch: 611000, WTA-Loss: -2.3546441953778268\n",
      "Epoch: 612000, WTA-Loss: -2.2648337683975694\n",
      "Epoch: 613000, WTA-Loss: -2.270089091062546\n",
      "Epoch: 614000, WTA-Loss: -2.3496690934300424\n",
      "Epoch: 615000, WTA-Loss: -1.7321022012531757\n",
      "Epoch: 616000, WTA-Loss: -2.098543636575341\n",
      "Epoch: 617000, WTA-Loss: -2.0609124990068377\n",
      "Epoch: 618000, WTA-Loss: -2.1771908527761696\n",
      "Epoch: 619000, WTA-Loss: -2.1664544771015644\n",
      "Epoch: 620000, WTA-Loss: -2.163917855501175\n",
      "Epoch: 621000, WTA-Loss: -2.149021106898785\n",
      "Epoch: 622000, WTA-Loss: -2.2069913759827613\n",
      "Epoch: 623000, WTA-Loss: -2.250407965481281\n",
      "Epoch: 624000, WTA-Loss: -2.1397195937708022\n",
      "Epoch: 625000, WTA-Loss: -2.2877947482764722\n",
      "Epoch: 626000, WTA-Loss: -2.3018758425712584\n",
      "Epoch: 627000, WTA-Loss: -2.364654716670513\n",
      "Epoch: 628000, WTA-Loss: -2.254399065166712\n",
      "Epoch: 629000, WTA-Loss: -2.190410965889692\n",
      "Epoch: 630000, WTA-Loss: -2.285549926877022\n",
      "Epoch: 631000, WTA-Loss: -2.168974381223321\n",
      "Epoch: 632000, WTA-Loss: -2.266869953393936\n",
      "Epoch: 633000, WTA-Loss: -2.1930533169209956\n",
      "Epoch: 634000, WTA-Loss: -2.17791204726696\n",
      "Epoch: 635000, WTA-Loss: -1.6054148747446015\n",
      "Epoch: 636000, WTA-Loss: -2.1772468821704387\n",
      "Epoch: 637000, WTA-Loss: -2.2709272002577783\n",
      "Epoch: 638000, WTA-Loss: -1.7828159034252167\n",
      "Epoch: 639000, WTA-Loss: -2.100368420764804\n",
      "Epoch: 640000, WTA-Loss: -2.1373498674482105\n",
      "Epoch: 641000, WTA-Loss: -1.7681370520237834\n",
      "Epoch: 642000, WTA-Loss: -2.0960699501633644\n",
      "Epoch: 643000, WTA-Loss: -2.2360615568608044\n",
      "Epoch: 644000, WTA-Loss: -2.241075454890728\n",
      "Epoch: 645000, WTA-Loss: -2.2458938238173722\n",
      "Epoch: 646000, WTA-Loss: -2.1331788002103567\n",
      "Epoch: 647000, WTA-Loss: -2.153996947288513\n",
      "Epoch: 648000, WTA-Loss: -1.9401184386909007\n",
      "Epoch: 649000, WTA-Loss: -1.7151527825593949\n",
      "Epoch: 650000, WTA-Loss: -1.9625118122920393\n",
      "Epoch: 651000, WTA-Loss: -1.9217963413000108\n",
      "Epoch: 652000, WTA-Loss: -1.685736480292864\n",
      "Epoch: 653000, WTA-Loss: -2.163752934291959\n",
      "Epoch: 654000, WTA-Loss: -2.1691942036449907\n",
      "Epoch: 655000, WTA-Loss: -2.2640102910995483\n",
      "Epoch: 656000, WTA-Loss: -2.127902247205377\n",
      "Epoch: 657000, WTA-Loss: -2.035244949966669\n",
      "Epoch: 658000, WTA-Loss: -1.9227283869683742\n",
      "Epoch: 659000, WTA-Loss: -2.1681818397045136\n",
      "Epoch: 660000, WTA-Loss: -1.4157719547580927\n",
      "Epoch: 661000, WTA-Loss: -0.2840807357300073\n",
      "Epoch: 662000, WTA-Loss: -0.6286665879935026\n",
      "Epoch: 663000, WTA-Loss: -0.9042065810859203\n",
      "Epoch: 664000, WTA-Loss: -0.6437268111873418\n",
      "Epoch: 665000, WTA-Loss: -1.0515371871069075\n",
      "Epoch: 666000, WTA-Loss: -1.1415711273252964\n",
      "Epoch: 667000, WTA-Loss: -1.2190718214064837\n",
      "Epoch: 668000, WTA-Loss: -1.3181181075870991\n",
      "Epoch: 669000, WTA-Loss: -1.3710333917289972\n",
      "Epoch: 670000, WTA-Loss: -1.4497619249224662\n",
      "Epoch: 671000, WTA-Loss: -1.495273180142045\n",
      "Epoch: 672000, WTA-Loss: -1.5332166491895913\n",
      "Epoch: 673000, WTA-Loss: -1.5909452920556069\n",
      "Epoch: 674000, WTA-Loss: -1.619243862390518\n",
      "Epoch: 675000, WTA-Loss: -1.6303087761960924\n",
      "Epoch: 676000, WTA-Loss: -1.7210704914033412\n",
      "Epoch: 677000, WTA-Loss: -1.6854354154989124\n",
      "Epoch: 678000, WTA-Loss: -1.7876985796689988\n",
      "Epoch: 679000, WTA-Loss: -1.8240527278482914\n",
      "Epoch: 680000, WTA-Loss: -1.8560796619951725\n",
      "Epoch: 681000, WTA-Loss: -1.8197840721309184\n",
      "Epoch: 682000, WTA-Loss: -1.8955984064042568\n",
      "Epoch: 683000, WTA-Loss: -1.8914720620810985\n",
      "Epoch: 684000, WTA-Loss: -1.9523948737084866\n",
      "Epoch: 685000, WTA-Loss: -1.9563448148369789\n",
      "Epoch: 686000, WTA-Loss: -1.990427927210927\n",
      "Epoch: 687000, WTA-Loss: -2.000360935777426\n",
      "Epoch: 688000, WTA-Loss: -2.0254657405912875\n",
      "Epoch: 689000, WTA-Loss: -1.986267770498991\n",
      "Epoch: 690000, WTA-Loss: -2.0525639638155697\n",
      "Epoch: 691000, WTA-Loss: -1.985452306047082\n",
      "Epoch: 692000, WTA-Loss: -1.956781898313202\n",
      "Epoch: 693000, WTA-Loss: -0.36112101094890386\n",
      "Epoch: 694000, WTA-Loss: -1.1596480091512202\n",
      "Epoch: 695000, WTA-Loss: -1.4514195125252007\n",
      "Epoch: 696000, WTA-Loss: -1.5970128341838719\n",
      "Epoch: 697000, WTA-Loss: -1.6376889653448015\n",
      "Epoch: 698000, WTA-Loss: -1.8960526344180106\n",
      "Epoch: 699000, WTA-Loss: -2.0372243641018866\n",
      "Epoch: 700000, WTA-Loss: -2.1101275559961796\n",
      "Epoch: 701000, WTA-Loss: -1.9436879133749754\n",
      "Epoch: 702000, WTA-Loss: -2.1374572432637216\n",
      "Epoch: 703000, WTA-Loss: -2.123174099266529\n",
      "Epoch: 704000, WTA-Loss: -2.15120459112525\n",
      "Epoch: 705000, WTA-Loss: -2.1169509539306164\n",
      "Epoch: 706000, WTA-Loss: -2.176321871548891\n",
      "Epoch: 707000, WTA-Loss: -2.1899565567970276\n",
      "Epoch: 708000, WTA-Loss: -2.1763670324385167\n",
      "Epoch: 709000, WTA-Loss: -2.237904534697533\n",
      "Epoch: 710000, WTA-Loss: -2.209586277872324\n",
      "Epoch: 711000, WTA-Loss: 7199.010398539573\n",
      "Epoch: 712000, WTA-Loss: -2.0267236718684436\n",
      "Epoch: 713000, WTA-Loss: -2.1692740746438504\n",
      "Epoch: 714000, WTA-Loss: -2.1489895059466364\n",
      "Epoch: 715000, WTA-Loss: -2.0429082048535347\n",
      "Epoch: 716000, WTA-Loss: -2.142334702000022\n",
      "Epoch: 717000, WTA-Loss: -2.0217730420827866\n",
      "Epoch: 718000, WTA-Loss: -2.2139865983724594\n",
      "Epoch: 719000, WTA-Loss: -2.1406481079757214\n",
      "Epoch: 720000, WTA-Loss: -2.1808242705464362\n",
      "Epoch: 721000, WTA-Loss: -2.2514677083194257\n",
      "Epoch: 722000, WTA-Loss: -2.2344060087502005\n",
      "Epoch: 723000, WTA-Loss: -2.191226839184761\n",
      "Epoch: 724000, WTA-Loss: -2.267341891348362\n",
      "Epoch: 725000, WTA-Loss: -2.298704750806093\n",
      "Epoch: 726000, WTA-Loss: -2.2921658249199393\n",
      "Epoch: 727000, WTA-Loss: -2.3263999433517455\n",
      "Epoch: 728000, WTA-Loss: -2.346355997174978\n",
      "Epoch: 729000, WTA-Loss: -2.341262647271156\n",
      "Epoch: 730000, WTA-Loss: -2.348929012835026\n",
      "Epoch: 731000, WTA-Loss: -2.307890325307846\n",
      "Epoch: 732000, WTA-Loss: -2.395351227208972\n",
      "Epoch: 733000, WTA-Loss: -2.392817035779357\n",
      "Epoch: 734000, WTA-Loss: -2.3621466531157496\n",
      "Epoch: 735000, WTA-Loss: -2.3962400428950787\n",
      "Epoch: 736000, WTA-Loss: -2.4355853943526746\n",
      "Epoch: 737000, WTA-Loss: -2.4154855867624283\n",
      "Epoch: 738000, WTA-Loss: -2.4094492780268193\n",
      "Epoch: 739000, WTA-Loss: -2.454446900010109\n",
      "Epoch: 740000, WTA-Loss: -2.386273042231798\n",
      "Epoch: 741000, WTA-Loss: -2.486504243969917\n",
      "Epoch: 742000, WTA-Loss: -2.3724003358632326\n",
      "Epoch: 743000, WTA-Loss: -2.4644940569102762\n",
      "Epoch: 744000, WTA-Loss: -2.419332486450672\n",
      "Epoch: 745000, WTA-Loss: -2.4983825436830522\n",
      "Epoch: 746000, WTA-Loss: -2.4119123585522173\n",
      "Epoch: 747000, WTA-Loss: -2.4493268771767616\n",
      "Epoch: 748000, WTA-Loss: -2.4978232771158217\n",
      "Epoch: 749000, WTA-Loss: -2.530287373289466\n",
      "Epoch: 750000, WTA-Loss: -2.4796065106242895\n",
      "Epoch: 751000, WTA-Loss: -2.5150363759994505\n",
      "Epoch: 752000, WTA-Loss: -2.4772885745763777\n",
      "Epoch: 753000, WTA-Loss: -2.5287684314846994\n",
      "Epoch: 754000, WTA-Loss: -2.5483671382069586\n",
      "Epoch: 755000, WTA-Loss: -2.5364995310902594\n",
      "Epoch: 756000, WTA-Loss: -2.5478610910773276\n",
      "Epoch: 757000, WTA-Loss: -2.471462179660797\n",
      "Epoch: 758000, WTA-Loss: -2.549600020855665\n",
      "Epoch: 759000, WTA-Loss: -2.449163816049695\n",
      "Epoch: 760000, WTA-Loss: -2.5381607232689856\n",
      "Epoch: 761000, WTA-Loss: -2.582413589596748\n",
      "Epoch: 762000, WTA-Loss: -2.571681070178747\n",
      "Epoch: 763000, WTA-Loss: -2.5487964746654033\n",
      "Epoch: 764000, WTA-Loss: -2.5389335732907057\n",
      "Epoch: 765000, WTA-Loss: -2.6237836646437644\n",
      "Epoch: 766000, WTA-Loss: -2.6064957877397537\n",
      "Epoch: 767000, WTA-Loss: -2.5267862358391286\n",
      "Epoch: 768000, WTA-Loss: -2.5256155263781546\n",
      "Epoch: 769000, WTA-Loss: -2.6254169630110264\n",
      "Epoch: 770000, WTA-Loss: -2.593435094118118\n",
      "Epoch: 771000, WTA-Loss: -2.591314187541604\n",
      "Epoch: 772000, WTA-Loss: -2.3368171787261964\n",
      "Epoch: 773000, WTA-Loss: -2.6065834404528143\n",
      "Epoch: 774000, WTA-Loss: -2.639679126918316\n",
      "Epoch: 775000, WTA-Loss: -2.6211107867062093\n",
      "Epoch: 776000, WTA-Loss: -2.6356745322048662\n",
      "Epoch: 777000, WTA-Loss: -2.61626219522953\n",
      "Epoch: 778000, WTA-Loss: -2.63533137512207\n",
      "Epoch: 779000, WTA-Loss: -2.6285281130373477\n",
      "Epoch: 780000, WTA-Loss: -2.63734645986557\n",
      "Epoch: 781000, WTA-Loss: -2.672776947259903\n",
      "Epoch: 782000, WTA-Loss: -2.668242633223534\n",
      "Epoch: 783000, WTA-Loss: -2.6873592572808267\n",
      "Epoch: 784000, WTA-Loss: -0.7966926431045868\n",
      "Epoch: 785000, WTA-Loss: -0.916485406074673\n",
      "Epoch: 786000, WTA-Loss: -1.0666083740144967\n",
      "Epoch: 787000, WTA-Loss: -0.8746415151013062\n",
      "Epoch: 788000, WTA-Loss: -1.0995726431310178\n",
      "Epoch: 789000, WTA-Loss: -1.2313383613824844\n",
      "Epoch: 790000, WTA-Loss: -1.277790673673153\n",
      "Epoch: 791000, WTA-Loss: -1.451923906289041\n",
      "Epoch: 792000, WTA-Loss: -2.4687061813771725\n",
      "Epoch: 793000, WTA-Loss: -2.5646005853414535\n",
      "Epoch: 794000, WTA-Loss: -2.4988168703615665\n",
      "Epoch: 795000, WTA-Loss: -2.6006827462911604\n",
      "Epoch: 796000, WTA-Loss: -2.575583126604557\n",
      "Epoch: 797000, WTA-Loss: -2.6504494114220143\n",
      "Epoch: 798000, WTA-Loss: -2.658779136836529\n",
      "Epoch: 799000, WTA-Loss: -2.599598873496056\n",
      "Epoch: 800000, WTA-Loss: -2.5847366029024124\n",
      "Epoch: 801000, WTA-Loss: -2.6744900923371313\n",
      "Epoch: 802000, WTA-Loss: -2.677776697963476\n",
      "Epoch: 803000, WTA-Loss: -2.6366496065855025\n",
      "Epoch: 804000, WTA-Loss: -2.626805755376816\n",
      "Epoch: 805000, WTA-Loss: -2.659310823768377\n",
      "Epoch: 806000, WTA-Loss: -2.649318532317877\n",
      "Epoch: 807000, WTA-Loss: -2.5671701525896786\n",
      "Epoch: 808000, WTA-Loss: -2.606879331767559\n",
      "Epoch: 809000, WTA-Loss: -2.646616070270538\n",
      "Epoch: 810000, WTA-Loss: -2.699822072207928\n",
      "Epoch: 811000, WTA-Loss: 120.3430832376182\n",
      "Epoch: 812000, WTA-Loss: -2.350030237108469\n",
      "Epoch: 813000, WTA-Loss: -2.6322224029004575\n",
      "Epoch: 814000, WTA-Loss: -2.3637403621748088\n",
      "Epoch: 815000, WTA-Loss: -2.6495457272827627\n",
      "Epoch: 816000, WTA-Loss: -2.5671854554861784\n",
      "Epoch: 817000, WTA-Loss: -2.4824477191865446\n",
      "Epoch: 818000, WTA-Loss: -2.671224424958229\n",
      "Epoch: 819000, WTA-Loss: -2.690310998558998\n",
      "Epoch: 820000, WTA-Loss: -2.6814529528021813\n",
      "Epoch: 821000, WTA-Loss: -2.706359033703804\n",
      "Epoch: 822000, WTA-Loss: -2.6862334573864937\n",
      "Epoch: 823000, WTA-Loss: -2.3606303384155036\n",
      "Epoch: 824000, WTA-Loss: -2.5301843951642513\n",
      "Epoch: 825000, WTA-Loss: -2.4639554281202143\n",
      "Epoch: 826000, WTA-Loss: -2.6136048331558706\n",
      "Epoch: 827000, WTA-Loss: -2.486211125701666\n",
      "Epoch: 828000, WTA-Loss: -2.637742741301656\n",
      "Epoch: 829000, WTA-Loss: -2.685270683646202\n",
      "Epoch: 830000, WTA-Loss: -2.6802434022426604\n",
      "Epoch: 831000, WTA-Loss: -2.5826623469889163\n",
      "Epoch: 832000, WTA-Loss: -2.6481665925979616\n",
      "Epoch: 833000, WTA-Loss: -2.729648195028305\n",
      "Epoch: 834000, WTA-Loss: -2.69676934748888\n",
      "Epoch: 835000, WTA-Loss: -2.71099020126462\n",
      "Epoch: 836000, WTA-Loss: -2.615781883120537\n",
      "Epoch: 837000, WTA-Loss: -2.6300313590765\n",
      "Epoch: 838000, WTA-Loss: -2.6024357438087464\n",
      "Epoch: 839000, WTA-Loss: -2.6278622347712517\n",
      "Epoch: 840000, WTA-Loss: -2.6402640948295595\n",
      "Epoch: 841000, WTA-Loss: -2.6289811587035654\n",
      "Epoch: 842000, WTA-Loss: -2.6665652850568295\n",
      "Epoch: 843000, WTA-Loss: -2.68203364521265\n",
      "Epoch: 844000, WTA-Loss: -2.698883944645524\n",
      "Epoch: 845000, WTA-Loss: -2.6423992064893245\n",
      "Epoch: 846000, WTA-Loss: -2.636126938909292\n",
      "Epoch: 847000, WTA-Loss: -2.454482489824295\n",
      "Epoch: 848000, WTA-Loss: -2.595712352126837\n",
      "Epoch: 849000, WTA-Loss: -2.1860239968453534\n",
      "Epoch: 850000, WTA-Loss: -0.5681746242372319\n",
      "Epoch: 851000, WTA-Loss: -0.7790840277522803\n",
      "Epoch: 852000, WTA-Loss: -1.1508019246794283\n",
      "Epoch: 853000, WTA-Loss: -2.376071990996599\n",
      "Epoch: 854000, WTA-Loss: -1.8702743346076458\n",
      "Epoch: 855000, WTA-Loss: -1.5503950819969177\n",
      "Epoch: 856000, WTA-Loss: -2.1934496923983096\n",
      "Epoch: 857000, WTA-Loss: -1.8794990734215826\n",
      "Epoch: 858000, WTA-Loss: -2.166018083009869\n",
      "Epoch: 859000, WTA-Loss: -2.3358849320709707\n",
      "Epoch: 860000, WTA-Loss: -2.39540413621068\n",
      "Epoch: 861000, WTA-Loss: -2.4268631770908833\n",
      "Epoch: 862000, WTA-Loss: -2.2987447096779943\n",
      "Epoch: 863000, WTA-Loss: -2.249995651870966\n",
      "Epoch: 864000, WTA-Loss: -2.4387894575744866\n",
      "Epoch: 865000, WTA-Loss: -2.5348661459088326\n",
      "Epoch: 866000, WTA-Loss: -2.42030892303586\n",
      "Epoch: 867000, WTA-Loss: -2.12152842294611\n",
      "Epoch: 868000, WTA-Loss: -0.7306326696965844\n",
      "Epoch: 869000, WTA-Loss: -0.877283520965837\n",
      "Epoch: 870000, WTA-Loss: -0.9104874358922244\n",
      "Epoch: 871000, WTA-Loss: -0.958793006785214\n",
      "Epoch: 872000, WTA-Loss: -0.9851052291691303\n",
      "Epoch: 873000, WTA-Loss: -1.0196535032391547\n",
      "Epoch: 874000, WTA-Loss: -1.03044458553195\n",
      "Epoch: 875000, WTA-Loss: -1.050537898078561\n",
      "Epoch: 876000, WTA-Loss: -1.079850583218038\n",
      "Epoch: 877000, WTA-Loss: -1.0817338845431805\n",
      "Epoch: 878000, WTA-Loss: -1.1031067508496344\n",
      "Epoch: 879000, WTA-Loss: -1.1309743503779173\n",
      "Epoch: 880000, WTA-Loss: -1.1469026821553707\n",
      "Epoch: 881000, WTA-Loss: -1.1587592948675156\n",
      "Epoch: 882000, WTA-Loss: -1.1674135165959596\n",
      "Epoch: 883000, WTA-Loss: -1.1766988148242234\n",
      "Epoch: 884000, WTA-Loss: -1.1820885592848063\n",
      "Epoch: 885000, WTA-Loss: -1.1974353242218494\n",
      "Epoch: 886000, WTA-Loss: -1.1995218068286777\n",
      "Epoch: 887000, WTA-Loss: -1.2175433053076268\n",
      "Epoch: 888000, WTA-Loss: -1.2171601024568082\n",
      "Epoch: 889000, WTA-Loss: -1.1966233355104923\n",
      "Epoch: 890000, WTA-Loss: 105.40710771422833\n",
      "Epoch: 891000, WTA-Loss: -1.0414588623270393\n",
      "Epoch: 892000, WTA-Loss: -1.2019722634106875\n",
      "Epoch: 893000, WTA-Loss: -1.2468116898834705\n",
      "Epoch: 894000, WTA-Loss: -1.2854814300984143\n",
      "Epoch: 895000, WTA-Loss: -1.3327277677208185\n",
      "Epoch: 896000, WTA-Loss: -1.379362954825163\n",
      "Epoch: 897000, WTA-Loss: 0.12616996153444052\n",
      "Epoch: 898000, WTA-Loss: -0.7602935485094786\n",
      "Epoch: 899000, WTA-Loss: -1.2122174886986614\n",
      "Epoch: 900000, WTA-Loss: -1.488590057551861\n",
      "Epoch: 901000, WTA-Loss: -1.5575645650476218\n",
      "Epoch: 902000, WTA-Loss: -1.7933856786191464\n",
      "Epoch: 903000, WTA-Loss: -1.8698093553185462\n",
      "Epoch: 904000, WTA-Loss: -1.90995630659163\n",
      "Epoch: 905000, WTA-Loss: -2.003102576881647\n",
      "Epoch: 906000, WTA-Loss: -1.9976634542644023\n",
      "Epoch: 907000, WTA-Loss: -1.8991235847622157\n",
      "Epoch: 908000, WTA-Loss: -1.936793367281556\n",
      "Epoch: 909000, WTA-Loss: -2.0821265533566473\n",
      "Epoch: 910000, WTA-Loss: -2.2024163380861284\n",
      "Epoch: 911000, WTA-Loss: -2.0511432737112045\n",
      "Epoch: 912000, WTA-Loss: -1.6934730913117528\n",
      "Epoch: 913000, WTA-Loss: -2.0887881790697573\n",
      "Epoch: 914000, WTA-Loss: -1.4247217743154614\n",
      "Epoch: 915000, WTA-Loss: -1.6294422851651906\n",
      "Epoch: 916000, WTA-Loss: -1.976359525129199\n",
      "Epoch: 917000, WTA-Loss: -2.0078375354409217\n",
      "Epoch: 918000, WTA-Loss: -1.9932307337075472\n",
      "Epoch: 919000, WTA-Loss: -1.5537116470253094\n",
      "Epoch: 920000, WTA-Loss: -2.010451791550964\n",
      "Epoch: 921000, WTA-Loss: -1.8670895222574473\n",
      "Epoch: 922000, WTA-Loss: -1.7835105626285077\n",
      "Epoch: 923000, WTA-Loss: -1.8925798880904914\n",
      "Epoch: 924000, WTA-Loss: -2.031618805587292\n",
      "Epoch: 925000, WTA-Loss: -2.0063601392284034\n",
      "Epoch: 926000, WTA-Loss: -2.026051100883633\n",
      "Epoch: 927000, WTA-Loss: -1.8249953055158257\n",
      "Epoch: 928000, WTA-Loss: -1.261439031988848\n",
      "Epoch: 929000, WTA-Loss: -1.2047535759881138\n",
      "Epoch: 930000, WTA-Loss: -0.5557012815345078\n",
      "Epoch: 931000, WTA-Loss: -1.4108574023097753\n",
      "Epoch: 932000, WTA-Loss: -1.6204113953262567\n",
      "Epoch: 933000, WTA-Loss: -1.3734684053529054\n",
      "Epoch: 934000, WTA-Loss: -1.7901264249682427\n",
      "Epoch: 935000, WTA-Loss: -1.8299467004984618\n",
      "Epoch: 936000, WTA-Loss: -1.665898286920041\n",
      "Epoch: 937000, WTA-Loss: -1.5204043606556952\n",
      "Epoch: 938000, WTA-Loss: 2.8210306869060733\n",
      "Epoch: 939000, WTA-Loss: -1.148759722456336\n",
      "Epoch: 940000, WTA-Loss: -1.5923165301680564\n",
      "Epoch: 941000, WTA-Loss: -1.5825877955257892\n",
      "Epoch: 942000, WTA-Loss: -1.7490435769259929\n",
      "Epoch: 943000, WTA-Loss: -1.8593034825325012\n",
      "Epoch: 944000, WTA-Loss: -1.908303301602602\n",
      "Epoch: 945000, WTA-Loss: -1.9295367881655694\n",
      "Epoch: 946000, WTA-Loss: -1.9763168515861034\n",
      "Epoch: 947000, WTA-Loss: -1.7006284219920635\n",
      "Epoch: 948000, WTA-Loss: -1.9212183696329594\n",
      "Epoch: 949000, WTA-Loss: -1.5082709140107036\n",
      "Epoch: 950000, WTA-Loss: -1.2586612590774893\n",
      "Epoch: 951000, WTA-Loss: -1.776978462599218\n",
      "Epoch: 952000, WTA-Loss: -1.7081206056848168\n",
      "Epoch: 953000, WTA-Loss: -1.963771647810936\n",
      "Epoch: 954000, WTA-Loss: -2.0214332427978516\n",
      "Epoch: 955000, WTA-Loss: -2.0531564827561377\n",
      "Epoch: 956000, WTA-Loss: -1.9809312918782234\n",
      "Epoch: 957000, WTA-Loss: -2.1325144893527033\n",
      "Epoch: 958000, WTA-Loss: -2.033500191517174\n",
      "Epoch: 959000, WTA-Loss: -0.9438571142926813\n",
      "Epoch: 960000, WTA-Loss: -1.0003031583800912\n",
      "Epoch: 961000, WTA-Loss: -1.5181839509010315\n",
      "Epoch: 962000, WTA-Loss: -1.4536620904318989\n",
      "Epoch: 963000, WTA-Loss: -1.8743093565702438\n",
      "Epoch: 964000, WTA-Loss: -2.0413057926893234\n",
      "Epoch: 965000, WTA-Loss: -1.6097808528728783\n",
      "Epoch: 966000, WTA-Loss: -1.4626571371518076\n",
      "Epoch: 967000, WTA-Loss: -1.9300438529700041\n",
      "Epoch: 968000, WTA-Loss: -2.089100629031658\n",
      "Epoch: 969000, WTA-Loss: -2.084753973007202\n",
      "Epoch: 970000, WTA-Loss: -2.1709501205980777\n",
      "Epoch: 971000, WTA-Loss: -2.001928831025958\n",
      "Epoch: 972000, WTA-Loss: -2.171019527375698\n",
      "Epoch: 973000, WTA-Loss: -1.7906029572058468\n",
      "Epoch: 974000, WTA-Loss: -1.5190523103158922\n",
      "Epoch: 975000, WTA-Loss: -1.2497930011134595\n",
      "Epoch: 976000, WTA-Loss: -1.2565297002657316\n",
      "Epoch: 977000, WTA-Loss: -1.5405505643300712\n",
      "Epoch: 978000, WTA-Loss: -2.047993475943804\n",
      "Epoch: 979000, WTA-Loss: -2.136175225585699\n",
      "Epoch: 980000, WTA-Loss: -2.163785779416561\n",
      "Epoch: 981000, WTA-Loss: -1.9914559784978627\n",
      "Epoch: 982000, WTA-Loss: -1.8138922472447156\n",
      "Epoch: 983000, WTA-Loss: -0.043275634398611144\n",
      "Epoch: 984000, WTA-Loss: -0.21925158970523626\n",
      "Epoch: 985000, WTA-Loss: -0.2737156306123361\n",
      "Epoch: 986000, WTA-Loss: -0.3099140604659915\n",
      "Epoch: 987000, WTA-Loss: -0.3491249246690422\n",
      "Epoch: 988000, WTA-Loss: -0.3710202257819474\n",
      "Epoch: 989000, WTA-Loss: -0.41511538485158234\n",
      "Epoch: 990000, WTA-Loss: -0.45985268096253273\n",
      "Epoch: 991000, WTA-Loss: -0.49026070183422416\n",
      "Epoch: 992000, WTA-Loss: -0.5332917300248519\n",
      "Epoch: 993000, WTA-Loss: -0.5576730310157872\n",
      "Epoch: 994000, WTA-Loss: -0.5795383666343987\n",
      "Epoch: 995000, WTA-Loss: -0.5989416571892798\n",
      "Epoch: 996000, WTA-Loss: -0.6181420095823705\n",
      "Epoch: 997000, WTA-Loss: -0.6497720015309751\n",
      "Epoch: 998000, WTA-Loss: -0.6873909648135305\n",
      "Epoch: 999000, WTA-Loss: -0.6993876549564302\n",
      "Epoch: 1000000, WTA-Loss: -0.712234463531524\n",
      "Epoch: 1001000, WTA-Loss: -0.7316820571273566\n",
      "Epoch: 1002000, WTA-Loss: -0.749810867510736\n",
      "Epoch: 1003000, WTA-Loss: -0.7552585546635091\n",
      "Epoch: 1004000, WTA-Loss: -0.762432985663414\n",
      "Epoch: 1005000, WTA-Loss: -0.7779419693052769\n",
      "Epoch: 1006000, WTA-Loss: -0.7950452374480664\n",
      "Epoch: 1007000, WTA-Loss: -0.7958246825356037\n",
      "Epoch: 1008000, WTA-Loss: -0.797703240480274\n",
      "Epoch: 1009000, WTA-Loss: -0.7850930530978367\n",
      "Epoch: 1010000, WTA-Loss: -0.8097856466881931\n",
      "Epoch: 1011000, WTA-Loss: -0.8226558412201702\n",
      "Epoch: 1012000, WTA-Loss: -0.8395235809907318\n",
      "Epoch: 1013000, WTA-Loss: -0.842036149404943\n",
      "Epoch: 1014000, WTA-Loss: -0.8258518038354814\n",
      "Epoch: 1015000, WTA-Loss: -0.8539843198973686\n",
      "Epoch: 1016000, WTA-Loss: -0.8484721174817532\n",
      "Epoch: 1017000, WTA-Loss: -0.8600982310250401\n",
      "Epoch: 1018000, WTA-Loss: -0.8625052045919002\n",
      "Epoch: 1019000, WTA-Loss: -0.8772452140599489\n",
      "Epoch: 1020000, WTA-Loss: -0.8891397958248853\n",
      "Epoch: 1021000, WTA-Loss: -0.8911497993245721\n",
      "Epoch: 1022000, WTA-Loss: -0.8991309769898653\n",
      "Epoch: 1023000, WTA-Loss: -0.8958799960836769\n",
      "Epoch: 1024000, WTA-Loss: -0.9172086850106717\n",
      "Epoch: 1025000, WTA-Loss: -0.9338272694349289\n",
      "Epoch: 1026000, WTA-Loss: -0.9352910013645888\n",
      "Epoch: 1027000, WTA-Loss: -0.920704470384866\n",
      "Epoch: 1028000, WTA-Loss: -0.9396518475282938\n",
      "Epoch: 1029000, WTA-Loss: -0.9184577200487256\n",
      "Epoch: 1030000, WTA-Loss: -0.9446791600733996\n",
      "Epoch: 1031000, WTA-Loss: -0.947192025160417\n",
      "Epoch: 1032000, WTA-Loss: -0.9475298804454505\n",
      "Epoch: 1033000, WTA-Loss: -0.9607295461371541\n",
      "Epoch: 1034000, WTA-Loss: -0.9597793305404484\n",
      "Epoch: 1035000, WTA-Loss: -0.927176992379129\n",
      "Epoch: 1036000, WTA-Loss: -0.9767574526816607\n",
      "Epoch: 1037000, WTA-Loss: -0.9777714933119714\n",
      "Epoch: 1038000, WTA-Loss: -0.9733852908238768\n",
      "Epoch: 1039000, WTA-Loss: -0.9589545700065791\n",
      "Epoch: 1040000, WTA-Loss: -0.9756485657282173\n",
      "Epoch: 1041000, WTA-Loss: -0.9870033141225576\n",
      "Epoch: 1042000, WTA-Loss: -0.9870233195349574\n",
      "Epoch: 1043000, WTA-Loss: -0.9793122448138892\n",
      "Epoch: 1044000, WTA-Loss: -0.9882654315382242\n",
      "Epoch: 1045000, WTA-Loss: -0.9947579338848591\n",
      "Epoch: 1046000, WTA-Loss: -0.9793125514574349\n",
      "Epoch: 1047000, WTA-Loss: -0.9803641578406096\n",
      "Epoch: 1048000, WTA-Loss: -0.977047555744648\n",
      "Epoch: 1049000, WTA-Loss: -1.0053527966570108\n",
      "Epoch: 1050000, WTA-Loss: -1.0146914140097798\n",
      "Epoch: 1051000, WTA-Loss: -1.0068239355385304\n",
      "Epoch: 1052000, WTA-Loss: -0.9961620606929064\n",
      "Epoch: 1053000, WTA-Loss: -1.0245103730522096\n",
      "Epoch: 1054000, WTA-Loss: -1.010480428531766\n",
      "Epoch: 1055000, WTA-Loss: -1.0209860952571035\n",
      "Epoch: 1056000, WTA-Loss: -1.0300419259220361\n",
      "Epoch: 1057000, WTA-Loss: -0.995138242147863\n",
      "Epoch: 1058000, WTA-Loss: -1.0284641211405396\n",
      "Epoch: 1059000, WTA-Loss: -1.043134020447731\n",
      "Epoch: 1060000, WTA-Loss: -0.9824801061712205\n",
      "Epoch: 1061000, WTA-Loss: -1.054512629903853\n",
      "Epoch: 1062000, WTA-Loss: -1.0370332813337446\n",
      "Epoch: 1063000, WTA-Loss: -0.9510675244820305\n",
      "Epoch: 1064000, WTA-Loss: -1.0429622452408076\n",
      "Epoch: 1065000, WTA-Loss: -1.0477517849877476\n",
      "Epoch: 1066000, WTA-Loss: -1.050777849778533\n",
      "Epoch: 1067000, WTA-Loss: -1.0513616847582161\n",
      "Epoch: 1068000, WTA-Loss: -1.0483083013743162\n",
      "Epoch: 1069000, WTA-Loss: -1.0571690261214972\n",
      "Epoch: 1070000, WTA-Loss: -1.053879532918334\n",
      "Epoch: 1071000, WTA-Loss: -1.0525246561020614\n",
      "Epoch: 1072000, WTA-Loss: -1.0363592621050775\n",
      "Epoch: 1073000, WTA-Loss: -1.0693584095165134\n",
      "Epoch: 1074000, WTA-Loss: -1.0534241077974438\n",
      "Epoch: 1075000, WTA-Loss: -1.0706784799546003\n",
      "Epoch: 1076000, WTA-Loss: -1.0750937432833017\n",
      "Epoch: 1077000, WTA-Loss: -1.0530892626568675\n",
      "Epoch: 1078000, WTA-Loss: -1.0707201872803271\n",
      "Epoch: 1079000, WTA-Loss: -1.0738870278820396\n",
      "Epoch: 1080000, WTA-Loss: -1.0729900956451892\n",
      "Epoch: 1081000, WTA-Loss: -1.0647562142536044\n",
      "Epoch: 1082000, WTA-Loss: -1.0773817333579063\n",
      "Epoch: 1083000, WTA-Loss: -1.059911132030189\n",
      "Epoch: 1084000, WTA-Loss: -1.0826627106666564\n",
      "Epoch: 1085000, WTA-Loss: -1.061818870510906\n",
      "Epoch: 1086000, WTA-Loss: -1.0622010108921678\n",
      "Epoch: 1087000, WTA-Loss: -1.0870426134169102\n",
      "Epoch: 1088000, WTA-Loss: -1.077137600503862\n",
      "Epoch: 1089000, WTA-Loss: -1.0812742104865611\n",
      "Epoch: 1090000, WTA-Loss: -1.0867906332537531\n",
      "Epoch: 1091000, WTA-Loss: -1.0192713235039264\n",
      "Epoch: 1092000, WTA-Loss: -1.078839603304863\n",
      "Epoch: 1093000, WTA-Loss: -1.0956379463002086\n",
      "Epoch: 1094000, WTA-Loss: -1.0868483325541018\n",
      "Epoch: 1095000, WTA-Loss: -1.0993316204696895\n",
      "Epoch: 1096000, WTA-Loss: -1.1140759257003665\n",
      "Epoch: 1097000, WTA-Loss: -1.0902816956564785\n",
      "Epoch: 1098000, WTA-Loss: -1.098480622947216\n",
      "Epoch: 1099000, WTA-Loss: -1.1144055833294988\n",
      "Epoch: 1100000, WTA-Loss: -1.0862196797430514\n",
      "Epoch: 1101000, WTA-Loss: -1.0995177682489157\n",
      "Epoch: 1102000, WTA-Loss: -1.1111041543781757\n",
      "Epoch: 1103000, WTA-Loss: -1.091067999554798\n",
      "Epoch: 1104000, WTA-Loss: -1.1132312084138394\n",
      "Epoch: 1105000, WTA-Loss: -1.1081009303033351\n",
      "Epoch: 1106000, WTA-Loss: -1.1042422370612621\n",
      "Epoch: 1107000, WTA-Loss: -1.1035661632418632\n",
      "Epoch: 1108000, WTA-Loss: -1.1057259547710419\n",
      "Epoch: 1109000, WTA-Loss: -1.1102697634324432\n",
      "Epoch: 1110000, WTA-Loss: -1.1152549334168433\n",
      "Epoch: 1111000, WTA-Loss: -1.122310823507607\n",
      "Epoch: 1112000, WTA-Loss: -1.1252423928938806\n",
      "Epoch: 1113000, WTA-Loss: -1.0927652562633157\n",
      "Epoch: 1114000, WTA-Loss: -1.1282723622210324\n",
      "Epoch: 1115000, WTA-Loss: -1.1150837968960405\n",
      "Epoch: 1116000, WTA-Loss: -1.063335932508111\n",
      "Epoch: 1117000, WTA-Loss: -1.1233326871413738\n",
      "Epoch: 1118000, WTA-Loss: -1.091778947994113\n",
      "Epoch: 1119000, WTA-Loss: -1.1241536740623415\n",
      "Epoch: 1120000, WTA-Loss: -1.1225715632587672\n",
      "Epoch: 1121000, WTA-Loss: -1.1227704822272062\n",
      "Epoch: 1122000, WTA-Loss: -1.1247185788750649\n",
      "Epoch: 1123000, WTA-Loss: -1.0927813419662415\n",
      "Epoch: 1124000, WTA-Loss: -1.134740521878004\n",
      "Epoch: 1125000, WTA-Loss: -1.1204603188456967\n",
      "Epoch: 1126000, WTA-Loss: -1.115839162312448\n",
      "Epoch: 1127000, WTA-Loss: -1.126090902402997\n",
      "Epoch: 1128000, WTA-Loss: -1.1280057133808732\n",
      "Epoch: 1129000, WTA-Loss: -1.1301170544400811\n",
      "Epoch: 1130000, WTA-Loss: -1.1302487607598304\n",
      "Epoch: 1131000, WTA-Loss: -1.1412543798685073\n",
      "Epoch: 1132000, WTA-Loss: -1.1506457370221614\n",
      "Epoch: 1133000, WTA-Loss: -1.1007444954849779\n",
      "Epoch: 1134000, WTA-Loss: -1.1316604254040867\n",
      "Epoch: 1135000, WTA-Loss: -1.1187275013327598\n",
      "Epoch: 1136000, WTA-Loss: -1.1470742075517775\n",
      "Epoch: 1137000, WTA-Loss: -1.095264510411769\n",
      "Epoch: 1138000, WTA-Loss: -1.1290787925124168\n",
      "Epoch: 1139000, WTA-Loss: -1.1283213854413479\n",
      "Epoch: 1140000, WTA-Loss: -1.1377816216498613\n",
      "Epoch: 1141000, WTA-Loss: -1.1491526183113456\n",
      "Epoch: 1142000, WTA-Loss: -1.1488098423853517\n",
      "Epoch: 1143000, WTA-Loss: -1.0534723971672355\n",
      "Epoch: 1144000, WTA-Loss: -1.1235440704822541\n",
      "Epoch: 1145000, WTA-Loss: -1.134643461521715\n",
      "Epoch: 1146000, WTA-Loss: -1.1502765326350928\n",
      "Epoch: 1147000, WTA-Loss: -1.1648514320142567\n",
      "Epoch: 1148000, WTA-Loss: -1.1629419376924635\n",
      "Epoch: 1149000, WTA-Loss: -1.1149382922276854\n",
      "Epoch: 1150000, WTA-Loss: -1.1674057497419417\n",
      "Epoch: 1151000, WTA-Loss: -1.1587602846175433\n",
      "Epoch: 1152000, WTA-Loss: -1.1454341044947505\n",
      "Epoch: 1153000, WTA-Loss: -1.166880852110684\n",
      "Epoch: 1154000, WTA-Loss: -1.1635169300809503\n",
      "Epoch: 1155000, WTA-Loss: -1.1475651858635247\n",
      "Epoch: 1156000, WTA-Loss: -1.1563532227165998\n",
      "Epoch: 1157000, WTA-Loss: -1.1479189389739186\n",
      "Epoch: 1158000, WTA-Loss: -1.16796977198869\n",
      "Epoch: 1159000, WTA-Loss: -1.1673213019520043\n",
      "Epoch: 1160000, WTA-Loss: -1.1708578326627612\n",
      "Epoch: 1161000, WTA-Loss: -1.168985167093575\n",
      "Epoch: 1162000, WTA-Loss: -1.181178063042462\n",
      "Epoch: 1163000, WTA-Loss: -1.1649530465900897\n",
      "Epoch: 1164000, WTA-Loss: -1.159015908971429\n",
      "Epoch: 1165000, WTA-Loss: -1.1667243218272925\n",
      "Epoch: 1166000, WTA-Loss: -1.1820412299111485\n",
      "Epoch: 1167000, WTA-Loss: -1.1798044472634792\n",
      "Epoch: 1168000, WTA-Loss: -1.1770619858652354\n",
      "Epoch: 1169000, WTA-Loss: -1.1054313215613365\n",
      "Epoch: 1170000, WTA-Loss: -1.1763634704351424\n",
      "Epoch: 1171000, WTA-Loss: -1.1513728530742229\n",
      "Epoch: 1172000, WTA-Loss: -1.185154707044363\n",
      "Epoch: 1173000, WTA-Loss: -1.0872433917857707\n",
      "Epoch: 1174000, WTA-Loss: -1.1925595195144414\n",
      "Epoch: 1175000, WTA-Loss: -1.1158115302920342\n",
      "Epoch: 1176000, WTA-Loss: -1.1843643214702606\n",
      "Epoch: 1177000, WTA-Loss: -1.1734369259811939\n",
      "Epoch: 1178000, WTA-Loss: -1.1870491380319\n",
      "Epoch: 1179000, WTA-Loss: -1.1687320634275675\n",
      "Epoch: 1180000, WTA-Loss: -1.1715804275125266\n",
      "Epoch: 1181000, WTA-Loss: -1.18870045119524\n",
      "Epoch: 1182000, WTA-Loss: -1.1327631862536072\n",
      "Epoch: 1183000, WTA-Loss: -1.185398501187563\n",
      "Epoch: 1184000, WTA-Loss: -1.1854922921545803\n",
      "Epoch: 1185000, WTA-Loss: -1.1735642337426544\n",
      "Epoch: 1186000, WTA-Loss: -1.1837558293491601\n",
      "Epoch: 1187000, WTA-Loss: -1.1739177474975586\n",
      "Epoch: 1188000, WTA-Loss: -1.1775204083099962\n",
      "Epoch: 1189000, WTA-Loss: -1.1918131669200958\n",
      "Epoch: 1190000, WTA-Loss: -1.181253145828843\n",
      "Epoch: 1191000, WTA-Loss: -1.1759980044066907\n",
      "Epoch: 1192000, WTA-Loss: -1.1515197188574822\n",
      "Epoch: 1193000, WTA-Loss: -1.1923499957919121\n",
      "Epoch: 1194000, WTA-Loss: -1.1565379530135542\n",
      "Epoch: 1195000, WTA-Loss: -1.1920545440614223\n",
      "Epoch: 1196000, WTA-Loss: -1.152292277611792\n",
      "Epoch: 1197000, WTA-Loss: -1.1509799716826528\n",
      "Epoch: 1198000, WTA-Loss: -1.1917872229926287\n",
      "Epoch: 1199000, WTA-Loss: -1.1757224826961756\n",
      "Epoch: 1200000, WTA-Loss: -1.1938484749644995\n",
      "Epoch: 1201000, WTA-Loss: -1.1745896048769355\n",
      "Epoch: 1202000, WTA-Loss: -1.1698137453347444\n",
      "Epoch: 1203000, WTA-Loss: -1.0575917678363622\n",
      "Epoch: 1204000, WTA-Loss: -1.1909882775098086\n",
      "Epoch: 1205000, WTA-Loss: -1.1920864245519043\n",
      "Epoch: 1206000, WTA-Loss: -1.1803245819583534\n",
      "Epoch: 1207000, WTA-Loss: -1.1816450751572847\n",
      "Epoch: 1208000, WTA-Loss: -1.1363980222269894\n",
      "Epoch: 1209000, WTA-Loss: -1.1407987058237194\n",
      "Epoch: 1210000, WTA-Loss: -1.2004825451523065\n",
      "Epoch: 1211000, WTA-Loss: -1.1732627235278488\n",
      "Epoch: 1212000, WTA-Loss: -1.1940066898986696\n",
      "Epoch: 1213000, WTA-Loss: -1.1633242522776126\n",
      "Epoch: 1214000, WTA-Loss: -1.161125478927046\n",
      "Epoch: 1215000, WTA-Loss: -1.1915593323782085\n",
      "Epoch: 1216000, WTA-Loss: -1.0741190288756042\n",
      "Epoch: 1217000, WTA-Loss: -1.17680543333292\n",
      "Epoch: 1218000, WTA-Loss: -1.1674615971669555\n",
      "Epoch: 1219000, WTA-Loss: -1.178955488987267\n",
      "Epoch: 1220000, WTA-Loss: -1.153112891100347\n",
      "Epoch: 1221000, WTA-Loss: -1.1836327299326659\n",
      "Epoch: 1222000, WTA-Loss: -1.1775140388607979\n",
      "Epoch: 1223000, WTA-Loss: -1.1844728699848055\n",
      "Epoch: 1224000, WTA-Loss: -1.146872897112742\n",
      "Epoch: 1225000, WTA-Loss: -1.2104197698682546\n",
      "Epoch: 1226000, WTA-Loss: -1.179566273126751\n",
      "Epoch: 1227000, WTA-Loss: -1.2057643449157476\n",
      "Epoch: 1228000, WTA-Loss: -1.213874887868762\n",
      "Epoch: 1229000, WTA-Loss: -1.1456439276076853\n",
      "Epoch: 1230000, WTA-Loss: -1.192604579463601\n",
      "Epoch: 1231000, WTA-Loss: -1.2023807753622533\n",
      "Epoch: 1232000, WTA-Loss: -1.207284441627562\n",
      "Epoch: 1233000, WTA-Loss: -1.1087391409911216\n",
      "Epoch: 1234000, WTA-Loss: -1.195725830003619\n",
      "Epoch: 1235000, WTA-Loss: -1.2104104969576002\n",
      "Epoch: 1236000, WTA-Loss: -1.1931425217166542\n",
      "Epoch: 1237000, WTA-Loss: -1.1788488143905997\n",
      "Epoch: 1238000, WTA-Loss: -1.1815621796697378\n",
      "Epoch: 1239000, WTA-Loss: -0.8145761709585786\n",
      "Epoch: 1240000, WTA-Loss: -1.0963141274750232\n",
      "Epoch: 1241000, WTA-Loss: -1.1863151676245034\n",
      "Epoch: 1242000, WTA-Loss: -1.2072228990644216\n",
      "Epoch: 1243000, WTA-Loss: -1.2134606029689312\n",
      "Epoch: 1244000, WTA-Loss: -1.1985752131268381\n",
      "Epoch: 1245000, WTA-Loss: -1.215685235001147\n",
      "Epoch: 1246000, WTA-Loss: -1.1925470093786716\n",
      "Epoch: 1247000, WTA-Loss: -1.2110156021937728\n",
      "Epoch: 1248000, WTA-Loss: -1.1716569689437748\n",
      "Epoch: 1249000, WTA-Loss: -1.1753616297096015\n",
      "Epoch: 1250000, WTA-Loss: -0.9916908080270513\n",
      "Epoch: 1251000, WTA-Loss: -1.0362116004843265\n",
      "Epoch: 1252000, WTA-Loss: -1.1801184614375233\n",
      "Epoch: 1253000, WTA-Loss: -1.1870776546671986\n",
      "Epoch: 1254000, WTA-Loss: -1.1983994835093617\n",
      "Epoch: 1255000, WTA-Loss: -1.207267055772245\n",
      "Epoch: 1256000, WTA-Loss: -1.1111546458639205\n",
      "Epoch: 1257000, WTA-Loss: -1.1838415334895254\n",
      "Epoch: 1258000, WTA-Loss: -1.2039516405686737\n",
      "Epoch: 1259000, WTA-Loss: -0.8546254529005383\n",
      "Epoch: 1260000, WTA-Loss: -1.389702254474163\n",
      "Epoch: 1261000, WTA-Loss: -1.7621370090842248\n",
      "Epoch: 1262000, WTA-Loss: -1.949707054078579\n",
      "Epoch: 1263000, WTA-Loss: -2.024488200753927\n",
      "Epoch: 1264000, WTA-Loss: -1.96397030261904\n",
      "Epoch: 1265000, WTA-Loss: -2.093899820804596\n",
      "Epoch: 1266000, WTA-Loss: -2.116423234730959\n",
      "Epoch: 1267000, WTA-Loss: -2.1036085431575775\n",
      "Epoch: 1268000, WTA-Loss: -2.210843246638775\n",
      "Epoch: 1269000, WTA-Loss: -2.212933014601469\n",
      "Epoch: 1270000, WTA-Loss: -0.8836610624184832\n",
      "Epoch: 1271000, WTA-Loss: -1.9604128946512938\n",
      "Epoch: 1272000, WTA-Loss: -1.8897987948022783\n",
      "Epoch: 1273000, WTA-Loss: -2.1013856491148473\n",
      "Epoch: 1274000, WTA-Loss: -2.094927154481411\n",
      "Epoch: 1275000, WTA-Loss: -2.168692092180252\n",
      "Epoch: 1276000, WTA-Loss: -2.069376076877117\n",
      "Epoch: 1277000, WTA-Loss: -2.208288918673992\n",
      "Epoch: 1278000, WTA-Loss: -2.2058687012791633\n",
      "Epoch: 1279000, WTA-Loss: -2.212736980468035\n",
      "Epoch: 1280000, WTA-Loss: -0.8297701891670004\n",
      "Epoch: 1281000, WTA-Loss: -0.09128532547550276\n",
      "Epoch: 1282000, WTA-Loss: -0.46099936207244174\n",
      "Epoch: 1283000, WTA-Loss: -0.6675200723279268\n",
      "Epoch: 1284000, WTA-Loss: -0.8289361741524189\n",
      "Epoch: 1285000, WTA-Loss: -0.962676758075133\n",
      "Epoch: 1286000, WTA-Loss: -1.030015769083053\n",
      "Epoch: 1287000, WTA-Loss: -1.0960748493894934\n",
      "Epoch: 1288000, WTA-Loss: -1.0083465198539197\n",
      "Epoch: 1289000, WTA-Loss: -1.1014857034608723\n",
      "Epoch: 1290000, WTA-Loss: -1.0663877153936774\n",
      "Epoch: 1291000, WTA-Loss: -1.1133052822947502\n",
      "Epoch: 1292000, WTA-Loss: -1.1125461473912002\n",
      "Epoch: 1293000, WTA-Loss: -1.0562291313409806\n",
      "Epoch: 1294000, WTA-Loss: -1.1331477149799467\n",
      "Epoch: 1295000, WTA-Loss: -1.1391726940721274\n",
      "Epoch: 1296000, WTA-Loss: -1.0884548034332693\n",
      "Epoch: 1297000, WTA-Loss: -1.1523488056734204\n",
      "Epoch: 1298000, WTA-Loss: -1.1190984708741307\n",
      "Epoch: 1299000, WTA-Loss: -1.0056975210029633\n",
      "Epoch: 1300000, WTA-Loss: -1.7711219084113836\n",
      "Epoch: 1301000, WTA-Loss: -1.9817893041074277\n",
      "Epoch: 1302000, WTA-Loss: -2.0395909738838673\n",
      "Epoch: 1303000, WTA-Loss: -2.100951043918729\n",
      "Epoch: 1304000, WTA-Loss: -1.8984010896533727\n",
      "Epoch: 1305000, WTA-Loss: -2.05279533752799\n",
      "Epoch: 1306000, WTA-Loss: -2.1541203927993773\n",
      "Epoch: 1307000, WTA-Loss: -2.1476895787119865\n",
      "Epoch: 1308000, WTA-Loss: -2.155489673256874\n",
      "Epoch: 1309000, WTA-Loss: -2.015609193891287\n",
      "Epoch: 1310000, WTA-Loss: -2.1725204672813416\n",
      "Epoch: 1311000, WTA-Loss: -2.2177391852736474\n",
      "Epoch: 1312000, WTA-Loss: -2.1800902411043643\n",
      "Epoch: 1313000, WTA-Loss: -2.0861178863793612\n",
      "Epoch: 1314000, WTA-Loss: -2.175309137403965\n",
      "Epoch: 1315000, WTA-Loss: -2.0321629855502397\n",
      "Epoch: 1316000, WTA-Loss: -2.195822280585766\n",
      "Epoch: 1317000, WTA-Loss: -2.206367088466883\n",
      "Epoch: 1318000, WTA-Loss: -2.164651088654995\n",
      "Epoch: 1319000, WTA-Loss: -2.1767874875664712\n",
      "Epoch: 1320000, WTA-Loss: -2.1647541544139384\n",
      "Epoch: 1321000, WTA-Loss: -2.1639821371138095\n",
      "Epoch: 1322000, WTA-Loss: -2.1982578535974024\n",
      "Epoch: 1323000, WTA-Loss: -2.258308741211891\n",
      "Epoch: 1324000, WTA-Loss: -2.1429068357609213\n",
      "Epoch: 1325000, WTA-Loss: -2.2186936205029486\n",
      "Epoch: 1326000, WTA-Loss: -2.1101303766667843\n",
      "Epoch: 1327000, WTA-Loss: -2.255610907077789\n",
      "Epoch: 1328000, WTA-Loss: -2.1860981974303724\n",
      "Epoch: 1329000, WTA-Loss: -2.2340705155730247\n",
      "Epoch: 1330000, WTA-Loss: -2.1688123684674503\n",
      "Epoch: 1331000, WTA-Loss: -2.2467379097640516\n",
      "Epoch: 1332000, WTA-Loss: -2.249879377186298\n",
      "Epoch: 1333000, WTA-Loss: -2.279338208436966\n",
      "Epoch: 1334000, WTA-Loss: -2.2570414491295816\n",
      "Epoch: 1335000, WTA-Loss: -2.2216984406709672\n",
      "Epoch: 1336000, WTA-Loss: -2.1785356745272875\n",
      "Epoch: 1337000, WTA-Loss: -2.047816746518016\n",
      "Epoch: 1338000, WTA-Loss: -2.2643663737475874\n",
      "Epoch: 1339000, WTA-Loss: -2.2381464773714543\n",
      "Epoch: 1340000, WTA-Loss: -2.269478612065315\n",
      "Epoch: 1341000, WTA-Loss: -2.1828440242558718\n",
      "Epoch: 1342000, WTA-Loss: -2.279402685970068\n",
      "Epoch: 1343000, WTA-Loss: -2.258515361636877\n",
      "Epoch: 1344000, WTA-Loss: -2.2749705833494662\n",
      "Epoch: 1345000, WTA-Loss: -2.2889094613790513\n",
      "Epoch: 1346000, WTA-Loss: -2.31804832687974\n",
      "Epoch: 1347000, WTA-Loss: -2.1873004288226365\n",
      "Epoch: 1348000, WTA-Loss: -2.295232618391514\n",
      "Epoch: 1349000, WTA-Loss: -2.288279029071331\n",
      "Epoch: 1350000, WTA-Loss: -2.3297050975859164\n",
      "Epoch: 1351000, WTA-Loss: -2.2915352409183978\n",
      "Epoch: 1352000, WTA-Loss: -2.069866590514779\n",
      "Epoch: 1353000, WTA-Loss: -2.1547611857056617\n",
      "Epoch: 1354000, WTA-Loss: -2.2019416381418706\n",
      "Epoch: 1355000, WTA-Loss: -2.25854316586256\n",
      "Epoch: 1356000, WTA-Loss: 7.083435472853481\n",
      "Epoch: 1357000, WTA-Loss: -1.951470994487405\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_iter):\n\u001b[1;32m     48\u001b[0m     optimizer_wta\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m     obs_wta, tar_x_wta, tar_y_wta \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraj_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     pred_wta, gate_wta \u001b[38;5;241m=\u001b[39m model_wta(obs_wta, tar_x_wta)\n\u001b[1;32m     52\u001b[0m     loss_wta, wta_nll \u001b[38;5;241m=\u001b[39m model_wta\u001b[38;5;241m.\u001b[39mloss(pred_wta, gate_wta, tar_y_wta)\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(x, y, traj_ids, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m o_ids \u001b[38;5;241m=\u001b[39m random_query_ids[:n_o]\n\u001b[1;32m     13\u001b[0m t_ids \u001b[38;5;241m=\u001b[39m random_query_ids[n_o:n_o\u001b[38;5;241m+\u001b[39mn_t]\n\u001b[0;32m---> 15\u001b[0m obs[i, :, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraj_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_ids\u001b[49m\u001b[43m]\u001b[49m, y[traj_ids[i], o_ids]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m tar[i, :, :] \u001b[38;5;241m=\u001b[39m x[traj_ids[i], t_ids]\n\u001b[1;32m     17\u001b[0m tar_val[i, :, :] \u001b[38;5;241m=\u001b[39m y[traj_ids[i], t_ids]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'outputs/mocapact/{dy}D/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "# if not os.path.exists(f'{root_folder}img/'):\n",
    "#     os.makedirs(f'{root_folder}img/')\n",
    "\n",
    "torch.save(y, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 10_000_000\n",
    "epoch_iter = num_demos//batch_size  # number of batches per epoch (e.g. 100//32 = 3)\n",
    "v_epoch_iter = num_val//batch_size  # number of batches per validation (e.g. 100//32 = 3)\n",
    "avg_loss_wta = 0\n",
    "\n",
    "val_per_epoch = 1000\n",
    "min_val_loss_wta = 1000000\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loss_wta, validation_error_wta = [], []\n",
    "\n",
    "wta_tr_loss_path = f'{root_folder}wta_training_loss.pt'\n",
    "wta_val_err_path = f'{root_folder}wta_validation_error.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_wta = 0\n",
    "\n",
    "    # traj_ids = torch.randperm(x.shape[0])[:batch_size*epoch_iter].chunk(epoch_iter)  # [:batch_size*epoch_iter] because nof_trajectories may be indivisible by batch_size\n",
    "    traj_ids, v_traj_ids = [], []\n",
    "    inds = torch.randperm(num_indiv)\n",
    "    vinds = torch.randperm(num_val_indiv)\n",
    "    for i in inds:\n",
    "        traj_ids.append([inds[i], num_demos-inds[i]-1])\n",
    "\n",
    "    for i in vinds:\n",
    "        v_traj_ids.append([vinds[i], num_val-vinds[i]-1])\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        optimizer_wta.zero_grad()\n",
    "\n",
    "        obs_wta, tar_x_wta, tar_y_wta = get_batch(x, y, traj_ids[i], device)\n",
    "        pred_wta, gate_wta = model_wta(obs_wta, tar_x_wta)\n",
    "        loss_wta, wta_nll = model_wta.loss(pred_wta, gate_wta, tar_y_wta)\n",
    "        loss_wta.backward()\n",
    "        optimizer_wta.step()\n",
    "\n",
    "        epoch_loss_wta += wta_nll.item()\n",
    "\n",
    "    training_loss_wta.append(epoch_loss_wta)\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            # v_traj_ids = torch.randperm(vx.shape[0])[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss_wta = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                o_wta, t_wta, tr_wta = get_validation_batch(vx, vy, v_traj_ids[j], device=device)\n",
    "\n",
    "                p_wta, g_wta = model_wta(o_wta, t_wta)\n",
    "                dec_id = torch.argmax(g_wta.squeeze(1), dim=-1)\n",
    "                vp_means = p_wta[dec_id, torch.arange(batch_size), :, :dy]\n",
    "                val_loss_wta += mse_loss(vp_means, tr_wta).item()\n",
    "\n",
    "            validation_error_wta.append(val_loss_wta)\n",
    "            if val_loss_wta < min_val_loss_wta:\n",
    "                min_val_loss_wta = val_loss_wta\n",
    "                print(f'(WTA)New best: {min_val_loss_wta}')\n",
    "                torch.save(model_wta_.state_dict(), f'{root_folder}saved_models/wta_on_synth.pt')\n",
    "  \n",
    "        # if epoch % (val_per_epoch*10) == 0:\n",
    "        #     draw_val_plot(root_folder, epoch)\n",
    "\n",
    "\n",
    "    avg_loss_wta += epoch_loss_wta\n",
    "\n",
    "    if epoch % val_per_epoch == 0:\n",
    "        print(\"Epoch: {}, WTA-Loss: {}\".format(epoch, avg_loss_wta/val_per_epoch))\n",
    "        avg_loss_wta = 0\n",
    "\n",
    "torch.save(torch.Tensor(training_loss_wta), wta_tr_loss_path)\n",
    "torch.save(torch.Tensor(validation_error_wta), wta_val_err_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WTA_CNP(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=261, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=1229, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=112, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gate): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.wta_cnp import WTA_CNP\n",
    "\n",
    "root_folder = f'outputs/experimental/56D/1701871167/'\n",
    "wta_model_path = f'{root_folder}saved_models/wta_on_synth.pt'\n",
    "\n",
    "y = torch.load(f'{root_folder}y.pt').cpu()\n",
    "num_samples, t_steps, dy = y.shape\n",
    "dx = 205\n",
    "batch_size = 1\n",
    "n_max_obs, n_max_tar = 6, 6\n",
    "\n",
    "wta = WTA_CNP(dx, dy, n_max_obs, n_max_tar, [1024, 1024, 1024], num_decoders=2, decoder_hidden_dims=[512, 512, 512], batch_size=batch_size, scale_coefs=True).to(device)\n",
    "\n",
    "wta.load_state_dict(torch.load(wta_model_path))\n",
    "wta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dm_control import viewer\n",
    "from dm_control import composer\n",
    "from dm_control.locomotion import arenas\n",
    "from dm_control.locomotion.tasks import go_to_target\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "from dm_control_wrapper import StandInitializer\n",
    "from dm_control.composer import ObservationPadding\n",
    "\n",
    "seed = 0\n",
    "dind = 13\n",
    "\n",
    "def prepare_obs(obs, ind, a):\n",
    "    vv = []\n",
    "    for k in obs.keys():\n",
    "        real_key = k.split('/')[1]\n",
    "        if real_key in desired_observables:\n",
    "            vals = obs[k].flatten()\n",
    "            vv.extend([vals])\n",
    "    if ind == 0:\n",
    "        vv.extend([y[dind, ind].numpy()])\n",
    "    else:\n",
    "        vv.extend([a])\n",
    "    v = np.concatenate(vv).reshape(-1)\n",
    "    return torch.from_numpy(v).view(1, 1, dx+dy).float().to(device)\n",
    "\n",
    "\n",
    "def prepare_tar(ind):\n",
    "    return x[dind, ind].view(1, 1, dx).float().to(device)\n",
    "\n",
    "initializer = StandInitializer()\n",
    "walker = cmu_humanoid.CMUHumanoidPositionControlledV2020(initializer=initializer)\n",
    "\n",
    "# Build an empty arena.\n",
    "arena = arenas.Floor()\n",
    "\n",
    "# Build a task that rewards the agent for tracking motion capture reference\n",
    "# data.\n",
    "task = go_to_target.GoToTarget(walker=walker, arena=arena, physics_timestep=0.005, control_timestep=0.03)\n",
    "env = composer.Environment(task=task, random_state=seed)\n",
    "# print(env.control_timestep())\n",
    "\n",
    "ind = -1\n",
    "inst_a = None\n",
    "\n",
    "def tst(ts):\n",
    "    global ind, inst_a\n",
    "    ind += 1\n",
    "    # dm_obs, dm_tar = prepare_obs(ts.observation, ind, inst_a), prepare_tar(ind+1)\n",
    "    # p_wta, g_wta = wta(dm_obs, dm_tar)\n",
    "    # # print(g_wta.squeeze(1))\n",
    "    # inst_a = p_wta[torch.argmax(g_wta.squeeze(1), dim=-1), 0, 0, :dy].cpu().detach().numpy().squeeze()\n",
    "    # return inst_a\n",
    "    return y[dind, ind].numpy()\n",
    "    # return full_act[dind][ind]\n",
    "\n",
    "# Viewer for visualization\n",
    "viewer.launch(env, policy=tst)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dm_obs = prepare_obs(env.reset()[3], 0)\n",
    "# dm_tar = prepare_tar(1)\n",
    "\n",
    "# for i in range(1, t_steps):\n",
    "#     # p_wta, g_wta = wta(dm_obs, dm_tar)\n",
    "#     # a = p_wta[torch.argmax(g_wta.squeeze(1), dim=-1), 0, 0, :dy].cpu().detach().numpy().squeeze()\n",
    "#     # print(a)\n",
    "#     a = y[0, i].numpy()\n",
    "#     s = env.step(a)\n",
    "#     # dm_obs, dm_tar = prepare_obs(s.observation, i-1), prepare_tar(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuator_activation <class 'numpy.ndarray'>\n",
      "appendages_pos <class 'numpy.ndarray'>\n",
      "body_height <class 'numpy.ndarray'>\n",
      "end_effectors_pos <class 'numpy.ndarray'>\n",
      "gyro_anticlockwise_spin <class 'numpy.ndarray'>\n",
      "gyro_backward_roll <class 'numpy.ndarray'>\n",
      "gyro_control <class 'numpy.ndarray'>\n",
      "gyro_rightward_roll <class 'numpy.ndarray'>\n",
      "head_height <class 'numpy.ndarray'>\n",
      "joints_pos <class 'numpy.ndarray'>\n",
      "joints_vel <class 'numpy.ndarray'>\n",
      "joints_vel_control <class 'numpy.ndarray'>\n",
      "orientation <class 'numpy.ndarray'>\n",
      "position <class 'numpy.ndarray'>\n",
      "sensors_accelerometer <class 'numpy.ndarray'>\n",
      "sensors_gyro <class 'numpy.ndarray'>\n",
      "sensors_torque <class 'numpy.ndarray'>\n",
      "sensors_touch <class 'numpy.ndarray'>\n",
      "sensors_velocimeter <class 'numpy.ndarray'>\n",
      "time_in_clip <class 'numpy.ndarray'>\n",
      "torso_xvel <class 'numpy.ndarray'>\n",
      "torso_yvel <class 'numpy.ndarray'>\n",
      "veloc_forward <class 'numpy.ndarray'>\n",
      "veloc_strafe <class 'numpy.ndarray'>\n",
      "veloc_up <class 'numpy.ndarray'>\n",
      "velocimeter_control <class 'numpy.ndarray'>\n",
      "world_zaxis <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for k in f['observable_indices']['walker']:\n",
    "    if 'reference' not in k:\n",
    "        print(k, type(f['observable_indices']['walker'][k][()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuator_activation (1, 56)\n",
      "appendages_pos (1, 15)\n",
      "body_height (1,)\n",
      "end_effectors_pos (1, 12)\n",
      "joints_pos (1, 56)\n",
      "joints_vel (1, 56)\n",
      "sensors_accelerometer (1, 3)\n",
      "sensors_force (1, 0)\n",
      "sensors_gyro (1, 3)\n",
      "sensors_torque (1, 6)\n",
      "sensors_touch (1, 10)\n",
      "sensors_velocimeter (1, 3)\n",
      "world_zaxis (1, 3)\n",
      "target (1, 3)\n"
     ]
    }
   ],
   "source": [
    "for k in s[3].keys():\n",
    "    print(k.replace('/', '.').split('.')[-1], s[3][k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([1, 781, 1]) Y: torch.Size([1, 781, 62]) VX: torch.Size([1, 781, 1]) VY: torch.Size([1, 781, 62])\n"
     ]
    }
   ],
   "source": [
    "# def transform_data(data):\n",
    "#     num_dimensions = data.shape[2]\n",
    "\n",
    "#     transformation_matrix = torch.zeros((num_dimensions, 2))\n",
    "#     transformed_data = torch.zeros_like(data)\n",
    "\n",
    "#     # Apply transformations to each dimension\n",
    "#     for i in range(num_dimensions):\n",
    "#         dim_data = data[:, :, i]\n",
    "\n",
    "#         min_val = dim_data.min()\n",
    "#         max_val = dim_data.max()\n",
    "\n",
    "#         transformation_matrix[i, 0] = min_val\n",
    "#         transformation_matrix[i, 1] = max_val\n",
    "\n",
    "#         interval = max_val - min_val\n",
    "#         if interval < 1e-6:\n",
    "#             interval = 1\n",
    "\n",
    "#         transformed_dim = 2 * (dim_data - min_val) / interval - 1\n",
    "#         transformed_data[:, :, i] = transformed_dim\n",
    "\n",
    "#     return transformed_data, transformation_matrix\n",
    "\n",
    "# def reconstruct_data(transformed_data, transformation_matrix):\n",
    "#     num_dimensions = transformed_data.shape[2]\n",
    "\n",
    "#     reconstructed_data = torch.zeros_like(transformed_data)\n",
    "\n",
    "#     for i in range(num_dimensions):\n",
    "#         transformed_dim = transformed_data[:, :, i]\n",
    "#         min_val, max_val = transformation_matrix[i, 0], transformation_matrix[i, 1]\n",
    "\n",
    "#         reconstructed_dim = ((transformed_dim + 1) / 2) * (max_val - min_val) + min_val\n",
    "#         reconstructed_data[:, :, i] = reconstructed_dim\n",
    "\n",
    "#     return reconstructed_data\n",
    "\n",
    "# y = data.clone().to(device)\n",
    "# x = torch.unsqueeze(torch.linspace(0, 1, t_steps).repeat(num_demos, 1), -1).to(device)\n",
    "\n",
    "# vx = x.clone()\n",
    "# noise = torch.clamp(torch.randn(x.shape)*1e-4**0.5, min=0).to(device)\n",
    "# vy = y.clone() + noise\n",
    "\n",
    "# print(\"X:\", x.shape, \"Y:\", y.shape, \"VX:\", vx.shape, \"VY:\", vy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
